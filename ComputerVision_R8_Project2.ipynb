{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case study is from an open source dataset from Kaggle. You are provided with a training set and a test set of images of dogs. Each image has a filename that is its unique id. The dataset comprises 120 breeds of dogs. The goal of the project is to create a classifier capable of determining a dog's breed from a photo\n",
    "\n",
    "Link to the Kaggle project site:\n",
    "\n",
    "https://www.kaggle.com/c/dog-breed-identification/data\n",
    "\n",
    "Please find below the link to the dataset:\n",
    "\n",
    "https://drive.google.com/drive/u/2/folders/1lFFmE4lxsIBhF18T9XswvAfKm9FF4pd6\n",
    "\n",
    "(Open in a new tab if the link doesn't open directly)\n",
    "\n",
    "\n",
    "Video for creating train_data (Optional)\n",
    "\n",
    "Watch this video if you face any issue in creating the training data\n",
    "\n",
    " \n",
    "\n",
    "https://greatlearning.wistia.com/medias/8q0v2mqqor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read labels.csv\n",
    "    # Create Y from the breeds column in the read file\n",
    "    # Loop through the id column in the read file\n",
    "        # In the downloaded images folder find the image matching the id column value\n",
    "        # Resize the image to 128*128\n",
    "        # Save the image pixels into a array\n",
    "    # Save the array and Y into a .h5 file\n",
    "# Read the .h5 file\n",
    "    # break into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_breed_identification_resizedImages_to_200h_200w.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import skimage.io as io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "#Importing opencv module for the resizing function\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "rootdir_trainValData = 'D:\\\\workingDirectory\\\\PROJECTS\\\\12 - PROJECT - R8 - 02 - Computer Vision with CNN\\\\dog-breed-identification\\\\train'\n",
    "res_dim_h = 200\n",
    "res_dim_w = 200\n",
    "\n",
    "resizedImagesFilename = 'dog_breed_identification_resizedImages_to_'+str(res_dim_h)+'h_'+str(res_dim_w)+'w.h5'\n",
    "print(resizedImagesFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RESIZE\n",
    "\"\"\"\n",
    "\n",
    "def resizeToDesiredShape(inputImage, h, w):\n",
    "    # Seperate the r, g, b layers\n",
    "    r_array = np.array(inputImage[:, :, 0])\n",
    "    g_array = np.array(inputImage[:, :, 1])\n",
    "    b_array = np.array(inputImage[:, :, 2])\n",
    "    \n",
    "    # Apply resize on each layer (r, g, b)\n",
    "    r_res = cv2.resize(r_array, dsize=(h, w), interpolation=cv2.INTER_CUBIC)\n",
    "    g_res = cv2.resize(r_array, dsize=(h, w), interpolation=cv2.INTER_CUBIC)\n",
    "    b_res = cv2.resize(r_array, dsize=(h, w), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Prepare image using the resized r, g, b\n",
    "    outputImage = np.array([r_res, g_res, b_res])  # Put the reshaped values into a array\n",
    "    outputImage = np.transpose(outputImage, (1, 2, 0))  # Swap the dimensions of the array\n",
    "\n",
    "    return outputImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing image:  0\n",
      "Resizing image:  50\n",
      "Resizing image:  100\n",
      "Resizing image:  150\n",
      "Resizing image:  200\n",
      "Resizing image:  250\n",
      "Resizing image:  300\n",
      "Resizing image:  350\n",
      "Resizing image:  400\n",
      "Resizing image:  450\n",
      "Resizing image:  500\n",
      "Resizing image:  550\n",
      "Resizing image:  600\n",
      "Resizing image:  650\n",
      "Resizing image:  700\n",
      "Resizing image:  750\n",
      "Resizing image:  800\n",
      "Resizing image:  850\n",
      "Resizing image:  900\n",
      "Resizing image:  950\n",
      "Resizing image:  1000\n",
      "Resizing image:  1050\n",
      "Resizing image:  1100\n",
      "Resizing image:  1150\n",
      "Resizing image:  1200\n",
      "Resizing image:  1250\n",
      "Resizing image:  1300\n",
      "Resizing image:  1350\n",
      "Resizing image:  1400\n",
      "Resizing image:  1450\n",
      "Resizing image:  1500\n",
      "Resizing image:  1550\n",
      "Resizing image:  1600\n",
      "Resizing image:  1650\n",
      "Resizing image:  1700\n",
      "Resizing image:  1750\n",
      "Resizing image:  1800\n",
      "Resizing image:  1850\n",
      "Resizing image:  1900\n",
      "Resizing image:  1950\n",
      "Resizing image:  2000\n",
      "Resizing image:  2050\n",
      "Resizing image:  2100\n",
      "Resizing image:  2150\n",
      "Resizing image:  2200\n",
      "Resizing image:  2250\n",
      "Resizing image:  2300\n",
      "Resizing image:  2350\n",
      "Resizing image:  2400\n",
      "Resizing image:  2450\n",
      "Resizing image:  2500\n",
      "Resizing image:  2550\n",
      "Resizing image:  2600\n",
      "Resizing image:  2650\n",
      "Resizing image:  2700\n",
      "Resizing image:  2750\n",
      "Resizing image:  2800\n",
      "Resizing image:  2850\n",
      "Resizing image:  2900\n",
      "Resizing image:  2950\n",
      "Resizing image:  3000\n",
      "Resizing image:  3050\n",
      "Resizing image:  3100\n",
      "Resizing image:  3150\n",
      "Resizing image:  3200\n",
      "Resizing image:  3250\n",
      "Resizing image:  3300\n",
      "Resizing image:  3350\n",
      "Resizing image:  3400\n",
      "Resizing image:  3450\n",
      "Resizing image:  3500\n",
      "Resizing image:  3550\n",
      "Resizing image:  3600\n",
      "Resizing image:  3650\n",
      "Resizing image:  3700\n",
      "Resizing image:  3750\n",
      "Resizing image:  3800\n",
      "Resizing image:  3850\n",
      "Resizing image:  3900\n",
      "Resizing image:  3950\n",
      "Resizing image:  4000\n",
      "Resizing image:  4050\n",
      "Resizing image:  4100\n",
      "Resizing image:  4150\n",
      "Resizing image:  4200\n",
      "Resizing image:  4250\n",
      "Resizing image:  4300\n",
      "Resizing image:  4350\n",
      "Resizing image:  4400\n",
      "Resizing image:  4450\n",
      "Resizing image:  4500\n",
      "Resizing image:  4550\n",
      "Resizing image:  4600\n",
      "Resizing image:  4650\n",
      "Resizing image:  4700\n",
      "Resizing image:  4750\n",
      "Resizing image:  4800\n",
      "Resizing image:  4850\n",
      "Resizing image:  4900\n",
      "Resizing image:  4950\n",
      "Resizing image:  5000\n",
      "Resizing image:  5050\n",
      "Resizing image:  5100\n",
      "Resizing image:  5150\n",
      "Resizing image:  5200\n",
      "Resizing image:  5250\n",
      "Resizing image:  5300\n",
      "Resizing image:  5350\n",
      "Resizing image:  5400\n",
      "Resizing image:  5450\n",
      "Resizing image:  5500\n",
      "Resizing image:  5550\n",
      "Resizing image:  5600\n",
      "Resizing image:  5650\n",
      "Resizing image:  5700\n",
      "Resizing image:  5750\n",
      "Resizing image:  5800\n",
      "Resizing image:  5850\n",
      "Resizing image:  5900\n",
      "Resizing image:  5950\n",
      "Resizing image:  6000\n",
      "Resizing image:  6050\n",
      "Resizing image:  6100\n",
      "Resizing image:  6150\n",
      "Resizing image:  6200\n",
      "Resizing image:  6250\n",
      "Resizing image:  6300\n",
      "Resizing image:  6350\n",
      "Resizing image:  6400\n",
      "Resizing image:  6450\n",
      "Resizing image:  6500\n",
      "Resizing image:  6550\n",
      "Resizing image:  6600\n",
      "Resizing image:  6650\n",
      "Resizing image:  6700\n",
      "Resizing image:  6750\n",
      "Resizing image:  6800\n",
      "Resizing image:  6850\n",
      "Resizing image:  6900\n",
      "Resizing image:  6950\n",
      "Resizing image:  7000\n",
      "Resizing image:  7050\n",
      "Resizing image:  7100\n",
      "Resizing image:  7150\n",
      "Resizing image:  7200\n",
      "Resizing image:  7250\n",
      "Resizing image:  7300\n",
      "Resizing image:  7350\n",
      "Resizing image:  7400\n",
      "Resizing image:  7450\n",
      "Resizing image:  7500\n",
      "Resizing image:  7550\n",
      "Resizing image:  7600\n",
      "Resizing image:  7650\n",
      "Resizing image:  7700\n",
      "Resizing image:  7750\n",
      "Resizing image:  7800\n",
      "Resizing image:  7850\n",
      "Resizing image:  7900\n",
      "Resizing image:  7950\n",
      "Resizing image:  8000\n",
      "Resizing image:  8050\n",
      "Resizing image:  8100\n",
      "Resizing image:  8150\n",
      "Resizing image:  8200\n",
      "Resizing image:  8250\n",
      "Resizing image:  8300\n",
      "Resizing image:  8350\n",
      "Resizing image:  8400\n",
      "Resizing image:  8450\n",
      "Resizing image:  8500\n",
      "Resizing image:  8550\n",
      "Resizing image:  8600\n",
      "Resizing image:  8650\n",
      "Resizing image:  8700\n",
      "Resizing image:  8750\n",
      "Resizing image:  8800\n",
      "Resizing image:  8850\n",
      "Resizing image:  8900\n",
      "Resizing image:  8950\n",
      "Resizing image:  9000\n",
      "Resizing image:  9050\n",
      "Resizing image:  9100\n",
      "Resizing image:  9150\n",
      "Resizing image:  9200\n",
      "Resizing image:  9250\n",
      "Resizing image:  9300\n",
      "Resizing image:  9350\n",
      "Resizing image:  9400\n",
      "Resizing image:  9450\n",
      "Resizing image:  9500\n",
      "Resizing image:  9550\n",
      "Resizing image:  9600\n",
      "Resizing image:  9650\n",
      "Resizing image:  9700\n",
      "Resizing image:  9750\n",
      "Resizing image:  9800\n",
      "Resizing image:  9850\n",
      "Resizing image:  9900\n",
      "Resizing image:  9950\n",
      "Resizing image:  10000\n",
      "Resizing image:  10050\n",
      "Resizing image:  10100\n",
      "Resizing image:  10150\n",
      "Resizing image:  10200\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = df['breed']\n",
    "\n",
    "print('Resize h: ', res_dim_h)\n",
    "print('Resize w: ', res_dim_w)\n",
    "\n",
    "for i in df.index:\n",
    "    if i%200 == 0:\n",
    "        print(\"Resizing image: \", i)\n",
    "    id_i = df.id[i]\n",
    "    img = io.imread(os.path.join(rootdir_trainValData, id_i+'.jpg'))\n",
    "    # plt.figure(figsize=[10,10])\n",
    "    # plt.imshow(img)\n",
    "    img_res = resizeToDesiredShape(img, res_dim_h, res_dim_w)\n",
    "    # plt.figure(figsize=[10, 10])\n",
    "    # plt.imshow(img_res.astype('uint8'))\n",
    "    X.append(img_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize h:  200\n",
      "Resize w:  200\n"
     ]
    }
   ],
   "source": [
    "print('Resize h: ', res_dim_h)\n",
    "print('Resize w: ', res_dim_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizedImagesFilename = 'dog_breed_identification_resizedImages_to_'+str(res_dim_h)+'h_'+str(res_dim_w)+'w.h5'\n",
    "# print(resizedImagesFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save data to HDF5 file (commented to avoid running again)\n",
    "\"\"\"\n",
    "\n",
    "# hf = h5py.File(resizedImagesFilename, 'w')  # Create file with write(w) access\n",
    "# hf.create_dataset('X_train_val', data=X)\n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "y_train_val = pd.DataFrame(df['breed'], columns=['breed'])     # Convert series(df['breed'] to a dataframe)\n",
    "\n",
    "\"\"\"\n",
    "CONVERT y TO NUMBERS\n",
    "\"\"\"\n",
    "\n",
    "unique_y_count = y_train_val.breed.nunique()\n",
    "print(unique_y_count)\n",
    "\n",
    "unique_y = y_train_val.breed.unique()\n",
    "print(len(unique_y))\n",
    "\n",
    "# dummy_label_y_df = pd.DataFrame(columns=['ActualLabel', 'DummyLabel'])\n",
    "dummy_label_y_df = pd.DataFrame(unique_y, columns=['breed'])\n",
    "dummy_label_y_df['DummyLabel'] = dummy_label_y_df.index\n",
    "dummy_label_y_df\n",
    "\n",
    "\n",
    "y_train_val = pd.merge(y_train_val, dummy_label_y_df, on=['breed'], how='inner')\n",
    "\n",
    "\"\"\"\n",
    "Save y_train_val to file\n",
    "\"\"\"\n",
    "# y_train_val.to_csv('y_breed_dummyLabel.csv')     # Commenting to avoid overwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE\n",
    "Saved the training/validation images into a .h5 file\n",
    "Created a mapping of target column to the numeric label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in .h5 file:  <KeysViewHDF5 ['X_train_val']>\n",
      "(10222, 200, 200, 3)\n",
      "(10222,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "READ .h5\n",
    "\"\"\"\n",
    "h5f = h5py.File(resizedImagesFilename, 'r')\n",
    "print(\"Keys in .h5 file: \", h5f.keys())\n",
    "X_train_val = h5f['X_train_val'][:]\n",
    "h5f.close()\n",
    "\n",
    "\"\"\"\n",
    "RE READ Y\n",
    "\"\"\"\n",
    "df = pd.read_csv('y_breed_dummyLabel.csv')\n",
    "y_train_val = df.DummyLabel\n",
    "\n",
    "print(X_train_val.shape)\n",
    "print(y_train_val.shape)\n",
    "print(type(y_train_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train Validation Split\n",
    "\"\"\"\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3)\n",
    "\n",
    "\"\"\"\n",
    "Normalize X\n",
    "\"\"\"\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "- Test and validation data preperation\n",
    "\n",
    "## Start\n",
    "- Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, backend\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 120\n",
    "epochs = 10\n",
    "input_shape = (res_dim_h, res_dim_w, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buile CNN\n",
    "\n",
    "#Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "#Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,name='conv_1'))\n",
    "\n",
    "#Add a Convolutional Layer with 64 filters of size 3X3 and activation function as 'ReLU' \n",
    "model.add(Conv2D(64, (3, 3), activation='relu',name='conv_2'))\n",
    "\n",
    "#Add a MaxPooling Layer of size 2X2 \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name='max_1'))\n",
    "\n",
    "#Apply Dropout with 0.25 probability \n",
    "model.add(Dropout(0.25,name='drop_1'))\n",
    "\n",
    "#Flatten the layer\n",
    "model.add(Flatten())\n",
    "\n",
    "#Add Fully Connected Layer with 128 units and activation function as 'ReLU'\n",
    "model.add(Dense(64, activation='relu',name='dense_1'))\n",
    "#Apply Dropout with 0.5 probability \n",
    "model.add(Dropout(0.5,name='drop_2'))\n",
    "\n",
    "#Add Fully Connected Layer with 10 units and activation function as 'softmax'\n",
    "model.add(Dense(num_classes, activation='softmax',name='dense_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use adam optimizer for learning weights with learning rate = 0.001\n",
    "optimizer = Adam(lr=0.001)\n",
    "#Set the loss function and optimizer for the model training\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/10\n",
      "7155/7155 [==============================] - 185s 26ms/sample - loss: 4.9686 - accuracy: 0.0077 - val_loss: 4.7862 - val_accuracy: 0.0108\n",
      "Epoch 2/10\n",
      "7155/7155 [==============================] - 171s 24ms/sample - loss: 4.7856 - accuracy: 0.0124 - val_loss: 4.7854 - val_accuracy: 0.0117\n",
      "Epoch 3/10\n",
      "7155/7155 [==============================] - 172s 24ms/sample - loss: 4.7826 - accuracy: 0.0126 - val_loss: 4.7847 - val_accuracy: 0.0117\n",
      "Epoch 4/10\n",
      "7155/7155 [==============================] - 171s 24ms/sample - loss: 4.7811 - accuracy: 0.0126 - val_loss: 4.7843 - val_accuracy: 0.0117\n",
      "Epoch 5/10\n",
      "7155/7155 [==============================] - 171s 24ms/sample - loss: 4.7798 - accuracy: 0.0126 - val_loss: 4.7842 - val_accuracy: 0.0117\n",
      "Epoch 6/10\n",
      "7155/7155 [==============================] - 171s 24ms/sample - loss: 4.7788 - accuracy: 0.0126 - val_loss: 4.7840 - val_accuracy: 0.0117\n",
      "Epoch 7/10\n",
      "7155/7155 [==============================] - 171s 24ms/sample - loss: 4.7781 - accuracy: 0.0126 - val_loss: 4.7841 - val_accuracy: 0.0117\n",
      "Epoch 8/10\n",
      "7155/7155 [==============================] - 142s 20ms/sample - loss: 4.7775 - accuracy: 0.0126 - val_loss: 4.7842 - val_accuracy: 0.0117\n",
      "Epoch 9/10\n",
      "7155/7155 [==============================] - 65s 9ms/sample - loss: 4.7770 - accuracy: 0.0126 - val_loss: 4.7844 - val_accuracy: 0.0117\n",
      "Epoch 10/10\n",
      "7155/7155 [==============================] - 56s 8ms/sample - loss: 4.7766 - accuracy: 0.0126 - val_loss: 4.7845 - val_accuracy: 0.0117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29cc69b4ac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_cat,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augumentation to increase the data set for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData augumentation\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data augumentation\n",
    "\"\"\"\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.01,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.01,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Prepare the generator\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with augumented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "224/223 [==============================] - 178s 794ms/step - loss: 4.7763 - accuracy: 0.0126 - val_loss: 4.7846 - val_accuracy: 0.0117\n",
      "Epoch 2/10\n",
      "224/223 [==============================] - 176s 786ms/step - loss: 4.7761 - accuracy: 0.0126 - val_loss: 4.7847 - val_accuracy: 0.0117\n",
      "Epoch 3/10\n",
      "224/223 [==============================] - 169s 756ms/step - loss: 4.7758 - accuracy: 0.0126 - val_loss: 4.7849 - val_accuracy: 0.0117\n",
      "Epoch 4/10\n",
      "224/223 [==============================] - 165s 735ms/step - loss: 4.7757 - accuracy: 0.0126 - val_loss: 4.7852 - val_accuracy: 0.0117\n",
      "Epoch 5/10\n",
      "224/223 [==============================] - 160s 716ms/step - loss: 4.7756 - accuracy: 0.0126 - val_loss: 4.7854 - val_accuracy: 0.0117\n",
      "Epoch 6/10\n",
      "224/223 [==============================] - 163s 728ms/step - loss: 4.7755 - accuracy: 0.0126 - val_loss: 4.7855 - val_accuracy: 0.0117\n",
      "Epoch 7/10\n",
      "224/223 [==============================] - 186s 832ms/step - loss: 4.7753 - accuracy: 0.0126 - val_loss: 4.7857 - val_accuracy: 0.0117\n",
      "Epoch 8/10\n",
      "224/223 [==============================] - 165s 735ms/step - loss: 4.7754 - accuracy: 0.0126 - val_loss: 4.7858 - val_accuracy: 0.0117\n",
      "Epoch 9/10\n",
      "224/223 [==============================] - 166s 740ms/step - loss: 4.7753 - accuracy: 0.0126 - val_loss: 4.7860 - val_accuracy: 0.0117\n",
      "Epoch 10/10\n",
      "224/223 [==============================] - 164s 732ms/step - loss: 4.7752 - accuracy: 0.0126 - val_loss: 4.7861 - val_accuracy: 0.0117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29e6960eb88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(X_train, y_train_cat, batch_size=batch_size),\n",
    "          steps_per_epoch=X_train.shape[0]/batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(X_val, y_val_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelData/cnn_svhn_' +str(res_dim_h) + 'h_' + str(res_dim_w) + 'w.h5')\n",
    "model.save_weights('modelData/cnn_svhn_weights_' +str(res_dim_h) + 'h_' + str(res_dim_w) + 'w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = datagen.flow(X_train[:1], batch_size=1)\n",
    "# for i in range(1, 6):\n",
    "#     plt.figure(figsize=[10,10])\n",
    "#     plt.subplot(1,5,i)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(gen.next().squeeze(), cmap='gray')\n",
    "#     plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POC\n",
    "Take a few images (say 10) and try to create a model with high pixel images.\n",
    "    - If the model works that would mean that the memory used by image as well\n",
    "    - If the model fails that would mean that the memory constraing is due to model size\n",
    "\n",
    "Also try using existing Keras models.\n",
    "    - Found ResNet50 in Keras, which seems to be already trained for this data set.\n",
    "    - Since the purpose of this project is to train or transfer training, hence not using ResNet50 as such.\n",
    "      Instead, trying to create a transfer learning project. These existing Keras models seem to be using lot of memory.\n",
    "      Therefore, trying to use DataGenerator custom object to efficiently work on the memory.\n",
    "      However, the custom DataGenerator usage seems to be running through following error\n",
    "      - UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, \n",
    "        so try looking to see if a warning log message was printed above. [Op:Conv2D]\n",
    "        \n",
    "        Unable to get to the root of the above error.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune InceptionV3 on a new set of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "dog_breed_identification_resizedImages_to_224h_224w.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import skimage.io as io\n",
    "#Importing opencv module for the resizing function\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3,allow_growth=True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(\n",
    "                                                            memory_limit=3072)])\n",
    "\n",
    "\"\"\"\n",
    "VARIABLES\n",
    "\"\"\"\n",
    "\n",
    "rootdir_trainValData = 'D:\\\\workingDirectory\\\\PROJECTS\\\\12 - PROJECT - R8 - 02 - Computer Vision with CNN\\\\dog-breed-identification\\\\train'\n",
    "\n",
    "res_dim_h = 224\n",
    "res_dim_w = 224\n",
    "\n",
    "resizedImagesFilename_np = 'dog_breed_identification_resizedImages_to_'+str(res_dim_h)+'h_'+str(res_dim_w)+'w.npy'\n",
    "print(resizedImagesFilename_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize h:  224\n",
      "Resize w:  224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10222/10222 [03:40<00:00, 46.38it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "df.head()\n",
    "\n",
    "\"\"\"\n",
    "RESIZE FUNCTION\n",
    "\"\"\"\n",
    "\n",
    "def resizeToDesiredShape(inputImage, h, w):\n",
    "    # Seperate the r, g, b layers\n",
    "    r_array = np.array(inputImage[:, :, 0])\n",
    "    g_array = np.array(inputImage[:, :, 1])\n",
    "    b_array = np.array(inputImage[:, :, 2])\n",
    "    \n",
    "    # Apply resize on each layer (r, g, b)\n",
    "    r_res = cv2.resize(r_array, dsize=(h, w), interpolation=cv2.INTER_CUBIC)\n",
    "    g_res = cv2.resize(r_array, dsize=(h, w), interpolation=cv2.INTER_CUBIC)\n",
    "    b_res = cv2.resize(r_array, dsize=(h, w), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Prepare image using the resized r, g, b\n",
    "    outputImage = np.array([r_res, g_res, b_res])  # Put the reshaped values into a array\n",
    "    outputImage = np.transpose(outputImage, (1, 2, 0))  # Swap the dimensions of the array\n",
    "\n",
    "    return outputImage\n",
    "\n",
    "\"\"\"\n",
    "Loop through images and invoke the resize function\n",
    "\"\"\"\n",
    "\n",
    "X = []\n",
    "# Y = df['breed']\n",
    "Y = []\n",
    "\n",
    "print('Resize h: ', res_dim_h)\n",
    "print('Resize w: ', res_dim_w)\n",
    "\n",
    "#for i in df.index:\n",
    "for i in tqdm(df.index):\n",
    "    id_i = df.id[i]\n",
    "    img_path = os.path.join(rootdir_trainValData, id_i+'.jpg')\n",
    "    \n",
    "    if path.exists(img_path):\n",
    "        \n",
    "        #img_res = image.load_img(img_path, target_size=(res_dim_h, res_dim_w))\n",
    "        \n",
    "        img = io.imread(img_path)\n",
    "        img_res = resizeToDesiredShape(img, res_dim_h, res_dim_w)\n",
    "        X.append(img_res)\n",
    "        \n",
    "        \n",
    "        id_i_y = df[df.id==id_i].breed\n",
    "        Y.append(id_i_y[id_i_y.index[0]])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Save to npy file\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html\n",
    "\"\"\"\n",
    "\n",
    "np.save(resizedImagesFilename_np, X)\n",
    "\n",
    "pd.DataFrame(Y, columns=['breed']).to_csv('Y.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Y.csv')\n",
    "y_train_val = pd.DataFrame(df['breed'], columns=['breed'])     # Convert series(df['breed'] to a dataframe)\n",
    "\n",
    "\"\"\"\n",
    "CONVERT y TO NUMBERS\n",
    "\"\"\"\n",
    "\n",
    "unique_y_count = y_train_val.breed.nunique()\n",
    "print(unique_y_count)\n",
    "\n",
    "unique_y = y_train_val.breed.unique()\n",
    "print(len(unique_y))\n",
    "\n",
    "# dummy_label_y_df = pd.DataFrame(columns=['ActualLabel', 'DummyLabel'])\n",
    "dummy_label_y_df = pd.DataFrame(unique_y, columns=['breed'])\n",
    "dummy_label_y_df['DummyLabel'] = dummy_label_y_df.index\n",
    "dummy_label_y_df\n",
    "\n",
    "\n",
    "y_train_val = pd.merge(y_train_val, dummy_label_y_df, on=['breed'], how='inner')\n",
    "\n",
    "\"\"\"\n",
    "Save y_train_val to file\n",
    "\"\"\"\n",
    "y_train_val.to_csv('y_breed_dummyLabel.csv')     # Commenting to avoid overwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code referenced from: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\"\"\"\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    \"\"\"\n",
    "    We put as arguments relevant information about the data, such as dimension sizes (e.g. a volume of length 32 \n",
    "    will have dim=(32,32,32)), number of channels, number of classes, batch size, or decide whether we want to \n",
    "    shuffle our data at generation. We also store important information such as labels and the list of IDs that \n",
    "    we wish to generate at each pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(224,224), n_channels=3,\n",
    "             n_classes=120, shuffle=True):\n",
    "        print(\"Calling __init__\")\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \"\"\"\n",
    "        Mem-map the stored array, and then access the second row directly from disk:\n",
    "        \"\"\"\n",
    "        self.xOnDisk = np.load(resizedImagesFilename_np, mmap_mode='r')\n",
    "    \n",
    "    \"\"\"\n",
    "    Here, the method on_epoch_end is triggered once at the very beginning as well as at the end of each epoch. \n",
    "    If the shuffle parameter is set to True, we will get a new order of exploration at each pass (or just keep \n",
    "    a linear exploration scheme otherwise).\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self):\n",
    "        print(\"Calling on_epoch_end\")\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Another method that is core to the generation process is the one that achieves the most crucial job: producing \n",
    "    batches of data. The private method in charge of this task is called __data_generation and takes as argument \n",
    "    the list of IDs of the target batch.\n",
    "    \n",
    "    During data generation, this code reads the NumPy array of each example from its corresponding file ID.npy. \n",
    "    Since our code is multicore-friendly, note that you can do more complex operations instead (e.g. computations \n",
    "    from source files) without worrying that data generation becomes a bottleneck in the training process.\n",
    "    \n",
    "    Also, please note that we used Keras' keras.utils.to_categorical function to convert our numerical labels \n",
    "    stored in y to a binary form (e.g. in a 6-class problem, the third label corresponds to [0 0 1 0 0 0]) suited \n",
    "    for classification.\n",
    "    \"\"\"\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        print(\"Calling __data_generation\")\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            #X[i,] = np.load('data/' + ID + '.npy')\n",
    "            X[i,] = self.xOnDisk[ID]\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    Now comes the part where we build up all these components together. Each call requests a batch index between \n",
    "    0 and the total number of batches, where the latter is specified in the __len__ method.\n",
    "    \"\"\"\n",
    "    def __len__(self):\n",
    "        print(\"Calling __len__\")\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    \"\"\"\n",
    "    Now, when the batch corresponding to a given index is called, the generator executes the __getitem__ method \n",
    "    to generate it.\n",
    "    \n",
    "    For each iteration, the input 'index' is incremented by one so that the next batch is selected for processing.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        print(\"Calling __getitem__\")\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        #X = list_IDs_temp\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 10219 10220 10221]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(7155,)\n",
      "(3067,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RE READ Y\n",
    "\"\"\"\n",
    "df = pd.read_csv('y_breed_dummyLabel.csv')\n",
    "y_train_val = df.DummyLabel\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Find train and validation indexs\n",
    "\"\"\"\n",
    "allIndexes = df.index.values\n",
    "trainIndexes = df.sample(frac=0.7).index.values\n",
    "valIndexes = df[~df.index.isin(trainIndexes)].index.values\n",
    "\n",
    "print(allIndexes)\n",
    "print(type(trainIndexes))\n",
    "print(type(valIndexes))\n",
    "print(trainIndexes.shape)\n",
    "print(valIndexes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "#num_classes = 200\n",
    "#num_classes = len(unique_y)\n",
    "num_classes = len(y_train_val.unique())\n",
    "epochs = 2\n",
    "input_shape = (res_dim_h, res_dim_w, 3)\n",
    "\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load existing model\n",
    "\"\"\"\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(100, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling __init__\n",
      "Calling on_epoch_end\n",
      "Calling __init__\n",
      "Calling on_epoch_end\n",
      "Calling __len__\n",
      "Calling __len__\n",
      "Epoch 1/2\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n",
      "Calling __getitem__\n",
      "Calling __data_generation\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-541ba8cc77ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m model.fit_generator(generator=trainingGenerator,\n\u001b[0;32m      8\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidationGenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                    epochs=2, verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m               training=training))\n\u001b[0m\u001b[0;32m    253\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m   \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m   1132\u001b[0m           call_from_convolution=False)\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m     \u001b[1;31m# copybara:strip_end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[1;31m# copybara:insert return self.conv_op(inp, filter)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         name=self.name)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   2008\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m                            name=name)\n\u001b[0m\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1029\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   1032\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[0;32m   1128\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0;32m   1129\u001b[0m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[1;32m-> 1130\u001b[1;33m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[0;32m   1131\u001b[0m   _execute.record_gradient(\n\u001b[0;32m   1132\u001b[0m       \"Conv2D\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "# trainingGenerator = Mygenerator(X_train, y_train_cat, batch_size)\n",
    "# validationGenerator = Mygenerator(X_val, y_val_cat, batch_size)\n",
    "\n",
    "trainingGenerator = DataGenerator(trainIndexes, y_train_val, batch_size)\n",
    "validationGenerator = DataGenerator(valIndexes, y_train_val, batch_size)\n",
    "\n",
    "model.fit_generator(generator=trainingGenerator,\n",
    "                   validation_data=validationGenerator,\n",
    "                   epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
