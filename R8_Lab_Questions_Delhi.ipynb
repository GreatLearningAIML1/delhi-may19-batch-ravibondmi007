{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFfDTfhlaEI_"
   },
   "source": [
    "# Transfer Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNwbqCFRaEJC"
   },
   "source": [
    "* Train a simple convnet on the MNIST dataset the first 5 digits [0-4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5-9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Dnzaw1i5sXF"
   },
   "source": [
    "## MNIST Dataset\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images taken from American Census Bureau employees and American high school students. The MNIST dataset is one of the most common datasets used for image classification and accessible from many different sources. In fact, even Tensorflow and Keras allow us to import and download the MNIST dataset directly from their API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpUULy1Z5sXF"
   },
   "source": [
    "Let's import keras and load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiG7IPhm5sXG"
   },
   "outputs": [],
   "source": [
    "# Initialize the random number generator\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5482,
     "status": "ok",
     "timestamp": 1566722328661,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "ZqUiJM_Z5sXL",
    "outputId": "c34563d3-8d95-454a-9c19-2278074700c9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import backend\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVw4wsuW5sXO"
   },
   "source": [
    "X_train and X_test contain greyscale RGB codes (from 0 to 255) while y_train and y_test contains labels from 0 to 9 which represents which number they actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgQ86Vhw5sXP"
   },
   "source": [
    "Let's visualize some numbers using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5473,
     "status": "ok",
     "timestamp": 1566722328664,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "VZwg00gO5sXQ",
    "outputId": "d86db025-bdc3-406a-a48b-e65b05919dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18b6190f908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"Label: {}\".format(y_train[1000]))\n",
    "plt.imshow(X_train[1000], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p64mhwp95sXS"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxNAiWYd5sXT"
   },
   "source": [
    "### Create two datasets\n",
    "- First having digits from 0 to 4\n",
    "- Second having digits from 5 to 9\n",
    "\n",
    "Hint: use labels to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1807m1CL5sXT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index01234 = np.where(y_train <= 4)\n",
    "train_index56789 = np.where(y_train > 4)\n",
    "test_index01234 = np.where(y_test <= 4)\n",
    "test_index56789 = np.where(y_test > 4)\n",
    "\n",
    "X_train_01234 = X_train[train_index01234]\n",
    "y_train_01234 = y_train[train_index01234]\n",
    "\n",
    "X_train_56789 = X_train[train_index56789]\n",
    "y_train_56789 = y_train[train_index56789]\n",
    "\n",
    "X_test_01234 = X_test[test_index01234]\n",
    "y_test_01234 = y_test[test_index01234]\n",
    "\n",
    "X_test_56789 = X_test[test_index56789]\n",
    "y_test_56789 = y_test[test_index56789]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9jKcF1z5sXV"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMo7lvwQ5sXW"
   },
   "source": [
    "### Print shape of the data\n",
    "- print shape of all variables of both the datasets you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kH7ZjEoH5sXW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_01234:  (30596, 28, 28)\n",
      "y_train_01234:  (30596,)\n",
      "X_train_56789:  (29404, 28, 28)\n",
      "y_train_56789:  (29404,)\n",
      "X_test_01234:  (5139, 28, 28)\n",
      "y_test_01234:  (5139,)\n",
      "X_test_56789:  (4861, 28, 28)\n",
      "y_test_56789:  (4861,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_01234: \", X_train_01234.shape)\n",
    "print(\"y_train_01234: \", y_train_01234.shape)\n",
    "print(\"X_train_56789: \", X_train_56789.shape)\n",
    "print(\"y_train_56789: \", y_train_56789.shape)\n",
    "print(\"X_test_01234: \", X_test_01234.shape)\n",
    "print(\"y_test_01234: \", y_test_01234.shape)\n",
    "print(\"X_test_56789: \", X_test_56789.shape)\n",
    "print(\"y_test_56789: \", y_test_56789.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUU4PkKU5sXY"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I8ajqdt5sXY"
   },
   "source": [
    "### Reshape data\n",
    "- reshape first dataset\n",
    "- To be able to use the dataset in Keras, we need 4-dims numpy arrays. \n",
    "- reshape features to pass it to a Conv2D layer\n",
    "- channel = 1\n",
    "- reshape features of first dataset only\n",
    "- do not reshape labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38wgBEcz5sXa"
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#Keras expects data to be in the format (N_E.N_H,N_W,N_C)  \n",
    "# N_E = Number of Examples, \n",
    "# N_H = height, \n",
    "# N_W = Width, \n",
    "# N_C = Number of Channels.\n",
    "X_train_01234 = X_train_01234.reshape(X_train_01234.shape[0], img_rows, img_cols, 1)\n",
    "X_test_01234 = X_test_01234.reshape(X_test_01234.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5H-BtNm5sXg"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ahCMtCl5sXh"
   },
   "source": [
    "### Normalize data\n",
    "- normalize first dataset\n",
    "- we must normalize our data as it is always required in neural network models\n",
    "- we can achieve this by dividing the RGB codes to 255 (which is the maximum RGB code minus the minimum RGB code)\n",
    "- normalize X_train and X_test\n",
    "- make sure that the values are float so that we can get decimal points after division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Without type casting, the normalize code gives error: \n",
    "    \n",
    "    TypeError: No loop matching the specified signature and casting\n",
    "    was found for ufunc true_divide\n",
    "\n",
    "\"\"\"\n",
    "X_train_01234 = X_train_01234.astype('float32')\n",
    "X_test_01234 = X_test_01234.astype('float32')\n",
    "\n",
    "#Normalizing the input\n",
    "X_train_01234 /= 255\n",
    "X_test_01234 /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfQ6545D5sXp"
   },
   "source": [
    "### Print shape of data and number of images\n",
    "- for first dataset\n",
    "- print shape of X_train\n",
    "- print number of images in X_train\n",
    "- print number of images in X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQfQXZMo5sXp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_01234 shape: (30596, 28, 28)\n",
      "30596 train samples\n",
      "5139 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train_01234 shape:', X_train_01234.shape)\n",
    "print(X_train_01234.shape[0], 'train samples')\n",
    "print(X_test_01234.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oFjomSh5sXu"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lEFQQNk5sXu"
   },
   "source": [
    "### One-hot encode the class vector\n",
    "- encode labels of first dataset\n",
    "- convert class vectors (integers) to binary class matrix\n",
    "- convert y_train and y_test\n",
    "- number of classes: 5\n",
    "- we are doing this to use categorical_crossentropy as loss\n",
    "\n",
    "Hint: you can use keras.utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aejx3Zb35sXv"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "num_classes = 5  # 0, 1, 2, 3, 4\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train_01234 = tensorflow.keras.utils.to_categorical(y_train_01234, num_classes)\n",
    "y_test_01234 = tensorflow.keras.utils.to_categorical(y_test_01234, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30596, 5)\n",
      "(5139, 5)\n",
      "[1. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_01234.shape)\n",
    "print(y_test_01234.shape)\n",
    "print(y_train_01234[0])\n",
    "print(y_test_01234[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlkiipRA5sXw"
   },
   "source": [
    "## Question 6\n",
    "We will build our model by using high level Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzYMC_xm5sXx"
   },
   "source": [
    "### Initialize a sequential model\n",
    "- define a sequential model\n",
    "- add 2 convolutional layers\n",
    "    - no of filters: 32\n",
    "    - kernel size: 3x3\n",
    "    - activation: \"relu\"\n",
    "    - input shape: (28, 28, 1) for first layer\n",
    "- add a max pooling layer of size 2x2\n",
    "- add a dropout layer\n",
    "    - dropout layers fight with the overfitting by disregarding some of the neurons while training\n",
    "    - use dropout rate 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "#Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,name='conv_1'))\n",
    "\n",
    "#Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
    "model.add(Conv2D(32, (3, 3), activation='relu',name='conv_2'))\n",
    "\n",
    "#Add a MaxPooling Layer of size 2X2 \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name='max_1'))\n",
    "\n",
    "#Apply Dropout with 0.20 probability \n",
    "model.add(Dropout(0.20,name='drop_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2RaPWiP5sXz"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ajGIM6t5sXz"
   },
   "source": [
    "### Add classification layers\n",
    "- do this after doing question 6\n",
    "- flatten the data\n",
    "    - add Flatten later\n",
    "    - flatten layers flatten 2D arrays to 1D array before building the fully connected layers\n",
    "- add 2 dense layers\n",
    "    - number of neurons in first layer: 128\n",
    "    - number of neurons in last layer: number of classes\n",
    "    - activation function in first layer: relu\n",
    "    - activation function in last layer: softmax\n",
    "    - we may experiment with any number of neurons for the first Dense layer; however, the final Dense layer must have neurons equal to the number of output classes\n",
    "- you can add a dropout layer in between, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBxWAN265sX0"
   },
   "outputs": [],
   "source": [
    "#Flatten the layer\n",
    "model.add(Flatten())\n",
    "\n",
    "#Add Fully Connected Layer with 128 units and activation function as 'ReLU'\n",
    "model.add(Dense(128, activation='relu',name='dense_1'))\n",
    "\n",
    "#Apply Dropout with 0.5 probability \n",
    "model.add(Dropout(0.5,name='drop_2'))\n",
    "\n",
    "#Add Fully Connected Layer with 10 units and activation function as 'softmax'\n",
    "model.add(Dense(num_classes, activation='softmax',name='dense_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhtm4d5K5sX1"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmXg8EaF5sX2"
   },
   "source": [
    "### Compile and fit the model\n",
    "- compile your model\n",
    "    - loss: \"categorical_crossentropy\"\n",
    "    - metrics: \"accuracy\"\n",
    "    - optimizer: \"sgd\"\n",
    "- fit your model\n",
    "    - give train data - features and labels\n",
    "    - batch size: 128\n",
    "    - epochs: 10\n",
    "    - give validation data - features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDr-HKl-5sXx"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgclxC8s5sX4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "#To use adam optimizer for learning weights with learning rate = 0.001\n",
    "optimizer = SGD(lr=0.001)\n",
    "#Set the loss function and optimizer for the model training\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/10\n",
      "30596/30596 [==============================] - 15s 504us/sample - loss: 1.5910 - accuracy: 0.2830 - val_loss: 1.5593 - val_accuracy: 0.4777\n",
      "Epoch 2/10\n",
      "30596/30596 [==============================] - 8s 268us/sample - loss: 1.5285 - accuracy: 0.4602 - val_loss: 1.4640 - val_accuracy: 0.6495\n",
      "Epoch 3/10\n",
      "30596/30596 [==============================] - 8s 257us/sample - loss: 1.3902 - accuracy: 0.6017 - val_loss: 1.2272 - val_accuracy: 0.8017\n",
      "Epoch 4/10\n",
      "30596/30596 [==============================] - 8s 264us/sample - loss: 1.0706 - accuracy: 0.7382 - val_loss: 0.7372 - val_accuracy: 0.8963\n",
      "Epoch 5/10\n",
      "30596/30596 [==============================] - 8s 269us/sample - loss: 0.6697 - accuracy: 0.8241 - val_loss: 0.3760 - val_accuracy: 0.9288\n",
      "Epoch 6/10\n",
      "30596/30596 [==============================] - 8s 264us/sample - loss: 0.4495 - accuracy: 0.8692 - val_loss: 0.2413 - val_accuracy: 0.9469\n",
      "Epoch 7/10\n",
      "30596/30596 [==============================] - 8s 265us/sample - loss: 0.3488 - accuracy: 0.8941 - val_loss: 0.1866 - val_accuracy: 0.9556\n",
      "Epoch 8/10\n",
      "30596/30596 [==============================] - 8s 260us/sample - loss: 0.2998 - accuracy: 0.9081 - val_loss: 0.1598 - val_accuracy: 0.9599\n",
      "Epoch 9/10\n",
      "30596/30596 [==============================] - 8s 265us/sample - loss: 0.2676 - accuracy: 0.9176 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
      "Epoch 10/10\n",
      "30596/30596 [==============================] - 8s 264us/sample - loss: 0.2467 - accuracy: 0.9240 - val_loss: 0.1307 - val_accuracy: 0.9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18b4f41f948>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset\n",
    "model.fit(X_train_01234, y_train_01234,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_01234, y_test_01234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSVZUu3p5sX5"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5TQ3yLV5sX6"
   },
   "source": [
    "### Evaluate model\n",
    "- evaluate your model and get accuracy\n",
    "- use test features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBvuD3ba5sX7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.13072554838752487\n",
      "Test accuracy: 0.96400076\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on test set\n",
    "score = model.evaluate(X_test_01234, y_test_01234, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Set the path where you want to store the model and weights. \n",
    "# model.save('cnn_svhn_01234.h5')\n",
    "# model.save_weights('cnn_svhn_01234_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aUzOh9m5sX-"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srd-YYNH5sX-"
   },
   "source": [
    "## Transfer learning\n",
    "Now we will apply this model on second dataset (5-9 digits)\n",
    "\n",
    "- fix the first convolution layers so that the weights in the convolution layers dont get updated in the process of training\n",
    "- get the second dataset\n",
    "- train the last 2 dense layers\n",
    "- predict the accuracy and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvhdH7D55sYA"
   },
   "source": [
    "### Make only dense layers trainable\n",
    "- set trainalble = False for all layers other than Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brN7VZHFaEJ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mconv_1\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mconv_2\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mmax_1\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdrop_1\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mflatten\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mflatten_1\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense_1\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n",
      "\u001b[34mdrop_2\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense_2\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Freezing layers in the model which don't have 'dense' in their name\n",
    "for layer in model.layers:\n",
    "  if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n",
    "    #Freezing a layer\n",
    "    layer.trainable = False\n",
    "\n",
    "#Module to print colourful statements\n",
    "from termcolor import colored\n",
    "\n",
    "#Check which layers have been frozen \n",
    "for layer in model.layers:\n",
    "  print (colored(layer.name, 'blue'))\n",
    "  print (colored(layer.trainable, 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The pre-trained weights must exist in a folder called \"data\" in the current folder\n",
    "model.load_weights('cnn_svhn_01234_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.001)\n",
    "#Set the loss function and optimizer for the model training\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYR9VGzO5sYE"
   },
   "source": [
    "### Modify data\n",
    "- in your second data, class labels will start from 5 to 9 but for keras.utils.to_categorical the labels should start from 0\n",
    "- so you need to subtract 5 from train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # 5, 6, 7, 8, 9\n",
    "\n",
    "\"\"\"\n",
    "For using in to_categorical(), the values should start from 0.\n",
    "However, in the 2nd dataset, the values start from 5.\n",
    "Therefore, subtract 5 from each label in the 2nd dataset.\n",
    "\"\"\"\n",
    "y_train_56789 = y_train_56789 - 5\n",
    "y_test_56789 = y_test_56789 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 0 ... 0 1 3]\n",
      "[2 4 0 ... 4 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_56789)\n",
    "print(y_test_56789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGY3OTBt5sYG"
   },
   "source": [
    "### Reshape data\n",
    "- reshape second dataset\n",
    "- To be able to use the dataset in Keras, we need 4-dims numpy arrays. \n",
    "- reshape features to pass it to a Conv2D layer\n",
    "- channel = 1\n",
    "- reshape features of first dataset only\n",
    "- do not reshape labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREPROCESS SECOND DATA SET FOR TRAINING\n",
    "\"\"\"\n",
    "\n",
    "#Keras expects data to be in the format (N_E.N_H,N_W,N_C)  \n",
    "# N_E = Number of Examples, \n",
    "# N_H = height, \n",
    "# N_W = Width, \n",
    "# N_C = Number of Channels.\n",
    "X_train_56789 = X_train_56789.reshape(X_train_56789.shape[0], img_rows, img_cols, 1)\n",
    "X_test_56789 = X_test_56789.reshape(X_test_56789.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7omqMQH5sYJ"
   },
   "source": [
    "### Normalize data\n",
    "- normalize second data\n",
    "- we must normalize our data as it is always required in neural network models\n",
    "- we can achieve this by dividing the RGB codes to 255 (which is the maximum RGB code minus the minimum RGB code)\n",
    "- normalize X_train and X_test\n",
    "- make sure that the values are float so that we can get decimal points after division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEFYNHRp5sYJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Without type casting, the normalize code gives error: \n",
    "    \n",
    "    TypeError: No loop matching the specified signature and casting\n",
    "    was found for ufunc true_divide\n",
    "\n",
    "\"\"\"\n",
    "X_train_01234 = X_train_01234.astype('float32')\n",
    "X_test_01234 = X_test_01234.astype('float32')\n",
    "\n",
    "#Normalizing the input\n",
    "X_train_01234 /= 255\n",
    "X_test_01234 /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OfdlF655sYM"
   },
   "source": [
    "### Print shape of data and number of images\n",
    "- print shape of X_train\n",
    "- print number of images in X_train\n",
    "- print number of images in X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZEkCQ-P5sYO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_56789 shape: (29404, 28, 28, 1)\n",
      "29404 train samples\n",
      "4861 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train_56789 shape:', X_train_56789.shape)\n",
    "print(X_train_56789.shape[0], 'train samples')\n",
    "print(X_test_56789.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_3-0Qwo5sYQ"
   },
   "source": [
    "### One-hot encode the class vector\n",
    "- convert class vectors (integers) to binary class matrix\n",
    "- convert y_train and y_test\n",
    "- number of classes: 5\n",
    "- we are doing this to use categorical_crossentropy as loss\n",
    "\n",
    "Hint: you can use keras.utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k46Me5Re5sYR"
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train_56789 = tensorflow.keras.utils.to_categorical(y_train_56789, num_classes)\n",
    "y_test_56789 = tensorflow.keras.utils.to_categorical(y_test_56789, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9xEoW515sYS"
   },
   "source": [
    "### Fit the model\n",
    "- give train data - features and labels\n",
    "- batch size: 128\n",
    "- epochs: 10\n",
    "- give validation data - features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29404 samples, validate on 4861 samples\n",
      "Epoch 1/10\n",
      "29404/29404 [==============================] - 5s 172us/sample - loss: 18.9848 - accuracy: 0.2969 - val_loss: 1.3669 - val_accuracy: 0.3637\n",
      "Epoch 2/10\n",
      "29404/29404 [==============================] - 4s 124us/sample - loss: 1.4226 - accuracy: 0.3353 - val_loss: 1.2122 - val_accuracy: 0.4024\n",
      "Epoch 3/10\n",
      "29404/29404 [==============================] - 4s 132us/sample - loss: 1.3423 - accuracy: 0.3693 - val_loss: 1.0685 - val_accuracy: 0.4775\n",
      "Epoch 4/10\n",
      "29404/29404 [==============================] - 4s 123us/sample - loss: 1.3046 - accuracy: 0.4006 - val_loss: 0.9583 - val_accuracy: 0.6363\n",
      "Epoch 5/10\n",
      "29404/29404 [==============================] - 4s 130us/sample - loss: 1.2470 - accuracy: 0.4592 - val_loss: 0.8889 - val_accuracy: 0.6830\n",
      "Epoch 6/10\n",
      "29404/29404 [==============================] - 4s 119us/sample - loss: 1.2045 - accuracy: 0.4834 - val_loss: 0.8657 - val_accuracy: 0.7005\n",
      "Epoch 7/10\n",
      "29404/29404 [==============================] - 4s 129us/sample - loss: 1.1809 - accuracy: 0.4884 - val_loss: 0.8565 - val_accuracy: 0.6902\n",
      "Epoch 8/10\n",
      "29404/29404 [==============================] - 4s 124us/sample - loss: 1.1621 - accuracy: 0.4989 - val_loss: 0.8223 - val_accuracy: 0.7077\n",
      "Epoch 9/10\n",
      "29404/29404 [==============================] - 4s 122us/sample - loss: 1.1411 - accuracy: 0.5123 - val_loss: 0.8691 - val_accuracy: 0.6869\n",
      "Epoch 10/10\n",
      "29404/29404 [==============================] - 4s 122us/sample - loss: 1.1316 - accuracy: 0.5130 - val_loss: 0.8095 - val_accuracy: 0.7221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18c1558b208>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset\n",
    "model.fit(X_train_56789, y_train_56789,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_56789, y_test_56789))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85ginrII5sYV"
   },
   "source": [
    "### Evaluate model\n",
    "- evaluate your model and get accuracy\n",
    "- use test features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k11-vrsm5sYW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8094658199785865\n",
      "Test accuracy: 0.7220737\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on test set\n",
    "score = model.evaluate(X_test_56789, y_test_56789, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTNhSDqn5sYY"
   },
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "The objective of the second problem is to perform Sentiment analysis from the tweets collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIWWfNks5sYa"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### Read the data\n",
    "- read tweets.csv\n",
    "- use latin encoding if it gives encoding error while loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv', encoding='latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39pqw0aE5sYe"
   },
   "source": [
    "### Drop null values\n",
    "- drop all the rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BF_69oyI5sYf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before drop:  (9093, 3)\n",
      "shape after drop:  (9093, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape before drop: \", df.shape)\n",
    "df.dropna()\n",
    "print(\"shape after drop: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bm4bDiy5sYg"
   },
   "source": [
    "### Print the dataframe\n",
    "- print initial 5 rows of the data\n",
    "- use df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ceSlvAVa5sYh",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcWfPVqG5sYi"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBbAeip_5sYj"
   },
   "source": [
    "### Preprocess data\n",
    "- convert all text to lowercase - use .lower()\n",
    "- select only numbers, alphabets, and #+_ from text - use re.sub()\n",
    "- strip all the text - use .strip()\n",
    "    - this is for removing extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PE4Bn_YT5sYj"
   },
   "outputs": [],
   "source": [
    "# Loop through each column and apply lowercase\n",
    "for columnLabel in df.columns:\n",
    "    df[columnLabel] = df[columnLabel].apply(lambda x: x if type(x) != str else x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlMvbtrK5sYl"
   },
   "source": [
    "print dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afocjaUn5sYm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 i have a 3g iphone. after 3 hrs twe...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee know about @fludapp ? awesome ipad/i...</td>\n",
       "      <td>ipad or iphone app</td>\n",
       "      <td>positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin can not wait for #ipad 2 also. the...</td>\n",
       "      <td>ipad</td>\n",
       "      <td>positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw i hope this year's festival isn't as cra...</td>\n",
       "      <td>ipad or iphone app</td>\n",
       "      <td>negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on fri #sxsw: marissa m...</td>\n",
       "      <td>google</td>\n",
       "      <td>positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 i have a 3g iphone. after 3 hrs twe...   \n",
       "1  @jessedee know about @fludapp ? awesome ipad/i...   \n",
       "2  @swonderlin can not wait for #ipad 2 also. the...   \n",
       "3  @sxsw i hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on fri #sxsw: marissa m...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iphone   \n",
       "1              ipad or iphone app   \n",
       "2                            ipad   \n",
       "3              ipad or iphone app   \n",
       "4                          google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   negative emotion  \n",
       "1                                   positive emotion  \n",
       "2                                   positive emotion  \n",
       "3                                   negative emotion  \n",
       "4                                   positive emotion  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcTUnvtg5sYn"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4gnaeSXZ5sYo"
   },
   "source": [
    "### Preprocess data\n",
    "- in column \"is_there_an_emotion_directed_at_a_brand_or_product\"\n",
    "    - select only those rows where value equal to \"positive emotion\" or \"negative emotion\"\n",
    "- find the value counts of \"positive emotion\" and \"negative emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLewJh_35sYp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative emotion', 'positive emotion',\n",
       "       'no emotion toward brand or product', \"i can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2]].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VFYB4eh5sYr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3548, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 i have a 3g iphone. after 3 hrs twe...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee know about @fludapp ? awesome ipad/i...</td>\n",
       "      <td>ipad or iphone app</td>\n",
       "      <td>positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin can not wait for #ipad 2 also. the...</td>\n",
       "      <td>ipad</td>\n",
       "      <td>positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw i hope this year's festival isn't as cra...</td>\n",
       "      <td>ipad or iphone app</td>\n",
       "      <td>negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on fri #sxsw: marissa m...</td>\n",
       "      <td>google</td>\n",
       "      <td>positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 i have a 3g iphone. after 3 hrs twe...   \n",
       "1  @jessedee know about @fludapp ? awesome ipad/i...   \n",
       "2  @swonderlin can not wait for #ipad 2 also. the...   \n",
       "3  @sxsw i hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on fri #sxsw: marissa m...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iphone   \n",
       "1              ipad or iphone app   \n",
       "2                            ipad   \n",
       "3              ipad or iphone app   \n",
       "4                          google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   negative emotion  \n",
       "1                                   positive emotion  \n",
       "2                                   positive emotion  \n",
       "3                                   negative emotion  \n",
       "4                                   positive emotion  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.loc[df.columns[2].isin('positive emotion', 'negative emotion')]\n",
    "\n",
    "df = df[(df[df.columns[2]] == 'positive emotion') | (df[df.columns[2]] == 'negative emotion')]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6icGcVTE5sYz"
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rg0rSepj5sYz"
   },
   "source": [
    "### Encode labels\n",
    "- in column \"is_there_an_emotion_directed_at_a_brand_or_product\"\n",
    "    - change \"positive emotion\" to 1\n",
    "    - change \"negative emotion\" to 0\n",
    "- use map function to replace values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 i have a 3g iphone. after 3 hrs twe...</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee know about @fludapp ? awesome ipad/i...</td>\n",
       "      <td>ipad or iphone app</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin can not wait for #ipad 2 also. the...</td>\n",
       "      <td>ipad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw i hope this year's festival isn't as cra...</td>\n",
       "      <td>ipad or iphone app</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on fri #sxsw: marissa m...</td>\n",
       "      <td>google</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 i have a 3g iphone. after 3 hrs twe...   \n",
       "1  @jessedee know about @fludapp ? awesome ipad/i...   \n",
       "2  @swonderlin can not wait for #ipad 2 also. the...   \n",
       "3  @sxsw i hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on fri #sxsw: marissa m...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iphone   \n",
       "1              ipad or iphone app   \n",
       "2                            ipad   \n",
       "3              ipad or iphone app   \n",
       "4                          google   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                                  0   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  0   \n",
       "4                                                  1   "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[2]] = df[df.columns[2]].apply(lambda x: 1 if x=='positive emotion' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sC1qSe3h5sY2"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWlAN_Ts5sY2"
   },
   "source": [
    "### Get feature and label\n",
    "- get column \"tweet_text\" as feature\n",
    "- get column \"is_there_an_emotion_directed_at_a_brand_or_product\" as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9A3sOZzR5sY4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 i have a 3g iphone. after 3 hrs twe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee know about @fludapp ? awesome ipad/i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin can not wait for #ipad 2 also. the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw i hope this year's festival isn't as cra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on fri #sxsw: marissa m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 i have a 3g iphone. after 3 hrs twe...   \n",
       "1  @jessedee know about @fludapp ? awesome ipad/i...   \n",
       "2  @swonderlin can not wait for #ipad 2 also. the...   \n",
       "3  @sxsw i hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on fri #sxsw: marissa m...   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                                  0   \n",
       "1                                                  1   \n",
       "2                                                  1   \n",
       "3                                                  0   \n",
       "4                                                  1   "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 2nd column (tells about the device)\n",
    "df.drop(df.columns[1], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### Create train and test data\n",
    "- use train_test_split to get train and test set\n",
    "- set a random_state\n",
    "- test_size: 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2661, 1)\n",
      "(2661,)\n",
      "(887, 1)\n",
      "(887,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(df.columns[1], axis=1)\n",
    "Y = df[df.columns[1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMok2IX35sY8"
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data\n",
    "- create document-term matrix\n",
    "- use CountVectorizer()\n",
    "    - ngram_range: (1, 2)\n",
    "    - stop_words: 'english'\n",
    "    - min_df: 2   \n",
    "- do fit_transform on X_train\n",
    "- do transform on X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCreate a new folder 'corpora' into one of these folders. \\nMove the zips downloaded from http://www.nltk.org/nltk_data/ into this folder. Or unzip the zip here.\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "nltk.data.path\n",
    "\"\"\"\n",
    "Create a new folder 'corpora' into one of these folders. \n",
    "Move the zips downloaded from http://www.nltk.org/nltk_data/ into this folder. Or unzip the zip here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "    Function to convert a raw review to a string of words.\n",
    "    The input is a single string (a raw movie review).\n",
    "    The output is a single string (a preprocessed movie review).\n",
    "\"\"\"\n",
    "def review_to_words(raw_review):\n",
    "    \n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2661\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in X_train.index:\n",
    "    # Call function for each one, and add the result to the list of clean reviews\n",
    "    inputDoc = X_train['tweet_text'][i]\n",
    "    clean_train_reviews.append(review_to_words(inputDoc))\n",
    "\n",
    "print(len(clean_train_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bb9PnnqT5sY8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "(2661, 4874)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating the bag of words...\\n\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "# train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Apply the vectorizer on the test data\n",
    "\"\"\"\n",
    "clean_test_reviews = []\n",
    "\n",
    "for i in X_test.index:\n",
    "    # Call function for each one, and add the result to the list of clean reviews\n",
    "    inputDoc = X_test['tweet_text'][i]\n",
    "    clean_test_reviews.append(review_to_words(inputDoc))\n",
    "\n",
    "print(len(clean_test_reviews))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qanDXve15sY_"
   },
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMaRNFkV5sY_"
   },
   "source": [
    "### Select classifier logistic regression\n",
    "- use logistic regression for predicting sentiment of the given tweet\n",
    "- initialize classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GT3dNgB55sZA"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model=LogisticRegression(penalty='l2', max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqQ6_HX35sZD"
   },
   "source": [
    "### Fit the classifer\n",
    "- fit logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIzvnNkq5sZD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZpMsYQF5sZF"
   },
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGnQnUww5sZF"
   },
   "source": [
    "### Select classifier naive bayes\n",
    "- use naive bayes for predicting sentiment of the given tweet\n",
    "- initialize classifier\n",
    "- use MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naiveBayes_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEaG942m5sZI"
   },
   "source": [
    "### Fit the classifer\n",
    "- fit naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLwRBj1R5sZI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naiveBayes_model.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7mgwYDJ5sZM"
   },
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZkA3tce5sZN"
   },
   "source": [
    "### Make predictions on logistic regression\n",
    "- use your trained logistic regression model to make predictions on X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3f0M1ch5sZO"
   },
   "outputs": [],
   "source": [
    "logistic_prediction=logistic_model.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrIxjMUB5sZQ"
   },
   "source": [
    "### Make predictions on naive bayes\n",
    "- use your trained naive bayes model to make predictions on X_test\n",
    "- use a different variable name to store predictions so that they are kept separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSQnwyLU5sZQ"
   },
   "outputs": [],
   "source": [
    "naiveBayes_prediction=naiveBayes_model.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwXQUE7b5sZS"
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6SITIE75sZT"
   },
   "source": [
    "### Calculate accuracy of logistic regression\n",
    "- check accuracy of logistic regression classifer\n",
    "- use sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clv2X0kKH0Ok"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8759864712514093"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, logistic_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Fd_Gnd05sZV"
   },
   "source": [
    "### Calculate accuracy of naive bayes\n",
    "- check accuracy of naive bayes classifer\n",
    "- use sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d32uBpHi5sZW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7474633596392334"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, naiveBayes_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRAS\n",
    "\n",
    "Use random forest classifier for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.8741094308805722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "print(\"Training the random forest...\")\n",
    "forest = forest.fit(train_data_features, y_train)\n",
    "\n",
    "# random forest performance through cross vaidation \n",
    "print(forest)\n",
    "print(np.mean(cross_val_score(forest,train_data_features,y_train,cv=10)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R8_Internal_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
