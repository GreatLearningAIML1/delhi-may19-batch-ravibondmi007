{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCE: https://www.kaggle.com/rtatman/blog-authorship-corpus\n",
    "\n",
    "\n",
    "# Context:\n",
    "“A blog (a truncation of the expression \"weblog\") is a discussion or informational website published on the World Wide Web consisting of discrete, often informal diary-style text entries (\"posts\"). Posts are typically displayed in reverse chronological order, so that the most recent post appears first, at the top of the web page. Until 2009, blogs were usually the work of a single individual, occasionally of a small group, and often covered a single subject or topic.” -- Wikipedia article “Blog”\n",
    "\n",
    "This dataset contains text from blogs written on or before 2004, with each blog being the work of a single user.\n",
    "\n",
    "\n",
    "# Content:\n",
    "The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.\n",
    "\n",
    "Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)\n",
    "\n",
    "All bloggers included in the corpus fall into one of three age groups:\n",
    "\n",
    "- 8240 \"10s\" blogs (ages 13-17),\n",
    "- 8086 \"20s\" blogs(ages 23-27)\n",
    "- 2994 \"30s\" blogs (ages 33-47).\n",
    "\n",
    "For each age group there are an equal number of male and female bloggers.\n",
    "\n",
    "Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink.\n",
    "\n",
    "\n",
    "# Acknowledgements\n",
    "The corpus may be freely used for non-commercial research purposes. Any resulting publications should cite the following:\n",
    "\n",
    "J. Schler, M. Koppel, S. Argamon and J. Pennebaker (2006). Effects of Age and Gender on Blogging in Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. URL: http://www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf\n",
    "\n",
    "\n",
    "# Inspiration:\n",
    "- This dataset contains information on writers demographics, including their age, gender and zodiac sign. Can you build a classifier to guess someone’s zodiac sign from blog posts they’ve written?\n",
    "- Which are bigger: differences between demographic groups or differences between blogs on different topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach and steps\n",
    "1. Load the dataset (5 points)\n",
    "    - Tip: As the dataset is large, use fewer rows. Check what is working well on your machine and decide accordingly.\n",
    "2. Preprocess rows of the “text” column (7.5 points)\n",
    "    - Remove unwanted characters\n",
    "    - Convert text to lowercase\n",
    "    - Remove unwanted spaces\n",
    "    - Remove stopwords\n",
    "3. As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)\n",
    "    - Label columns to merge: “gender”, “age”, “topic”, “sign”\n",
    "    - After completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image\n",
    "4. Separate features and labels, and split the data into training and testing (5 points)\n",
    "5. Vectorize the features (5 points)\n",
    "    - Create a Bag of Words using count vectorizer\n",
    "        - Use ngram_range=(1, 2)\n",
    "        - Vectorize training and testing features\n",
    "    - Print the term-document matrix\n",
    "6. Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference (5 points)\n",
    "7. Transform the labels - (7.5 points)\n",
    "    As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
    "    - Convert your train and test labels using MultiLabelBinarizer\n",
    "8. Choose a classifier - (5 points)\n",
    "    In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression . It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large.\n",
    "    - Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label\n",
    "    - As One-vs-Rest approach might not have been discussed in the sessions, we are providing you the code for that\n",
    "9. Fit the classifier, make predictions and get the accuracy (5 points)\n",
    "    - Print the following\n",
    "        - Accuracy score\n",
    "        - F1 score\n",
    "        - Average precision score\n",
    "        - Average recall score\n",
    "        - Tip: Make sure you are familiar with all of them. How would you expect the things to work for the multi-label scenario? Read about micro/macro/weighted averaging\n",
    "10. Print true label and predicted label for any five examples (7.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTIONING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "Load the dataset (5 points)\n",
    "- Tip: As the dataset is large, use fewer rows. Check what is working well on your machine and decide accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('blogtext.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "Preprocess rows of the “text” column (7.5 points)\n",
    "- Remove unwanted characters\n",
    "- Convert text to lowercase\n",
    "- Remove unwanted spaces\n",
    "- Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Drop rows with null values\"\"\"\n",
    "print(\"shape before drop: \", df.shape)\n",
    "df.dropna()\n",
    "print(\"shape after drop: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply \n",
    "- lowercase to each string column (using .lower)\n",
    "- strip of any extra leading or trailing spaces (using .strip)\n",
    "\"\"\"\n",
    "for columnLabel in df.columns:\n",
    "    df[columnLabel] = df[columnLabel].apply(lambda x: x if type(x) != str else x.lower().strip())\n",
    "\n",
    "#df.to_csv('data_lower_striped.csv', index=False)   # Commented to re run the save to file\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from lower_striped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# df = pd.read_csv('data_lower_striped.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"select only numbers, alphabets, and #+_ from text\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9#+_.]\", \" \", raw_review) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "# \n",
    "\n",
    "testString = \"This is a test 1.1 string testing. 123, # and ? and ~ and _ and + and others like @ ! $ % ^ * = *** ??? or @@@\"\n",
    "print(review_to_words(testString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to hold the clean reviews\n",
    "# #clean_train_reviews = []\n",
    "\n",
    "# for i in tqdm(df.index):\n",
    "#     # Call function for each one, and add the result to the list of clean reviews\n",
    "#     inputDoc = df['text'][i]\n",
    "#     #clean_train_reviews.append(review_to_words(inputDoc))\n",
    "#     df['text'][i] = review_to_words(inputDoc)\n",
    "\n",
    "# df.to_csv('data_lower_striped_onlyNumAlpha_noStopWords.csv', index=False)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process from data_lower_striped_onlyNumAlpha_noStopWords.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>student</td>\n",
       "      <td>leo</td>\n",
       "      <td>14,may,2004</td>\n",
       "      <td>info found + 100 pages 4.5 mb .pdf files wait ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>student</td>\n",
       "      <td>leo</td>\n",
       "      <td>13,may,2004</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>student</td>\n",
       "      <td>leo</td>\n",
       "      <td>12,may,2004</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>student</td>\n",
       "      <td>leo</td>\n",
       "      <td>12,may,2004</td>\n",
       "      <td>testing testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>investmentbanking</td>\n",
       "      <td>aquarius</td>\n",
       "      <td>11,june,2004</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups...whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            student       leo   14,may,2004   \n",
       "1  2059027   male   15            student       leo   13,may,2004   \n",
       "2  2059027   male   15            student       leo   12,may,2004   \n",
       "3  2059027   male   15            student       leo   12,may,2004   \n",
       "4  3581210   male   33  investmentbanking  aquarius  11,june,2004   \n",
       "\n",
       "                                                text  \n",
       "0  info found + 100 pages 4.5 mb .pdf files wait ...  \n",
       "1  team members drewes van der laag urllink mail ...  \n",
       "2  het kader van kernfusie op aarde maak je eigen...  \n",
       "3                                    testing testing  \n",
       "4  thanks yahoo toolbar capture urls popups...whi...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('data_lower_striped_onlyNumAlpha_noStopWords.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "26\n",
      "40\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(df.gender.nunique())\n",
    "print(df.age.nunique())\n",
    "print(df.topic.nunique())\n",
    "print(df.sign.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "\n",
    "- As we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points)\n",
    "    - Label columns to merge: “gender”, “age”, “topic”, “sign”\n",
    "    - After completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>temp_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>info found + 100 pages 4.5 mb .pdf files wait ...</td>\n",
       "      <td>male, 15, student, leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "      <td>male, 15, student, leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>male, 15, student, leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>testing testing</td>\n",
       "      <td>male, 15, student, leo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups...whi...</td>\n",
       "      <td>male, 33, investmentbanking, aquarius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  info found + 100 pages 4.5 mb .pdf files wait ...   \n",
       "1  team members drewes van der laag urllink mail ...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups...whi...   \n",
       "\n",
       "                             temp_labels  \n",
       "0                 male, 15, student, leo  \n",
       "1                 male, 15, student, leo  \n",
       "2                 male, 15, student, leo  \n",
       "3                 male, 15, student, leo  \n",
       "4  male, 33, investmentbanking, aquarius  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(temp_labels = df.gender + ', ' + df.age.astype(str) + ', ' + df.topic + ', ' + df.sign)\n",
    "\n",
    "df.drop(['id', 'gender', 'age', 'topic', 'sign', 'date'], axis=1, inplace=True)\n",
    "df.head()\n",
    "\n",
    "# df.to_csv('data_lower_striped_onlyNumAlpha_noStopWords_ExtraColumnsRemoved.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# import collections\n",
    "# my_dict = collections.defaultdict(int)\n",
    "\n",
    "# for i in tqdm(df.index):\n",
    "#     for lab in df.temp_labels[i].strip().split():\n",
    "#         my_dict[lab] += 1\n",
    "\n",
    "# # Print the dictionary\n",
    "# my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "\n",
    "Separate features and labels, and split the data into training and testing (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>info found + 100 pages 4.5 mb .pdf files wait ...</td>\n",
       "      <td>[male, 15, student, leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "      <td>[male, 15, student, leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, student, leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, student, leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups...whi...</td>\n",
       "      <td>[male, 33, investmentbanking, aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  info found + 100 pages 4.5 mb .pdf files wait ...   \n",
       "1  team members drewes van der laag urllink mail ...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups...whi...   \n",
       "\n",
       "                                    labels  \n",
       "0                 [male, 15, student, leo]  \n",
       "1                 [male, 15, student, leo]  \n",
       "2                 [male, 15, student, leo]  \n",
       "3                 [male, 15, student, leo]  \n",
       "4  [male, 33, investmentbanking, aquarius]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "\n",
    "\n",
    "df = pd.read_csv('data_lower_striped_onlyNumAlpha_noStopWords_ExtraColumnsRemoved.csv')\n",
    "df['labels'] = df.temp_labels.apply(lambda x: x.split(', '))\n",
    "df.drop(['temp_labels'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training records count:  (76302,)\n",
      "validation records count:  (32702,)\n",
      "testing records count:  (27252,)\n"
     ]
    }
   ],
   "source": [
    "X = df.text\n",
    "y = df.labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exclude = 0.80\n",
    "\n",
    "# Some data will be not used\n",
    "X, X_notUsed, y, y_notUsed = train_test_split(X, y, test_size=exclude)\n",
    "\n",
    "# 20% is for testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 30% of remaining is for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3)\n",
    "\n",
    "print('training records count: ', X_train.shape)\n",
    "print('validation records count: ', X_val.shape)\n",
    "print('testing records count: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5\n",
    "\n",
    "- Vectorize the features (5 points)\n",
    "    - Create a Bag of Words using count vectorizer\n",
    "        - Use ngram_range=(1, 2)\n",
    "        - Vectorize training and testing features\n",
    "    - Print the term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  76.8528673\n",
      "(76302, 5000)\n"
     ]
    }
   ],
   "source": [
    "# 6 minutes for 2000 features\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectors = 5000\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's \n",
    "# bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = vectors,\n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train.astype('U'))\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "X_train_vectorized = X_train_vectorized.toarray()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time taken: ', stop - start)\n",
    "\n",
    "print(X_train_vectorized.shape)\n",
    "# (381518, 486259)   # Full train data set for ngram_range=(1,1)\n",
    "# (381518, 14735264) # Full train data set for ngram_range=(1,2)\n",
    "# (381518, 46840509) # Full train data set for ngram_range=(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrint the term document matrix\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print the term document matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  14.019949400000002\n"
     ]
    }
   ],
   "source": [
    "# 2 minutes for 2000 features\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "X_val_vectorized = vectorizer.transform(X_val.astype('U'))\n",
    "X_val_vectorized = X_val_vectorized.toarray()\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time taken: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6\n",
    "Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 681284/681284 [00:45<00:00, 14832.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'female') :  336091\n",
      "(0, 'male') :  345193\n",
      "(1, '13') :  13133\n",
      "(1, '14') :  27400\n",
      "(1, '15') :  41767\n",
      "(1, '16') :  72708\n",
      "(1, '17') :  80859\n",
      "(1, '23') :  72889\n",
      "(1, '24') :  80071\n",
      "(1, '25') :  67051\n",
      "(1, '26') :  55312\n",
      "(1, '27') :  46124\n",
      "(1, '33') :  17584\n",
      "(1, '34') :  21347\n",
      "(1, '35') :  17462\n",
      "(1, '36') :  14229\n",
      "(1, '37') :  9317\n",
      "(1, '38') :  7545\n",
      "(1, '39') :  5556\n",
      "(1, '40') :  5016\n",
      "(1, '41') :  3738\n",
      "(1, '42') :  2908\n",
      "(1, '43') :  4230\n",
      "(1, '44') :  2044\n",
      "(1, '45') :  4482\n",
      "(1, '46') :  2733\n",
      "(1, '47') :  2207\n",
      "(1, '48') :  3572\n",
      "(2, 'accounting') :  3832\n",
      "(2, 'advertising') :  4676\n",
      "(2, 'agriculture') :  1235\n",
      "(2, 'architecture') :  1638\n",
      "(2, 'arts') :  32449\n",
      "(2, 'automotive') :  1244\n",
      "(2, 'banking') :  4049\n",
      "(2, 'biotech') :  2234\n",
      "(2, 'businessservices') :  4500\n",
      "(2, 'chemicals') :  3928\n",
      "(2, 'communications-media') :  20140\n",
      "(2, 'construction') :  1093\n",
      "(2, 'consulting') :  5862\n",
      "(2, 'education') :  29633\n",
      "(2, 'engineering') :  11653\n",
      "(2, 'environment') :  592\n",
      "(2, 'fashion') :  4851\n",
      "(2, 'government') :  6907\n",
      "(2, 'humanresources') :  3010\n",
      "(2, 'indunk') :  251015\n",
      "(2, 'internet') :  16006\n",
      "(2, 'investmentbanking') :  1292\n",
      "(2, 'law') :  9040\n",
      "(2, 'lawenforcement-security') :  1878\n",
      "(2, 'manufacturing') :  2272\n",
      "(2, 'maritime') :  280\n",
      "(2, 'marketing') :  4769\n",
      "(2, 'military') :  3128\n",
      "(2, 'museums-libraries') :  3096\n",
      "(2, 'non-profit') :  14700\n",
      "(2, 'publishing') :  7753\n",
      "(2, 'realestate') :  2870\n",
      "(2, 'religion') :  5235\n",
      "(2, 'science') :  7269\n",
      "(2, 'sports-recreation') :  3038\n",
      "(2, 'student') :  153903\n",
      "(2, 'technology') :  42055\n",
      "(2, 'telecommunications') :  3891\n",
      "(2, 'tourism') :  1942\n",
      "(2, 'transportation') :  2326\n",
      "(3, 'aquarius') :  49687\n",
      "(3, 'aries') :  64979\n",
      "(3, 'cancer') :  65048\n",
      "(3, 'capricorn') :  49201\n",
      "(3, 'gemini') :  51985\n",
      "(3, 'leo') :  53811\n",
      "(3, 'libra') :  62363\n",
      "(3, 'pisces') :  54053\n",
      "(3, 'sagittarius') :  50036\n",
      "(3, 'scorpio') :  57161\n",
      "(3, 'taurus') :  62561\n",
      "(3, 'virgo') :  60399\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import collections\n",
    "label_dict = collections.defaultdict(int)\n",
    "\n",
    "for i in tqdm(df.index):\n",
    "    #for lab in str(df.labels[i]).strip().split():\n",
    "    for lab in enumerate(df.labels[i]):\n",
    "        label_dict[lab] += 1\n",
    "\n",
    "# Print the dictionary\n",
    "for i in sorted (label_dict.keys()) :  \n",
    "     print(i, \": \", label_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take a look at the words in the vocabulary\n",
    "# vocab = vectorizer.get_feature_names()\n",
    "# print(vocab)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Sum up the counts of each vocabulary word\n",
    "# dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# # For each, print the vocabulary word and the number of times it \n",
    "# # appears in the training set\n",
    "# i = 0\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     i = i + 1\n",
    "#     print(i, \": \", tag, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7\n",
    "Transform the labels - (7.5 points)\n",
    "\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
    "    - Convert your train and test labels using MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13' '14' '15' '16' '17' '23' '24' '25' '26' '27' '33' '34' '35' '36'\n",
      " '37' '38' '39' '40' '41' '42' '43' '44' '45' '46' '47' '48' 'accounting'\n",
      " 'advertising' 'agriculture' 'aquarius' 'architecture' 'aries' 'arts'\n",
      " 'automotive' 'banking' 'biotech' 'businessservices' 'cancer' 'capricorn'\n",
      " 'chemicals' 'communications-media' 'construction' 'consulting'\n",
      " 'education' 'engineering' 'environment' 'fashion' 'female' 'gemini'\n",
      " 'government' 'humanresources' 'indunk' 'internet' 'investmentbanking'\n",
      " 'law' 'lawenforcement-security' 'leo' 'libra' 'male' 'manufacturing'\n",
      " 'maritime' 'marketing' 'military' 'museums-libraries' 'non-profit'\n",
      " 'pisces' 'publishing' 'realestate' 'religion' 'sagittarius' 'science'\n",
      " 'scorpio' 'sports-recreation' 'student' 'taurus' 'technology'\n",
      " 'telecommunications' 'tourism' 'transportation' 'virgo']\n",
      "\n",
      " (76302, 80)\n",
      "(27252, 80)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "lb = MultiLabelBinarizer().fit(df.labels)\n",
    "print(lb.classes_)\n",
    "\n",
    "\n",
    "y_train_binarized = lb.fit_transform(y_train)\n",
    "y_test_binarized = lb.fit_transform(y_test)\n",
    "\n",
    "print(\"\\n\", y_train_binarized.shape)\n",
    "print(y_test_binarized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8\n",
    "Choose a classifier - (5 points)\n",
    "- In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression . It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large.\n",
    "        a. Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label\n",
    "        b. As One-vs-Rest approach might not have been discussed in the sessions, we are providing you the code for that\n",
    "            \n",
    "            # CODE ---START---\n",
    "            from sklearn.multiclass import OneVsRestClassifier\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "            logReg = LogisticRegression(solver='lbfgs')\n",
    "            clf = OneVsRestClassifier(logReg)\n",
    "            # CODE ---COMPLETE---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import timeit\n",
    "\n",
    "iterations = 300\n",
    "\n",
    "logReg = LogisticRegression(solver='lbfgs', max_iter=iterations, verbose=1)\n",
    "clf = OneVsRestClassifier(logReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 9\n",
    "Fit the classifier, make predictions and get the accuracy (5 points)\n",
    "- Print the following\n",
    "    - Accuracy score\n",
    "    - F1 score\n",
    "    - Average precision score\n",
    "    - Average recall score\n",
    "    - Tip: Make sure you are familiar with all of them. How would you expect the things to work for the multi-label scenario? Read about micro/macro/weighted averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   50.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   42.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   47.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   49.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   48.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   49.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\installationDirectory\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "\"\"\"\n",
    "Fit the classifier\n",
    "\"\"\"\n",
    "clf.fit(X_train_vectorized, y_train_binarized)\n",
    "\n",
    "stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  8606.1222725\n"
     ]
    }
   ],
   "source": [
    "print('Time taken: ', stop - start)\n",
    "# @500 vectors (10% records)\n",
    "# 199 for 50\n",
    "# 324 for 100\n",
    "# 434 for 150\n",
    "# 509 for 200\n",
    "\n",
    "# @1000 vectors (10% records)\n",
    "# 658 for 200\n",
    "\n",
    "# @2000 vectors (20% records)\n",
    "# 2301 for 200\n",
    "\n",
    "# @10000 vectors (20% records)\n",
    "# 12966 for 200\n",
    "\n",
    "# @5000 vectors (20% records)\n",
    "# 8606 for 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write logic to get label with max probability in each category\n",
    "\"\"\"\n",
    "\n",
    "label_label_dict = collections.defaultdict(int)\n",
    "for i in sorted (label_dict.keys()):\n",
    "    label_label_dict[i[1]] += i[0]\n",
    "\n",
    "\"\"\"\n",
    "Take probabilities for each class as input, and, output classes with max probability in each category.\n",
    "\"\"\"\n",
    "def topPredictions(arrayOfProb):\n",
    "    out = ['', '', '', '']\n",
    "    out_prob = [0, 0, 0, 0]\n",
    "    for i in range(0, arrayOfProb.shape[0]):\n",
    "        item_i_prob = arrayOfProb[i] # Probability\n",
    "        item_i = lb.classes_[i] # Class label\n",
    "        label_label = label_label_dict[item_i] # Labels label (0, 1, 2, 3)\n",
    "        \n",
    "        if item_i_prob > out_prob[label_label]:\n",
    "            out_prob[label_label] = item_i_prob\n",
    "            out[label_label] = item_i\n",
    "            #print(item_i, \" of type \", label_label, \" has probability: \", item_i_prob)\n",
    "    return out, out_prob\n",
    "\n",
    "\"\"\"\n",
    "Take vectorized X as input.\n",
    "Do prediction using the classifier.\n",
    "Find the class with max probability in each category.\n",
    "\"\"\"\n",
    "def predictLabels(X_input_vectorized):\n",
    "    y_input_predict = clf.predict_proba(X_input_vectorized)\n",
    "    y_output_predict = []\n",
    "#     for i in range(0, len(y_input_predict)):\n",
    "    for i in tqdm(range(0, len(y_input_predict))):\n",
    "        y_temp_i, y_temp_i_prob = topPredictions(y_input_predict[i])\n",
    "        y_output_predict.append(y_temp_i)\n",
    "    \n",
    "    return y_output_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 32702/32702 [00:06<00:00, 5134.59it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Predict\n",
    "\"\"\"\n",
    "y_val_predict = predictLabels(X_val_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Accuracy\n",
    "# \"\"\"\n",
    "# # predictions = clf.predict(x_test)\n",
    "# score = clf.score(X_val_vectorized, y_val)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 10\n",
    "Print true label and predicted label for any five examples (7.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['male', '17', 'student', 'taurus'],\n",
       " ['male', '16', 'indunk', 'libra'],\n",
       " ['female', '48', 'marketing', 'aries'],\n",
       " ['male', '35', 'student', 'aquarius'],\n",
       " ['female', '23', 'indunk', 'taurus']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_predict[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426634            [female, 24, arts, libra]\n",
      "538641    [female, 25, engineering, taurus]\n",
      "205083           [male, 16, student, virgo]\n",
      "635379     [female, 23, student, capricorn]\n",
      "62141          [female, 16, student, aries]\n",
      "Name: labels, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_val[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA\n",
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8exclude_5000Vectors_300Iterations.sav\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "modelPrefix = 'model_clf_'\n",
    "tfidfPrefix = 'vectors_'\n",
    "filename = str(exclude) + \"exclude_\" + str(vectors) + \"Vectors_\" + str(iterations) + \"Iterations.sav\"\n",
    "print(filename)\n",
    "# filename = \"model_clf.sav\"\n",
    "pickle.dump(clf, open(modelPrefix + filename, 'wb'))\n",
    "pickle.dump(X_train_vectorized, open(tfidfPrefix + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male', '26', 'indunk', 'aries']\n"
     ]
    }
   ],
   "source": [
    "# Testing loading the model\n",
    "\n",
    "loaded_clf = pickle.load(open(\"model_clf_20PerData_5000Vectors_200Iterations.sav\", 'rb'))\n",
    "\n",
    "temp, temp_prob = topPredictions(loaded_clf.predict_proba(np.reshape(X_val_vectorized[4], (1, -1)))[0])\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
