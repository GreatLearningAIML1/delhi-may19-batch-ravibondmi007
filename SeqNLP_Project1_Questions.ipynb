{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "\n",
    "### Generate Word Embeddings and retrieve outputs of each layer with Keras based on Classification task\n",
    "\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
    "\n",
    "It is a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "We willl use the imdb dataset to learn word embeddings as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with sentiment (positive or negative). \n",
    "\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "`from keras.datasets import imdb`\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "\n",
    "### Aim\n",
    "\n",
    "1. Import test and train data  \n",
    "2. Import the labels ( train and test) \n",
    "3. Get the word index and then Create key value pair for word and word_id. (12.5 points)\n",
    "4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)\n",
    "5. Report the Accuracy of the model. (5 points)  \n",
    "6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "#### Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZhMAgaNeCy5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length review:  2494\n",
      "Max length review is at index:  17934\n",
      "Number of unique words in the overall data set:  88586\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
    "\n",
    "# Find max length of review\n",
    "maxLen = 0\n",
    "maxI = 0\n",
    "for i, id in enumerate(range(x_train.shape[0])):\n",
    "    length = len(x_train[i])\n",
    "    if length > maxLen:\n",
    "        maxLen = length\n",
    "        maxI = i\n",
    "\n",
    "print(\"Max length review: \", maxLen)\n",
    "print(\"Max length review is at index: \", maxI)\n",
    "\n",
    "# Find the max value\n",
    "maxValue = 0\n",
    "for i, id in enumerate(range(x_train.shape[0])):\n",
    "    x_temp = x_train[i]\n",
    "    if max(x_temp) >= maxValue:\n",
    "        maxValue = max(x_temp)\n",
    "\n",
    "print(\"Number of unique words in the overall data set: \", maxValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(25000,)\n",
      "2494\n",
      "189\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(len(x_train[17934]))\n",
    "print(len(x_train[1]))\n",
    "print(type(x_train[0]))\n",
    "# x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "# vocab_size = 10000 #vocab size\n",
    "# maxlen = 300  #number of word used from each review\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy6n-uM2eCy2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(25000, 300)\n",
      "300\n",
      "300\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n",
       "       4468,   66, 3941,    4,  173,   36,  256,    5,   25,  100,   43,\n",
       "        838,  112,   50,  670,    2,    9,   35,  480,  284,    5,  150,\n",
       "          4,  172,  112,  167,    2,  336,  385,   39,    4,  172, 4536,\n",
       "       1111,   17,  546,   38,   13,  447,    4,  192,   50,   16,    6,\n",
       "        147, 2025,   19,   14,   22,    4, 1920, 4613,  469,    4,   22,\n",
       "         71,   87,   12,   16,   43,  530,   38,   76,   15,   13, 1247,\n",
       "          4,   22,   17,  515,   17,   12,   16,  626,   18,    2,    5,\n",
       "         62,  386,   12,    8,  316,    8,  106,    5,    4, 2223, 5244,\n",
       "         16,  480,   66, 3785,   33,    4,  130,   12,   16,   38,  619,\n",
       "          5,   25,  124,   51,   36,  135,   48,   25, 1415,   33,    6,\n",
       "         22,   12,  215,   28,   77,   52,    5,   14,  407,   16,   82,\n",
       "          2,    8,    4,  107,  117, 5952,   15,  256,    4,    2,    7,\n",
       "       3766,    5,  723,   36,   71,   43,  530,  476,   26,  400,  317,\n",
       "         46,    7,    4,    2, 1029,   13,  104,   88,    4,  381,   15,\n",
       "        297,   98,   32, 2071,   56,   26,  141,    6,  194, 7486,   18,\n",
       "          4,  226,   22,   21,  134,  476,   26,  480,    5,  144,   30,\n",
       "       5535,   18,   51,   36,   28,  224,   92,   25,  104,    4,  226,\n",
       "         65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "        113,  103,   32,   15,   16, 5345,   19,  178,   32,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(len(x_train[0]))\n",
    "print(len(x_train[1]))\n",
    "print(type(x_train[0]))\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x_train[17934]))\n",
    "# x_train[17934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "# Find the max value\n",
    "maxValue = 0\n",
    "for i, id in enumerate(range(x_train.shape[0])):\n",
    "    x_temp = x_train[i]\n",
    "    if max(x_temp) >= maxValue:\n",
    "        maxValue = max(x_temp)\n",
    "\n",
    "print(maxValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assessment till now:\n",
    "\n",
    "- We have a array of numeric values, where each value represents a word (unknown).\n",
    "\n",
    "- The max value is 9999, meaning there are 10000 words identified. \n",
    "Otherwise, there are more than 80000 unique words in the data set.\n",
    "\n",
    "- Each sub-array has 300 values.\n",
    "\n",
    "- Each value in the sub-array is a numeric value representing some word in the vocabalary(unknown).\n",
    "\n",
    "- If the review was of less than 300 words, then the words set into towards the last of the array. \n",
    "The initial values in the array are set to 0.\n",
    "\n",
    "---***---\n",
    "\n",
    "To do next:\n",
    "\n",
    "- For each entry in the array, create a array of size (300, 10000).\n",
    "Each row will contain value in only one column.\n",
    "The index of the row in this array will represent the index of the word in the review.\n",
    "The column which contains will represent the word (as given in the input).\n",
    "As output of this step there will be (25000, 300, 10000) size data set.\n",
    "\n",
    "- Pass it through a neural network to train against the labels.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Get word to index mapping\n",
    "# It is a dictonary object\n",
    "# Key is the word\n",
    "# Value is the numeric value corresponding to the word\n",
    "word_to_id = imdb.get_word_index()\n",
    "print(type(word_to_id))  # dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n",
      "88584\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_id))\n",
    "print(len(word_to_id.items()))\n",
    "# word_to_id[0]  # Gives error, meaning indexing starts from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase numbers of the each word in the dictonary by 2\n",
    "word_to_id = {k:(v+2) for k, v in word_to_id.items()}\n",
    "# This leaves index 0, 1 and 2 available for use.\n",
    "# NOTE: 0 was not used in the input data, therefore increasing index of each word by 2, instead of 3.\n",
    "\n",
    "# First 3 index are empty now\n",
    "# This code must be after increasing number of each word in the dictonary by 3\n",
    "word_to_id[\"<PAD>\"]=0\n",
    "word_to_id[\"<START>\"]=1\n",
    "word_to_id[\"<END>\"]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict_items'>\n",
      "1\n",
      "88587\n",
      "88587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(word_to_id))\n",
    "print(type(word_to_id.items()))\n",
    "print(word_to_id[\"<START>\"])\n",
    "print(len(word_to_id))\n",
    "print(len(word_to_id.items()))\n",
    "word_to_id['<START>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{34703: 'fawn',\n",
       " 52008: 'tsukino',\n",
       " 52009: 'nunnery',\n",
       " 16818: 'sonja',\n",
       " 63953: 'vani',\n",
       " 1410: 'woods',\n",
       " 16117: 'spiders',\n",
       " 2347: 'hanging',\n",
       " 2291: 'woody',\n",
       " 52010: 'trawling',\n",
       " 52011: \"hold's\",\n",
       " 11309: 'comically',\n",
       " 40832: 'localized',\n",
       " 30570: 'disobeying',\n",
       " 52012: \"'royale\",\n",
       " 40833: \"harpo's\",\n",
       " 52013: 'canet',\n",
       " 19315: 'aileen',\n",
       " 52014: 'acurately',\n",
       " 52015: \"diplomat's\",\n",
       " 25244: 'rickman',\n",
       " 6748: 'arranged',\n",
       " 52016: 'rumbustious',\n",
       " 52017: 'familiarness',\n",
       " 52018: \"spider'\",\n",
       " 68806: 'hahahah',\n",
       " 52019: \"wood'\",\n",
       " 40835: 'transvestism',\n",
       " 34704: \"hangin'\",\n",
       " 2340: 'bringing',\n",
       " 40836: 'seamier',\n",
       " 34705: 'wooded',\n",
       " 52020: 'bravora',\n",
       " 16819: 'grueling',\n",
       " 1638: 'wooden',\n",
       " 16820: 'wednesday',\n",
       " 52021: \"'prix\",\n",
       " 34706: 'altagracia',\n",
       " 52022: 'circuitry',\n",
       " 11587: 'crotch',\n",
       " 57768: 'busybody',\n",
       " 52023: \"tart'n'tangy\",\n",
       " 14131: 'burgade',\n",
       " 52025: 'thrace',\n",
       " 11040: \"tom's\",\n",
       " 52027: 'snuggles',\n",
       " 29116: 'francesco',\n",
       " 52029: 'complainers',\n",
       " 52127: 'templarios',\n",
       " 40837: '272',\n",
       " 52030: '273',\n",
       " 52132: 'zaniacs',\n",
       " 34708: '275',\n",
       " 27633: 'consenting',\n",
       " 40838: 'snuggled',\n",
       " 15494: 'inanimate',\n",
       " 52032: 'uality',\n",
       " 11928: 'bronte',\n",
       " 4012: 'errors',\n",
       " 3232: 'dialogs',\n",
       " 52033: \"yomada's\",\n",
       " 34709: \"madman's\",\n",
       " 30587: 'dialoge',\n",
       " 52035: 'usenet',\n",
       " 40839: 'videodrome',\n",
       " 26340: \"kid'\",\n",
       " 52036: 'pawed',\n",
       " 30571: \"'girlfriend'\",\n",
       " 52037: \"'pleasure\",\n",
       " 52038: \"'reloaded'\",\n",
       " 40841: \"kazakos'\",\n",
       " 52039: 'rocque',\n",
       " 52040: 'mailings',\n",
       " 11929: 'brainwashed',\n",
       " 16821: 'mcanally',\n",
       " 52041: \"tom''\",\n",
       " 25245: 'kurupt',\n",
       " 21907: 'affiliated',\n",
       " 52042: 'babaganoosh',\n",
       " 40842: \"noe's\",\n",
       " 40843: 'quart',\n",
       " 361: 'kids',\n",
       " 5036: 'uplifting',\n",
       " 7095: 'controversy',\n",
       " 21908: 'kida',\n",
       " 23381: 'kidd',\n",
       " 52043: \"error'\",\n",
       " 52044: 'neurologist',\n",
       " 18512: 'spotty',\n",
       " 30572: 'cobblers',\n",
       " 9880: 'projection',\n",
       " 40844: 'fastforwarding',\n",
       " 52045: 'sters',\n",
       " 52046: \"eggar's\",\n",
       " 52047: 'etherything',\n",
       " 40845: 'gateshead',\n",
       " 34710: 'airball',\n",
       " 25246: 'unsinkable',\n",
       " 7182: 'stern',\n",
       " 52048: \"cervi's\",\n",
       " 40846: 'dnd',\n",
       " 11588: 'dna',\n",
       " 20600: 'insecurity',\n",
       " 52049: \"'reboot'\",\n",
       " 11039: 'trelkovsky',\n",
       " 52050: 'jaekel',\n",
       " 52051: 'sidebars',\n",
       " 52052: \"sforza's\",\n",
       " 17635: 'distortions',\n",
       " 52053: 'mutinies',\n",
       " 30604: 'sermons',\n",
       " 40848: '7ft',\n",
       " 52054: 'boobage',\n",
       " 52055: \"o'bannon's\",\n",
       " 23382: 'populations',\n",
       " 52056: 'chulak',\n",
       " 27635: 'mesmerize',\n",
       " 52057: 'quinnell',\n",
       " 10309: 'yahoo',\n",
       " 52059: 'meteorologist',\n",
       " 42579: 'beswick',\n",
       " 15495: 'boorman',\n",
       " 40849: 'voicework',\n",
       " 52060: \"ster'\",\n",
       " 22924: 'blustering',\n",
       " 52061: 'hj',\n",
       " 27636: 'intake',\n",
       " 5623: 'morally',\n",
       " 40851: 'jumbling',\n",
       " 52062: 'bowersock',\n",
       " 52063: \"'porky's'\",\n",
       " 16823: 'gershon',\n",
       " 40852: 'ludicrosity',\n",
       " 52064: 'coprophilia',\n",
       " 40853: 'expressively',\n",
       " 19502: \"india's\",\n",
       " 34712: \"post's\",\n",
       " 52065: 'wana',\n",
       " 5285: 'wang',\n",
       " 30573: 'wand',\n",
       " 25247: 'wane',\n",
       " 52323: 'edgeways',\n",
       " 34713: 'titanium',\n",
       " 40854: 'pinta',\n",
       " 180: 'want',\n",
       " 30574: 'pinto',\n",
       " 52067: 'whoopdedoodles',\n",
       " 21910: 'tchaikovsky',\n",
       " 2105: 'travel',\n",
       " 52068: \"'victory'\",\n",
       " 11930: 'copious',\n",
       " 22435: 'gouge',\n",
       " 52069: \"chapters'\",\n",
       " 6704: 'barbra',\n",
       " 30575: 'uselessness',\n",
       " 52070: \"wan'\",\n",
       " 27637: 'assimilated',\n",
       " 16118: 'petiot',\n",
       " 52071: 'most\\x85and',\n",
       " 3932: 'dinosaurs',\n",
       " 354: 'wrong',\n",
       " 52072: 'seda',\n",
       " 52073: 'stollen',\n",
       " 34714: 'sentencing',\n",
       " 40855: 'ouroboros',\n",
       " 40856: 'assimilates',\n",
       " 40857: 'colorfully',\n",
       " 27638: 'glenne',\n",
       " 52074: 'dongen',\n",
       " 4762: 'subplots',\n",
       " 52075: 'kiloton',\n",
       " 23383: 'chandon',\n",
       " 34715: \"effect'\",\n",
       " 27639: 'snugly',\n",
       " 40858: 'kuei',\n",
       " 9094: 'welcomed',\n",
       " 30073: 'dishonor',\n",
       " 52077: 'concurrence',\n",
       " 23384: 'stoicism',\n",
       " 14898: \"guys'\",\n",
       " 52079: \"beroemd'\",\n",
       " 6705: 'butcher',\n",
       " 40859: \"melfi's\",\n",
       " 30625: 'aargh',\n",
       " 20601: 'playhouse',\n",
       " 11310: 'wickedly',\n",
       " 1182: 'fit',\n",
       " 52080: 'labratory',\n",
       " 40861: 'lifeline',\n",
       " 1929: 'screaming',\n",
       " 4289: 'fix',\n",
       " 52081: 'cineliterate',\n",
       " 52082: 'fic',\n",
       " 52083: 'fia',\n",
       " 34716: 'fig',\n",
       " 52084: 'fmvs',\n",
       " 52085: 'fie',\n",
       " 52086: 'reentered',\n",
       " 30576: 'fin',\n",
       " 52087: 'doctresses',\n",
       " 52088: 'fil',\n",
       " 12608: 'zucker',\n",
       " 31933: 'ached',\n",
       " 52090: 'counsil',\n",
       " 52091: 'paterfamilias',\n",
       " 13887: 'songwriter',\n",
       " 34717: 'shivam',\n",
       " 9656: 'hurting',\n",
       " 301: 'effects',\n",
       " 52092: 'slauther',\n",
       " 52093: \"'flame'\",\n",
       " 52094: 'sommerset',\n",
       " 52095: 'interwhined',\n",
       " 27640: 'whacking',\n",
       " 52096: 'bartok',\n",
       " 8777: 'barton',\n",
       " 21911: 'frewer',\n",
       " 52097: \"fi'\",\n",
       " 6194: 'ingrid',\n",
       " 30577: 'stribor',\n",
       " 52098: 'approporiately',\n",
       " 52099: 'wobblyhand',\n",
       " 52100: 'tantalisingly',\n",
       " 52101: 'ankylosaurus',\n",
       " 17636: 'parasites',\n",
       " 52102: 'childen',\n",
       " 52103: \"jenkins'\",\n",
       " 52104: 'metafiction',\n",
       " 17637: 'golem',\n",
       " 40862: 'indiscretion',\n",
       " 23385: \"reeves'\",\n",
       " 57783: \"inamorata's\",\n",
       " 52106: 'brittannica',\n",
       " 7918: 'adapt',\n",
       " 30578: \"russo's\",\n",
       " 48248: 'guitarists',\n",
       " 10555: 'abbott',\n",
       " 40863: 'abbots',\n",
       " 17651: 'lanisha',\n",
       " 40865: 'magickal',\n",
       " 52107: 'mattter',\n",
       " 52108: \"'willy\",\n",
       " 34718: 'pumpkins',\n",
       " 52109: 'stuntpeople',\n",
       " 30579: 'estimate',\n",
       " 40866: 'ugghhh',\n",
       " 11311: 'gameplay',\n",
       " 52110: \"wern't\",\n",
       " 40867: \"n'sync\",\n",
       " 16119: 'sickeningly',\n",
       " 40868: 'chiara',\n",
       " 4013: 'disturbed',\n",
       " 40869: 'portmanteau',\n",
       " 52111: 'ineffectively',\n",
       " 82145: \"duchonvey's\",\n",
       " 37521: \"nasty'\",\n",
       " 1287: 'purpose',\n",
       " 52114: 'lazers',\n",
       " 28107: 'lightened',\n",
       " 52115: 'kaliganj',\n",
       " 52116: 'popularism',\n",
       " 18513: \"damme's\",\n",
       " 30580: 'stylistics',\n",
       " 52117: 'mindgaming',\n",
       " 46451: 'spoilerish',\n",
       " 52119: \"'corny'\",\n",
       " 34720: 'boerner',\n",
       " 6794: 'olds',\n",
       " 52120: 'bakelite',\n",
       " 27641: 'renovated',\n",
       " 27642: 'forrester',\n",
       " 52121: \"lumiere's\",\n",
       " 52026: 'gaskets',\n",
       " 886: 'needed',\n",
       " 34721: 'smight',\n",
       " 1299: 'master',\n",
       " 25907: \"edie's\",\n",
       " 40870: 'seeber',\n",
       " 52122: 'hiya',\n",
       " 52123: 'fuzziness',\n",
       " 14899: 'genesis',\n",
       " 12609: 'rewards',\n",
       " 30581: 'enthrall',\n",
       " 40871: \"'about\",\n",
       " 52124: \"recollection's\",\n",
       " 11041: 'mutilated',\n",
       " 52125: 'fatherlands',\n",
       " 52126: \"fischer's\",\n",
       " 5401: 'positively',\n",
       " 34707: '270',\n",
       " 34722: 'ahmed',\n",
       " 9838: 'zatoichi',\n",
       " 13888: 'bannister',\n",
       " 52129: 'anniversaries',\n",
       " 30582: \"helm's\",\n",
       " 52130: \"'work'\",\n",
       " 34723: 'exclaimed',\n",
       " 52131: \"'unfunny'\",\n",
       " 52031: '274',\n",
       " 546: 'feeling',\n",
       " 52133: \"wanda's\",\n",
       " 33268: 'dolan',\n",
       " 52135: '278',\n",
       " 52136: 'peacoat',\n",
       " 40872: 'brawny',\n",
       " 40873: 'mishra',\n",
       " 40874: 'worlders',\n",
       " 52137: 'protags',\n",
       " 52138: 'skullcap',\n",
       " 57598: 'dastagir',\n",
       " 5624: 'affairs',\n",
       " 7801: 'wholesome',\n",
       " 52139: 'hymen',\n",
       " 25248: 'paramedics',\n",
       " 52140: 'unpersons',\n",
       " 52141: 'heavyarms',\n",
       " 52142: 'affaire',\n",
       " 52143: 'coulisses',\n",
       " 40875: 'hymer',\n",
       " 52144: 'kremlin',\n",
       " 30583: 'shipments',\n",
       " 52145: 'pixilated',\n",
       " 30584: \"'00s\",\n",
       " 18514: 'diminishing',\n",
       " 1359: 'cinematic',\n",
       " 14900: 'resonates',\n",
       " 40876: 'simplify',\n",
       " 40877: \"nature'\",\n",
       " 40878: 'temptresses',\n",
       " 16824: 'reverence',\n",
       " 19504: 'resonated',\n",
       " 34724: 'dailey',\n",
       " 52146: '2\\x85',\n",
       " 27643: 'treize',\n",
       " 52147: 'majo',\n",
       " 21912: 'kiya',\n",
       " 52148: 'woolnough',\n",
       " 39799: 'thanatos',\n",
       " 35733: 'sandoval',\n",
       " 40881: 'dorama',\n",
       " 52149: \"o'shaughnessy\",\n",
       " 4990: 'tech',\n",
       " 32020: 'fugitives',\n",
       " 30585: 'teck',\n",
       " 76127: \"'e'\",\n",
       " 40883: 'doesn’t',\n",
       " 52151: 'purged',\n",
       " 659: 'saying',\n",
       " 41097: \"martians'\",\n",
       " 23420: 'norliss',\n",
       " 27644: 'dickey',\n",
       " 52154: 'dicker',\n",
       " 52155: \"'sependipity\",\n",
       " 8424: 'padded',\n",
       " 57794: 'ordell',\n",
       " 40884: \"sturges'\",\n",
       " 52156: 'independentcritics',\n",
       " 5747: 'tempted',\n",
       " 34726: \"atkinson's\",\n",
       " 25249: 'hounded',\n",
       " 52157: 'apace',\n",
       " 15496: 'clicked',\n",
       " 30586: \"'humor'\",\n",
       " 17179: \"martino's\",\n",
       " 52158: \"'supporting\",\n",
       " 52034: 'warmongering',\n",
       " 34727: \"zemeckis's\",\n",
       " 21913: 'lube',\n",
       " 52159: 'shocky',\n",
       " 7478: 'plate',\n",
       " 40885: 'plata',\n",
       " 40886: 'sturgess',\n",
       " 40887: \"nerds'\",\n",
       " 20602: 'plato',\n",
       " 34728: 'plath',\n",
       " 40888: 'platt',\n",
       " 52161: 'mcnab',\n",
       " 27645: 'clumsiness',\n",
       " 3901: 'altogether',\n",
       " 42586: 'massacring',\n",
       " 52162: 'bicenntinial',\n",
       " 40889: 'skaal',\n",
       " 14362: 'droning',\n",
       " 8778: 'lds',\n",
       " 21914: 'jaguar',\n",
       " 34729: \"cale's\",\n",
       " 1779: 'nicely',\n",
       " 4590: 'mummy',\n",
       " 18515: \"lot's\",\n",
       " 10088: 'patch',\n",
       " 50204: 'kerkhof',\n",
       " 52163: \"leader's\",\n",
       " 27646: \"'movie\",\n",
       " 52164: 'uncomfirmed',\n",
       " 40890: 'heirloom',\n",
       " 47362: 'wrangle',\n",
       " 52165: 'emotion\\x85',\n",
       " 52166: \"'stargate'\",\n",
       " 40891: 'pinoy',\n",
       " 40892: 'conchatta',\n",
       " 41130: 'broeke',\n",
       " 40893: 'advisedly',\n",
       " 17638: \"barker's\",\n",
       " 52168: 'descours',\n",
       " 774: 'lots',\n",
       " 9261: 'lotr',\n",
       " 9881: 'irs',\n",
       " 52169: 'lott',\n",
       " 40894: 'xvi',\n",
       " 34730: 'irk',\n",
       " 52170: 'irl',\n",
       " 6889: 'ira',\n",
       " 21915: 'belzer',\n",
       " 52171: 'irc',\n",
       " 27647: 'ire',\n",
       " 40895: 'requisites',\n",
       " 7695: 'discipline',\n",
       " 52963: 'lyoko',\n",
       " 11312: 'extend',\n",
       " 875: 'nature',\n",
       " 52172: \"'dickie'\",\n",
       " 40896: 'optimist',\n",
       " 30588: 'lapping',\n",
       " 3902: 'superficial',\n",
       " 52173: 'vestment',\n",
       " 2825: 'extent',\n",
       " 52174: 'tendons',\n",
       " 52175: \"heller's\",\n",
       " 52176: 'quagmires',\n",
       " 52177: 'miyako',\n",
       " 20603: 'moocow',\n",
       " 52178: \"coles'\",\n",
       " 40897: 'lookit',\n",
       " 52179: 'ravenously',\n",
       " 40898: 'levitating',\n",
       " 52180: 'perfunctorily',\n",
       " 30589: 'lookin',\n",
       " 40900: \"lot'\",\n",
       " 52181: 'lookie',\n",
       " 34872: 'fearlessly',\n",
       " 52183: 'libyan',\n",
       " 40901: 'fondles',\n",
       " 35716: 'gopher',\n",
       " 40903: 'wearying',\n",
       " 52184: \"nz's\",\n",
       " 27648: 'minuses',\n",
       " 52185: 'puposelessly',\n",
       " 52186: 'shandling',\n",
       " 31270: 'decapitates',\n",
       " 11931: 'humming',\n",
       " 40904: \"'nother\",\n",
       " 21916: 'smackdown',\n",
       " 30590: 'underdone',\n",
       " 40905: 'frf',\n",
       " 52187: 'triviality',\n",
       " 25250: 'fro',\n",
       " 8779: 'bothers',\n",
       " 52188: \"'kensington\",\n",
       " 75: 'much',\n",
       " 34732: 'muco',\n",
       " 22617: 'wiseguy',\n",
       " 27650: \"richie's\",\n",
       " 40906: 'tonino',\n",
       " 52189: 'unleavened',\n",
       " 11589: 'fry',\n",
       " 40907: \"'tv'\",\n",
       " 40908: 'toning',\n",
       " 14363: 'obese',\n",
       " 30591: 'sensationalized',\n",
       " 40909: 'spiv',\n",
       " 6261: 'spit',\n",
       " 7366: 'arkin',\n",
       " 21917: 'charleton',\n",
       " 16825: 'jeon',\n",
       " 21918: 'boardroom',\n",
       " 4991: 'doubts',\n",
       " 3086: 'spin',\n",
       " 53085: 'hepo',\n",
       " 27651: 'wildcat',\n",
       " 10586: 'venoms',\n",
       " 52193: 'misconstrues',\n",
       " 18516: 'mesmerising',\n",
       " 40910: 'misconstrued',\n",
       " 52194: 'rescinds',\n",
       " 52195: 'prostrate',\n",
       " 40911: 'majid',\n",
       " 16481: 'climbed',\n",
       " 34733: 'canoeing',\n",
       " 52197: 'majin',\n",
       " 57806: 'animie',\n",
       " 40912: 'sylke',\n",
       " 14901: 'conditioned',\n",
       " 40913: 'waddell',\n",
       " 52198: '3\\x85',\n",
       " 41190: 'hyperdrive',\n",
       " 34734: 'conditioner',\n",
       " 53155: 'bricklayer',\n",
       " 2578: 'hong',\n",
       " 52200: 'memoriam',\n",
       " 30594: 'inventively',\n",
       " 25251: \"levant's\",\n",
       " 20640: 'portobello',\n",
       " 52202: 'remand',\n",
       " 19506: 'mummified',\n",
       " 27652: 'honk',\n",
       " 19507: 'spews',\n",
       " 40914: 'visitations',\n",
       " 52203: 'mummifies',\n",
       " 25252: 'cavanaugh',\n",
       " 23387: 'zeon',\n",
       " 40915: \"jungle's\",\n",
       " 34735: 'viertel',\n",
       " 27653: 'frenchmen',\n",
       " 52204: 'torpedoes',\n",
       " 52205: 'schlessinger',\n",
       " 34736: 'torpedoed',\n",
       " 69878: 'blister',\n",
       " 52206: 'cinefest',\n",
       " 34737: 'furlough',\n",
       " 52207: 'mainsequence',\n",
       " 40916: 'mentors',\n",
       " 9096: 'academic',\n",
       " 20604: 'stillness',\n",
       " 40917: 'academia',\n",
       " 52208: 'lonelier',\n",
       " 52209: 'nibby',\n",
       " 52210: \"losers'\",\n",
       " 40918: 'cineastes',\n",
       " 4451: 'corporate',\n",
       " 40919: 'massaging',\n",
       " 30595: 'bellow',\n",
       " 19508: 'absurdities',\n",
       " 53243: 'expetations',\n",
       " 40920: 'nyfiken',\n",
       " 75640: 'mehras',\n",
       " 52211: 'lasse',\n",
       " 52212: 'visability',\n",
       " 33948: 'militarily',\n",
       " 52213: \"elder'\",\n",
       " 19025: 'gainsbourg',\n",
       " 20605: 'hah',\n",
       " 13422: 'hai',\n",
       " 34738: 'haj',\n",
       " 25253: 'hak',\n",
       " 4313: 'hal',\n",
       " 4894: 'ham',\n",
       " 53261: 'duffer',\n",
       " 52215: 'haa',\n",
       " 68: 'had',\n",
       " 11932: 'advancement',\n",
       " 16827: 'hag',\n",
       " 25254: \"hand'\",\n",
       " 13423: 'hay',\n",
       " 20606: 'mcnamara',\n",
       " 52216: \"mozart's\",\n",
       " 30733: 'duffel',\n",
       " 30596: 'haq',\n",
       " 13889: 'har',\n",
       " 46: 'has',\n",
       " 2403: 'hat',\n",
       " 40921: 'hav',\n",
       " 30597: 'haw',\n",
       " 52217: 'figtings',\n",
       " 15497: 'elders',\n",
       " 52218: 'underpanted',\n",
       " 52219: 'pninson',\n",
       " 27654: 'unequivocally',\n",
       " 23675: \"barbara's\",\n",
       " 52221: \"bello'\",\n",
       " 12999: 'indicative',\n",
       " 40922: 'yawnfest',\n",
       " 52222: 'hexploitation',\n",
       " 52223: \"loder's\",\n",
       " 27655: 'sleuthing',\n",
       " 32624: \"justin's\",\n",
       " 52224: \"'ball\",\n",
       " 52225: \"'summer\",\n",
       " 34937: \"'demons'\",\n",
       " 52227: \"mormon's\",\n",
       " 34739: \"laughton's\",\n",
       " 52228: 'debell',\n",
       " 39726: 'shipyard',\n",
       " 30599: 'unabashedly',\n",
       " 40403: 'disks',\n",
       " 2292: 'crowd',\n",
       " 10089: 'crowe',\n",
       " 56436: \"vancouver's\",\n",
       " 34740: 'mosques',\n",
       " 6629: 'crown',\n",
       " 52229: 'culpas',\n",
       " 27656: 'crows',\n",
       " 53346: 'surrell',\n",
       " 52231: 'flowless',\n",
       " 52232: 'sheirk',\n",
       " 40925: \"'three\",\n",
       " 52233: \"peterson'\",\n",
       " 52234: 'ooverall',\n",
       " 40926: 'perchance',\n",
       " 1323: 'bottom',\n",
       " 53365: 'chabert',\n",
       " 52235: 'sneha',\n",
       " 13890: 'inhuman',\n",
       " 52236: 'ichii',\n",
       " 52237: 'ursla',\n",
       " 30600: 'completly',\n",
       " 40927: 'moviedom',\n",
       " 52238: 'raddick',\n",
       " 51997: 'brundage',\n",
       " 40928: 'brigades',\n",
       " 1183: 'starring',\n",
       " 52239: \"'goal'\",\n",
       " 52240: 'caskets',\n",
       " 52241: 'willcock',\n",
       " 52242: \"threesome's\",\n",
       " 52243: \"mosque'\",\n",
       " 52244: \"cover's\",\n",
       " 17639: 'spaceships',\n",
       " 40929: 'anomalous',\n",
       " 27657: 'ptsd',\n",
       " 52245: 'shirdan',\n",
       " 21964: 'obscenity',\n",
       " 30601: 'lemmings',\n",
       " 30602: 'duccio',\n",
       " 52246: \"levene's\",\n",
       " 52247: \"'gorby'\",\n",
       " 25257: \"teenager's\",\n",
       " 5342: 'marshall',\n",
       " 9097: 'honeymoon',\n",
       " 3233: 'shoots',\n",
       " 12260: 'despised',\n",
       " 52248: 'okabasho',\n",
       " 8291: 'fabric',\n",
       " 18517: 'cannavale',\n",
       " 3539: 'raped',\n",
       " 52249: \"tutt's\",\n",
       " 17640: 'grasping',\n",
       " 18518: 'despises',\n",
       " 40930: \"thief's\",\n",
       " 8928: 'rapes',\n",
       " 52250: 'raper',\n",
       " 27658: \"eyre'\",\n",
       " 52251: 'walchek',\n",
       " 23388: \"elmo's\",\n",
       " 40931: 'perfumes',\n",
       " 21920: 'spurting',\n",
       " 52252: \"exposition'\\x85\",\n",
       " 52253: 'denoting',\n",
       " 34742: 'thesaurus',\n",
       " 40932: \"shoot'\",\n",
       " 49761: 'bonejack',\n",
       " 52255: 'simpsonian',\n",
       " 30603: 'hebetude',\n",
       " 34743: \"hallow's\",\n",
       " 52256: 'desperation\\x85',\n",
       " 34744: 'incinerator',\n",
       " 10310: 'congratulations',\n",
       " 52257: 'humbled',\n",
       " 5926: \"else's\",\n",
       " 40847: 'trelkovski',\n",
       " 52258: \"rape'\",\n",
       " 59388: \"'chapters'\",\n",
       " 52259: '1600s',\n",
       " 7255: 'martian',\n",
       " 25258: 'nicest',\n",
       " 52261: 'eyred',\n",
       " 9459: 'passenger',\n",
       " 6043: 'disgrace',\n",
       " 52262: 'moderne',\n",
       " 5122: 'barrymore',\n",
       " 52263: 'yankovich',\n",
       " 40933: 'moderns',\n",
       " 52264: 'studliest',\n",
       " 52265: 'bedsheet',\n",
       " 14902: 'decapitation',\n",
       " 52266: 'slurring',\n",
       " 52267: \"'nunsploitation'\",\n",
       " 34745: \"'character'\",\n",
       " 9882: 'cambodia',\n",
       " 52268: 'rebelious',\n",
       " 27659: 'pasadena',\n",
       " 40934: 'crowne',\n",
       " 52269: \"'bedchamber\",\n",
       " 52270: 'conjectural',\n",
       " 52271: 'appologize',\n",
       " 52272: 'halfassing',\n",
       " 57818: 'paycheque',\n",
       " 20608: 'palms',\n",
       " 52273: \"'islands\",\n",
       " 40935: 'hawked',\n",
       " 21921: 'palme',\n",
       " 40936: 'conservatively',\n",
       " 64009: 'larp',\n",
       " 5560: 'palma',\n",
       " 21922: 'smelling',\n",
       " 13000: 'aragorn',\n",
       " 52274: 'hawker',\n",
       " 52275: 'hawkes',\n",
       " 3977: 'explosions',\n",
       " 8061: 'loren',\n",
       " 52276: \"pyle's\",\n",
       " 6706: 'shootout',\n",
       " 18519: \"mike's\",\n",
       " 52277: \"driscoll's\",\n",
       " 40937: 'cogsworth',\n",
       " 52278: \"britian's\",\n",
       " 34746: 'childs',\n",
       " 52279: \"portrait's\",\n",
       " 3628: 'chain',\n",
       " 2499: 'whoever',\n",
       " 52280: 'puttered',\n",
       " 52281: 'childe',\n",
       " 52282: 'maywether',\n",
       " 3038: 'chair',\n",
       " 52283: \"rance's\",\n",
       " 34747: 'machu',\n",
       " 4519: 'ballet',\n",
       " 34748: 'grapples',\n",
       " 76154: 'summerize',\n",
       " 30605: 'freelance',\n",
       " 52285: \"andrea's\",\n",
       " 52286: '\\x91very',\n",
       " 45881: 'coolidge',\n",
       " 18520: 'mache',\n",
       " 52287: 'balled',\n",
       " 40939: 'grappled',\n",
       " 18521: 'macha',\n",
       " 21923: 'underlining',\n",
       " 5625: 'macho',\n",
       " 19509: 'oversight',\n",
       " 25259: 'machi',\n",
       " 11313: 'verbally',\n",
       " 21924: 'tenacious',\n",
       " 40940: 'windshields',\n",
       " 18559: 'paychecks',\n",
       " 3398: 'jerk',\n",
       " 11933: \"good'\",\n",
       " 34750: 'prancer',\n",
       " 21925: 'prances',\n",
       " 52288: 'olympus',\n",
       " 21926: 'lark',\n",
       " 10787: 'embark',\n",
       " 7367: 'gloomy',\n",
       " 52289: 'jehaan',\n",
       " 52290: 'turaqui',\n",
       " 20609: \"child'\",\n",
       " 2896: 'locked',\n",
       " 52291: 'pranced',\n",
       " 2590: 'exact',\n",
       " 52292: 'unattuned',\n",
       " 785: 'minute',\n",
       " 16120: 'skewed',\n",
       " 40942: 'hodgins',\n",
       " 34751: 'skewer',\n",
       " 52293: 'think\\x85',\n",
       " 38767: 'rosenstein',\n",
       " 52294: 'helmit',\n",
       " 34752: 'wrestlemanias',\n",
       " 16828: 'hindered',\n",
       " 30606: \"martha's\",\n",
       " 52295: 'cheree',\n",
       " 52296: \"pluckin'\",\n",
       " 40943: 'ogles',\n",
       " 11934: 'heavyweight',\n",
       " 82192: 'aada',\n",
       " 11314: 'chopping',\n",
       " 61536: 'strongboy',\n",
       " 41344: 'hegemonic',\n",
       " 40944: 'adorns',\n",
       " 41348: 'xxth',\n",
       " 34753: 'nobuhiro',\n",
       " 52300: 'capitães',\n",
       " 52301: 'kavogianni',\n",
       " 13424: 'antwerp',\n",
       " 6540: 'celebrated',\n",
       " 52302: 'roarke',\n",
       " 40945: 'baggins',\n",
       " 31272: 'cheeseburgers',\n",
       " 52303: 'matras',\n",
       " 52304: \"nineties'\",\n",
       " 52305: \"'craig'\",\n",
       " 13001: 'celebrates',\n",
       " 3385: 'unintentionally',\n",
       " 14364: 'drafted',\n",
       " 52306: 'climby',\n",
       " 52307: '303',\n",
       " 18522: 'oldies',\n",
       " 9098: 'climbs',\n",
       " 9657: 'honour',\n",
       " 34754: 'plucking',\n",
       " 30076: '305',\n",
       " 5516: 'address',\n",
       " 40946: 'menjou',\n",
       " 42594: \"'freak'\",\n",
       " 19510: 'dwindling',\n",
       " 9460: 'benson',\n",
       " 52309: 'white’s',\n",
       " 40947: 'shamelessness',\n",
       " 21927: 'impacted',\n",
       " 52310: 'upatz',\n",
       " 3842: 'cusack',\n",
       " 37569: \"flavia's\",\n",
       " 52311: 'effette',\n",
       " 34755: 'influx',\n",
       " 52312: 'boooooooo',\n",
       " 52313: 'dimitrova',\n",
       " 13425: 'houseman',\n",
       " 25261: 'bigas',\n",
       " 52314: 'boylen',\n",
       " 52315: 'phillipenes',\n",
       " 40948: 'fakery',\n",
       " 27660: \"grandpa's\",\n",
       " 27661: 'darnell',\n",
       " 19511: 'undergone',\n",
       " 52317: 'handbags',\n",
       " 21928: 'perished',\n",
       " 37780: 'pooped',\n",
       " 27662: 'vigour',\n",
       " 3629: 'opposed',\n",
       " 52318: 'etude',\n",
       " 11801: \"caine's\",\n",
       " 52319: 'doozers',\n",
       " 34756: 'photojournals',\n",
       " 52320: 'perishes',\n",
       " 34757: 'constrains',\n",
       " 40950: 'migenes',\n",
       " 30607: 'consoled',\n",
       " 16829: 'alastair',\n",
       " 52321: 'wvs',\n",
       " 52322: 'ooooooh',\n",
       " 34758: 'approving',\n",
       " 40951: 'consoles',\n",
       " 52066: 'disparagement',\n",
       " 52324: 'futureistic',\n",
       " 52325: 'rebounding',\n",
       " 52326: \"'date\",\n",
       " 52327: 'gregoire',\n",
       " 21929: 'rutherford',\n",
       " 34759: 'americanised',\n",
       " 82198: 'novikov',\n",
       " 1044: 'following',\n",
       " 34760: 'munroe',\n",
       " 52328: \"morita'\",\n",
       " 52329: 'christenssen',\n",
       " 23108: 'oatmeal',\n",
       " 25262: 'fossey',\n",
       " 40952: 'livered',\n",
       " 13002: 'listens',\n",
       " 76166: \"'marci\",\n",
       " 52332: \"otis's\",\n",
       " 23389: 'thanking',\n",
       " 16021: 'maude',\n",
       " 34761: 'extensions',\n",
       " 52334: 'ameteurish',\n",
       " 52335: \"commender's\",\n",
       " 27663: 'agricultural',\n",
       " 4520: 'convincingly',\n",
       " 17641: 'fueled',\n",
       " 54016: 'mahattan',\n",
       " 40954: \"paris's\",\n",
       " 52338: 'vulkan',\n",
       " 52339: 'stapes',\n",
       " 52340: 'odysessy',\n",
       " 12261: 'harmon',\n",
       " 4254: 'surfing',\n",
       " 23496: 'halloran',\n",
       " 49582: 'unbelieveably',\n",
       " 52341: \"'offed'\",\n",
       " 30609: 'quadrant',\n",
       " 19512: 'inhabiting',\n",
       " 34762: 'nebbish',\n",
       " 40955: 'forebears',\n",
       " 34763: 'skirmish',\n",
       " 52342: 'ocassionally',\n",
       " 52343: \"'resist\",\n",
       " 21930: 'impactful',\n",
       " 52344: 'spicier',\n",
       " 40956: 'touristy',\n",
       " 52345: \"'football'\",\n",
       " 40957: 'webpage',\n",
       " 52347: 'exurbia',\n",
       " 52348: 'jucier',\n",
       " 14903: 'professors',\n",
       " 34764: 'structuring',\n",
       " 30610: 'jig',\n",
       " 40958: 'overlord',\n",
       " 25263: 'disconnect',\n",
       " 82203: 'sniffle',\n",
       " 40959: 'slimeball',\n",
       " 40960: 'jia',\n",
       " 16830: 'milked',\n",
       " 40961: 'banjoes',\n",
       " 1239: 'jim',\n",
       " 52350: 'workforces',\n",
       " 52351: 'jip',\n",
       " 52352: 'rotweiller',\n",
       " 34765: 'mundaneness',\n",
       " 52353: \"'ninja'\",\n",
       " 11042: \"dead'\",\n",
       " 40962: \"cipriani's\",\n",
       " 20610: 'modestly',\n",
       " 52354: \"professor'\",\n",
       " 40963: 'shacked',\n",
       " 34766: 'bashful',\n",
       " 23390: 'sorter',\n",
       " 16122: 'overpowering',\n",
       " 18523: 'workmanlike',\n",
       " 27664: 'henpecked',\n",
       " 18524: 'sorted',\n",
       " 52356: \"jōb's\",\n",
       " 52357: \"'always\",\n",
       " 34767: \"'baptists\",\n",
       " 52358: 'dreamcatchers',\n",
       " 52359: \"'silence'\",\n",
       " 21931: 'hickory',\n",
       " 52360: 'fun\\x97yet',\n",
       " 52361: 'breakumentary',\n",
       " 15498: 'didn',\n",
       " 52362: 'didi',\n",
       " 52363: 'pealing',\n",
       " 40964: 'dispite',\n",
       " 25264: \"italy's\",\n",
       " 21932: 'instability',\n",
       " 6541: 'quarter',\n",
       " 12610: 'quartet',\n",
       " 52364: 'padmé',\n",
       " 52365: \"'bleedmedry\",\n",
       " 52366: 'pahalniuk',\n",
       " 52367: 'honduras',\n",
       " 10788: 'bursting',\n",
       " 41467: \"pablo's\",\n",
       " 52369: 'irremediably',\n",
       " 40965: 'presages',\n",
       " 57834: 'bowlegged',\n",
       " 65185: 'dalip',\n",
       " 6262: 'entering',\n",
       " 76174: 'newsradio',\n",
       " 54152: 'presaged',\n",
       " 27665: \"giallo's\",\n",
       " 40966: 'bouyant',\n",
       " 52370: 'amerterish',\n",
       " 18525: 'rajni',\n",
       " 30612: 'leeves',\n",
       " 34769: 'macauley',\n",
       " 614: 'seriously',\n",
       " 52371: 'sugercoma',\n",
       " 52372: 'grimstead',\n",
       " 52373: \"'fairy'\",\n",
       " 30613: 'zenda',\n",
       " 52374: \"'twins'\",\n",
       " 17642: 'realisation',\n",
       " 27666: 'highsmith',\n",
       " 7819: 'raunchy',\n",
       " 40967: 'incentives',\n",
       " 52376: 'flatson',\n",
       " 35099: 'snooker',\n",
       " 16831: 'crazies',\n",
       " 14904: 'crazier',\n",
       " 7096: 'grandma',\n",
       " 52377: 'napunsaktha',\n",
       " 30614: 'workmanship',\n",
       " 52378: 'reisner',\n",
       " 61308: \"sanford's\",\n",
       " 52379: '\\x91doña',\n",
       " 6110: 'modest',\n",
       " 19155: \"everything's\",\n",
       " 40968: 'hamer',\n",
       " 52381: \"couldn't'\",\n",
       " 13003: 'quibble',\n",
       " 52382: 'socking',\n",
       " 21933: 'tingler',\n",
       " 52383: 'gutman',\n",
       " 40969: 'lachlan',\n",
       " 52384: 'tableaus',\n",
       " 52385: 'headbanger',\n",
       " 2849: 'spoken',\n",
       " 34770: 'cerebrally',\n",
       " 23492: \"'road\",\n",
       " 21934: 'tableaux',\n",
       " 40970: \"proust's\",\n",
       " 40971: 'periodical',\n",
       " 52387: \"shoveller's\",\n",
       " 25265: 'tamara',\n",
       " 17643: 'affords',\n",
       " 3251: 'concert',\n",
       " 87957: \"yara's\",\n",
       " 52388: 'someome',\n",
       " 8426: 'lingering',\n",
       " 41513: \"abraham's\",\n",
       " 34771: 'beesley',\n",
       " 34772: 'cherbourg',\n",
       " 28626: 'kagan',\n",
       " 9099: 'snatch',\n",
       " 9262: \"miyazaki's\",\n",
       " 25266: 'absorbs',\n",
       " 40972: \"koltai's\",\n",
       " 64029: 'tingled',\n",
       " 19513: 'crossroads',\n",
       " 16123: 'rehab',\n",
       " 52391: 'falworth',\n",
       " 52392: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create id to word mapping\n",
    "# Input, word_to_id, is word to id mapping\n",
    "# Output, will have a numeric key and the value will be the word\n",
    "id_to_word = {value: key for key, value in word_to_id.items()}\n",
    "print(type(id_to_word))\n",
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88587\n",
      "<START>\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "print(len(id_to_word))\n",
    "for key, value in id_to_word.items():\n",
    "    if (key == 1) or (value == \"<END>\"):\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that there is julia fantasy to repressed and film good br of loose and basic have into your whatever i i and and demented be hop this standards cole new be home all seek film wives lot br made and in at this of search how concept in thirty some this and not all it rachel are of boys and re is and animals deserve i i worst more it is renting concerned message made all and in does of nor of nor side be and center obviously know end computer here to all tries in does of nor side of home br be indeed i i all it officer in could is performance and fully in of and br by br and its and lit well of nor at coming it's it that an this obviously i i this as their has obviously bad and exist countless and mixed of and br work to of run up and and br dear nor this early her bad having tortured film and movie all care of their br be right acting i i and of and and it away of its shooting and to suffering version you br singers your way just and was can't compared condition film of and br united obviously are up obviously not other just and was and as true was least of and certainly lady poorly of setting produced and br refuse to make just have 2 which and of and dialog and br of frye say in can is you for it wasn't in singers as by it away plenty what have reason and are that willing that's have 2 which sister and of important br halfway to of took work 20 br similar more he good flower for hit at coming not see reputation\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "# print(x_train[n])\n",
    "\n",
    "# Create review (as words) for a sample train record\n",
    "print(' '.join(id_to_word[id+2] for id in x_train[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreate neural network\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create neural network\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_model(vocab_size, embedding_size, maxlen):\n",
    "    # define the model\n",
    "    model_out = Sequential()\n",
    "    model_out.add(Embedding(vocab_size, embedding_size, input_length=maxlen))  # 50*8 = 400 learnable weights (input so no bias)\n",
    "    model_out.add(Flatten())\n",
    "    model_out.add(Dense(1, activation='sigmoid'))  # 8*4 + 1 = 33 learnable weights\n",
    "    # compile the model\n",
    "    model_out.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    #print(model_out.summary())\n",
    "    return model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(model_in):\n",
    "    # fit the model\n",
    "    model_in.fit(x_train, y_train, epochs=2, verbose=1, batch_size=500)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model_in.evaluate(x_test, y_test, verbose=0)\n",
    "    #print('Accuracy: %f' % (accuracy))\n",
    "    #print('Loss: %f' % (loss))\n",
    "    print(\"Accuracy \", round(accuracy, 5), \", Loss \", round(loss, 5))\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---***---\n",
      "50000  words-dictionary,  5  maxlength,  5  embedding_size\n",
      "Accuracy  0.67272 , Loss  0.72647\n",
      "Time taken for this embedding:  2.12  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  5  maxlength,  10  embedding_size\n",
      "Accuracy  0.6684 , Loss  0.80344\n",
      "Time taken for this embedding:  3.42  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  5  maxlength,  30  embedding_size\n",
      "Accuracy  0.6588 , Loss  0.96455\n",
      "Time taken for this embedding:  6.4  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  5  maxlength,  50  embedding_size\n",
      "Accuracy  0.6576 , Loss  1.0222\n",
      "Time taken for this embedding:  9.36  minutes\n",
      "100  is too big value for embedding_size. Skipping it...\n",
      "200  is too big value for embedding_size. Skipping it...\n",
      "250  is too big value for embedding_size. Skipping it...\n",
      "300  is too big value for embedding_size. Skipping it...\n",
      "400  is too big value for embedding_size. Skipping it...\n",
      "***Time taken for this maxlen:  21.3  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  10  maxlength,  5  embedding_size\n",
      "Accuracy  0.70788 , Loss  0.69133\n",
      "Time taken for this embedding:  1.92  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  10  maxlength,  10  embedding_size\n",
      "Accuracy  0.69236 , Loss  0.80719\n",
      "Time taken for this embedding:  3.18  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  10  maxlength,  30  embedding_size\n",
      "Accuracy  0.69076 , Loss  1.02752\n",
      "Time taken for this embedding:  6.14  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  10  maxlength,  50  embedding_size\n",
      "Accuracy  0.68584 , Loss  1.16675\n",
      "Time taken for this embedding:  9.41  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  10  maxlength,  100  embedding_size\n",
      "Accuracy  0.68288 , Loss  1.37256\n",
      "Time taken for this embedding:  17.73  minutes\n",
      "200  is too big value for embedding_size. Skipping it...\n",
      "250  is too big value for embedding_size. Skipping it...\n",
      "300  is too big value for embedding_size. Skipping it...\n",
      "400  is too big value for embedding_size. Skipping it...\n",
      "***Time taken for this maxlen:  38.4  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  5  embedding_size\n",
      "Accuracy  0.75784 , Loss  0.64036\n",
      "Time taken for this embedding:  1.82  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  10  embedding_size\n",
      "Accuracy  0.75984 , Loss  0.72067\n",
      "Time taken for this embedding:  3.02  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  30  embedding_size\n",
      "Accuracy  0.76804 , Loss  0.77065\n",
      "Time taken for this embedding:  6.16  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  50  embedding_size\n",
      "Accuracy  0.76988 , Loss  0.79032\n",
      "Time taken for this embedding:  9.38  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  100  embedding_size\n",
      "Accuracy  0.77012 , Loss  0.81644\n",
      "Time taken for this embedding:  17.62  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  200  embedding_size\n",
      "Accuracy  0.76924 , Loss  0.83437\n",
      "Time taken for this embedding:  33.77  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  250  embedding_size\n",
      "Accuracy  0.76956 , Loss  0.83327\n",
      "Time taken for this embedding:  41.6  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  30  maxlength,  300  embedding_size\n",
      "Accuracy  0.7686 , Loss  0.83692\n",
      "Time taken for this embedding:  49.55  minutes\n",
      "400  is too big value for embedding_size. Skipping it...\n",
      "***Time taken for this maxlen:  162.94  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  5  embedding_size\n",
      "Accuracy  0.79148 , Loss  0.58231\n",
      "Time taken for this embedding:  1.83  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  10  embedding_size\n",
      "Accuracy  0.79748 , Loss  0.6333\n",
      "Time taken for this embedding:  3.04  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  30  embedding_size\n",
      "Accuracy  0.80352 , Loss  0.65615\n",
      "Time taken for this embedding:  6.18  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  50  embedding_size\n",
      "Accuracy  0.80736 , Loss  0.65782\n",
      "Time taken for this embedding:  9.24  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  100  embedding_size\n",
      "Accuracy  0.80888 , Loss  0.65379\n",
      "Time taken for this embedding:  17.63  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  200  embedding_size\n",
      "Accuracy  0.81032 , Loss  0.66156\n",
      "Time taken for this embedding:  33.55  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  250  embedding_size\n",
      "Accuracy  0.81112 , Loss  0.66509\n",
      "Time taken for this embedding:  41.83  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  300  embedding_size\n",
      "Accuracy  0.81292 , Loss  0.66627\n",
      "Time taken for this embedding:  49.67  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  50  maxlength,  400  embedding_size\n",
      "Accuracy  0.80972 , Loss  0.673\n",
      "Time taken for this embedding:  66.16  minutes\n",
      "***Time taken for this maxlen:  229.16  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  5  embedding_size\n",
      "Accuracy  0.83712 , Loss  0.50122\n",
      "Time taken for this embedding:  1.91  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  10  embedding_size\n",
      "Accuracy  0.839 , Loss  0.52557\n",
      "Time taken for this embedding:  3.05  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  30  embedding_size\n",
      "Accuracy  0.84468 , Loss  0.5334\n",
      "Time taken for this embedding:  6.34  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  50  embedding_size\n",
      "Accuracy  0.84608 , Loss  0.53401\n",
      "Time taken for this embedding:  9.56  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  100  embedding_size\n",
      "Accuracy  0.84904 , Loss  0.52428\n",
      "Time taken for this embedding:  10.73  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  200  embedding_size\n",
      "Accuracy  0.8512 , Loss  0.51867\n",
      "Time taken for this embedding:  20.86  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  250  embedding_size\n",
      "Accuracy  0.8516 , Loss  0.5162\n",
      "Time taken for this embedding:  30.77  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  300  embedding_size\n",
      "Accuracy  0.85064 , Loss  0.51903\n",
      "Time taken for this embedding:  53.81  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  100  maxlength,  400  embedding_size\n",
      "Accuracy  0.85148 , Loss  0.51642\n",
      "Time taken for this embedding:  72.94  minutes\n",
      "***Time taken for this maxlen:  210.01  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  5  embedding_size\n",
      "Accuracy  0.859 , Loss  0.44926\n",
      "Time taken for this embedding:  2.15  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  10  embedding_size\n",
      "Accuracy  0.85872 , Loss  0.47622\n",
      "Time taken for this embedding:  3.53  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  30  embedding_size\n",
      "Accuracy  0.86492 , Loss  0.48761\n",
      "Time taken for this embedding:  6.95  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  50  embedding_size\n",
      "Accuracy  0.86652 , Loss  0.48291\n",
      "Time taken for this embedding:  10.86  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  100  embedding_size\n",
      "Accuracy  0.86876 , Loss  0.47477\n",
      "Time taken for this embedding:  18.5  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  200  embedding_size\n",
      "Accuracy  0.86928 , Loss  0.46947\n",
      "Time taken for this embedding:  24.74  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  250  embedding_size\n",
      "Accuracy  0.8718 , Loss  0.46574\n",
      "Time taken for this embedding:  22.78  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  300  embedding_size\n",
      "Accuracy  0.8716 , Loss  0.46196\n",
      "Time taken for this embedding:  45.17  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  200  maxlength,  400  embedding_size\n",
      "Accuracy  0.87112 , Loss  0.46438\n",
      "Time taken for this embedding:  61.42  minutes\n",
      "***Time taken for this maxlen:  196.14  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  5  embedding_size\n",
      "Accuracy  0.858 , Loss  0.45727\n",
      "Time taken for this embedding:  1.92  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  10  embedding_size\n",
      "Accuracy  0.86052 , Loss  0.49036\n",
      "Time taken for this embedding:  2.84  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  30  embedding_size\n",
      "Accuracy  0.866 , Loss  0.4861\n",
      "Time taken for this embedding:  5.79  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  50  embedding_size\n",
      "Accuracy  0.87076 , Loss  0.4777\n",
      "Time taken for this embedding:  8.93  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  100  embedding_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.8714 , Loss  0.46577\n",
      "Time taken for this embedding:  16.38  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  200  embedding_size\n",
      "Accuracy  0.87236 , Loss  0.46233\n",
      "Time taken for this embedding:  25.6  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  250  embedding_size\n",
      "Accuracy  0.87404 , Loss  0.44937\n",
      "Time taken for this embedding:  26.68  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  300  embedding_size\n",
      "Accuracy  0.87272 , Loss  0.45699\n",
      "Time taken for this embedding:  42.68  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  250  maxlength,  400  embedding_size\n",
      "Accuracy  0.87352 , Loss  0.45638\n",
      "Time taken for this embedding:  64.07  minutes\n",
      "***Time taken for this maxlen:  194.93  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  5  embedding_size\n",
      "Accuracy  0.85732 , Loss  0.46205\n",
      "Time taken for this embedding:  1.88  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  10  embedding_size\n",
      "Accuracy  0.86152 , Loss  0.49472\n",
      "Time taken for this embedding:  2.99  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  30  embedding_size\n",
      "Accuracy  0.865 , Loss  0.48544\n",
      "Time taken for this embedding:  6.24  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  50  embedding_size\n",
      "Accuracy  0.86752 , Loss  0.49118\n",
      "Time taken for this embedding:  9.69  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  100  embedding_size\n",
      "Accuracy  0.87004 , Loss  0.47112\n",
      "Time taken for this embedding:  17.72  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  200  embedding_size\n",
      "Accuracy  0.87208 , Loss  0.46492\n",
      "Time taken for this embedding:  35.61  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  250  embedding_size\n",
      "Accuracy  0.87108 , Loss  0.46988\n",
      "Time taken for this embedding:  41.86  minutes\n",
      "\n",
      "---***---\n",
      "50000  words-dictionary,  300  maxlength,  300  embedding_size\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[50000,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Adam/Adam/update/Read/ReadVariableOp (defined at C:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Adam/Adam/update/AssignSubVariableOp/_41]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[50000,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Adam/Adam/update/Read/ReadVariableOp (defined at C:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_1610113]\n\nFunction call stack:\ndistributed_function -> distributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3a47f8bf5970>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m                   embedding_size_entry, \" embedding_size\" )\n\u001b[0;32m     63\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size_entry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size_entry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen_entry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0maccuracy_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size_entry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-79dbb0c2490d>\u001b[0m in \u001b[0;36mfit_evaluate\u001b[1;34m(model_in)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfit_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[50000,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Adam/Adam/update/Read/ReadVariableOp (defined at C:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Adam/Adam/update/AssignSubVariableOp/_41]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[50000,300] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node Adam/Adam/update/Read/ReadVariableOp (defined at C:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_function_1610113]\n\nFunction call stack:\ndistributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# vocab_size_list = [1000, 5000, 10000, 20000, 30000, 50000, 80000, 100000]  # Total words in dictionary to consider\n",
    "# maxlen_list = [5, 10, 20, 30, 50, 100, 150, 200, 250, 300, 350, 500, 1000]  # Length of document to consider\n",
    "# embedding_size_list = [5, 10, 20, 30, 50, 100, 150, 200, 250, 300, 350, 400, 500, 1000]\n",
    "\n",
    "# vocab_size_list = [5000, 10000, 30000, 50000, 89000]  # Total words in dictionary to consider\n",
    "# maxlen_list = [5, 10, 30, 50, 75, 100, 200, 300, 500]  # Length of document to consider\n",
    "# embedding_size_list = [5, 10, 30, 50, 100, 200, 300, 500, 1000]\n",
    "\n",
    "# vocab_size_list = [10000, 30000, 50000, 89000]  # Total words in dictionary to consider\n",
    "# maxlen_list = [5, 10, 30, 50, 100, 200, 300, 500]  # Length of document to consider\n",
    "# embedding_size_list = [5, 10, 30, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "# vocab_size_list = [30000, 50000, 89000]  # Total words in dictionary to consider\n",
    "# maxlen_list = [5, 10, 30, 50, 100, 200, 300, 500]  # Length of document to consider\n",
    "# embedding_size_list = [5, 10, 30, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "vocab_size_list = [50000, 89000]  # Total words in dictionary to consider\n",
    "maxlen_list = [5, 10, 30, 50, 100, 200, 250, 300]  # Length of document to consider\n",
    "embedding_size_list = [5, 10, 30, 50, 100, 200, 250, 300, 400]\n",
    "\n",
    "# Lists to save to the performance_df\n",
    "vocab_size = []\n",
    "maxlen = []\n",
    "embedding_size = []\n",
    "accuracy = []\n",
    "loss = []\n",
    "\n",
    "fileName = 'hyperparameterComparison_v5.csv'\n",
    "\n",
    "for i, vocab_size_entry in enumerate(vocab_size_list):\n",
    "    startingTime = time.time()\n",
    "    \n",
    "    #load dataset as a list of ints\n",
    "    (x_train_original, y_train), (x_test_original, y_test) = imdb.load_data(num_words=vocab_size_entry)\n",
    "    \n",
    "    for j, maxlen_entry in enumerate(maxlen_list):\n",
    "        performance_df = pd.DataFrame({'vocab_size': vocab_size, 'maxlen': maxlen, \n",
    "                               'embedding_size': embedding_size, 'accuracy': accuracy, 'loss': loss})\n",
    "        performance_df.to_csv(fileName, index=False)\n",
    "        \n",
    "        startingTime2 = time.time()\n",
    "        \n",
    "        #make all sequences of the same length\n",
    "        x_train = pad_sequences(x_train_original, maxlen=maxlen_entry, padding='post')\n",
    "        x_test =  pad_sequences(x_test_original, maxlen=maxlen_entry, padding='post')\n",
    "        \n",
    "        for k, embedding_size_entry in enumerate(embedding_size_list):\n",
    "            if embedding_size_entry > 10*maxlen_entry:\n",
    "                print(embedding_size_entry, ' is too big value for embedding_size. Skipping it...')\n",
    "                continue\n",
    "            # Else continue\n",
    "            startingTime3 = time.time()\n",
    "            \n",
    "            print()\n",
    "            print('---***---')\n",
    "            print(vocab_size_entry, \" words-dictionary, \", maxlen_entry, \" maxlength, \", \n",
    "                  embedding_size_entry, \" embedding_size\" )\n",
    "            model = create_model(vocab_size_entry, embedding_size_entry, maxlen_entry)\n",
    "            accuracy_i, loss_i = fit_evaluate(model)\n",
    "            \n",
    "            vocab_size.append(vocab_size_entry)\n",
    "            embedding_size.append(embedding_size_entry)\n",
    "            maxlen.append(maxlen_entry)\n",
    "            accuracy.append(round(accuracy_i, 3))\n",
    "            loss.append(round(loss_i, 3))\n",
    "            print('Time taken for this embedding: ', round(((time.time() - startingTime3)/60), 2), ' minutes')\n",
    "        print('***Time taken for this maxlen: ', round(((time.time() - startingTime2)/60), 2), ' minutes')\n",
    "    print('******Time taken for this vocab_size: ', round(((time.time() - startingTime)/60), 2), ' minutes')\n",
    "\n",
    "# Save\n",
    "performance_df = pd.DataFrame({'vocab_size': vocab_size, 'maxlen': maxlen, \n",
    "                               'embedding_size': embedding_size, 'accuracy': accuracy, 'loss': loss})\n",
    "performance_df.to_csv(fileName, index=False)\n",
    "\n",
    "\"\"\"\n",
    "100000 words, 3000 maxlength of document: Accuracy 0.86556, Loss 0.508562\n",
    "10000 words, 3000 maxlength: Accuracy 0.866080, Loss 0.543325\n",
    "10000 words, 300 maxlength: Accuracy 0.867640, Loss 0.515492\n",
    "10000  words,  300  maxlength, embedding_size  50  : Accuracy  0.86764 , Loss  0.5154921369147301\n",
    "\n",
    "5000\t300\t200\t0.867999971\t0.521\n",
    "10000\t300\t300\t0.871999979\t0.496\n",
    "30000\t300\t200\t0.873000026\t0.467\n",
    "50000\t250\t250\t0.874000013\t0.449\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model configuration was at\n",
    "- Vocab size: 50000\n",
    "- Sentence length: 250\n",
    "- Embedding size: 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 250)\n",
      "(25000, 250)\n",
      "...fit_evaluate\n",
      "Train on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 21s 853us/sample - loss: 0.6461 - accuracy: 0.6322\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 19s 758us/sample - loss: 0.3204 - accuracy: 0.8843\n",
      "Accuracy  0.86476 , Loss  0.32424\n"
     ]
    }
   ],
   "source": [
    "vocab_size_entry = 50000\n",
    "maxlen_entry = 250\n",
    "embedding_size_entry = 250\n",
    "\n",
    "(x_train_original, y_train), (x_test_original, y_test) = imdb.load_data(num_words=vocab_size_entry)\n",
    "\n",
    "x_train = pad_sequences(x_train_original, maxlen=maxlen_entry, padding='post')\n",
    "x_test =  pad_sequences(x_test_original, maxlen=maxlen_entry, padding='post')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "model = create_model(vocab_size_entry, embedding_size_entry, maxlen_entry)\n",
    "print('...fit_evaluate')\n",
    "accuracy_i, loss_i = fit_evaluate(model)\n",
    "\n",
    "# vocab_size.append(vocab_size_entry)\n",
    "# embedding_size.append(embedding_size_entry)\n",
    "# maxlen.append(maxlen_entry)\n",
    "# accuracy.append(round(accuracy_i, 3))\n",
    "# loss.append(round(loss_i, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 250, 250)          12500000  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 62500)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 62501     \n",
      "=================================================================\n",
      "Total params: 12,562,501\n",
      "Trainable params: 12,562,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score:  0.86476\n",
      "\n",
      "Model confusion matrix: \n",
      " [[10519  1981]\n",
      " [ 1400 11100]]\n",
      "\n",
      "Model classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86     12500\n",
      "           1       0.85      0.89      0.87     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.87      0.86      0.86     25000\n",
      "weighted avg       0.87      0.86      0.86     25000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "import numpy as np\n",
    "\n",
    "def probToVal(y_prob):\n",
    "    y_val = np.zeros((len(y_prob)))\n",
    "    for i in range(len(y_prob)):\n",
    "        if y_prob[i] > 0.5: y_val[i] = 1\n",
    "    return y_val\n",
    "\n",
    "def checkPerformance(Y_1, Y_2, verbose=1):\n",
    "    print(\"Model accuracy score: \", accuracy_score(Y_1, Y_2))\n",
    "    if verbose > 0:\n",
    "        print()\n",
    "        print(\"Model confusion matrix: \\n\", confusion_matrix(Y_1, Y_2))\n",
    "        print()\n",
    "        print(\"Model classification report: \\n\", classification_report(Y_1, Y_2))\n",
    "        print()\n",
    "    return\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_val = probToVal(y_pred)\n",
    "\n",
    "# y_pred[1]\n",
    "# y_test[0]\n",
    "\n",
    "\n",
    "checkPerformance(y_test, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AqOnLa2eCzH"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-f22fdc61f764>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-f22fdc61f764>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[0;32m   3771\u001b[0m       raise ValueError('Session keyword arguments are not support during '\n\u001b[0;32m   3772\u001b[0m                        'eager execution. You passed: %s' % (kwargs,))\n\u001b[1;32m-> 3773\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mEagerExecutionFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3775\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, name)\u001b[0m\n\u001b[0;32m   3668\u001b[0m             \u001b[0madd_sources\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3669\u001b[0m             \u001b[0mhandle_captures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3670\u001b[1;33m             base_graph=source_graph)\n\u001b[0m\u001b[0;32m   3671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3672\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlifted_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\u001b[0m in \u001b[0;36mlift_to_graph\u001b[1;34m(tensors, graph, sources, disallowed_placeholders, add_sources, handle_captures, base_graph, op_map)\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[1;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m   \u001b[0mvisited_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\installationDirectory\\lib\\site-packages\\tensorflow_core\\python\\eager\\lift_to_graph.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[1;31m# Check that the initializer does not depend on any placeholders.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m   \u001b[0msources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentitySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m   \u001b[0mvisited_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m   \u001b[0mop_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "inp = model.input\n",
    "outputs = [layer.output for layer in model.layers]\n",
    "fun = [K.function([inp, K.learning_phase()], [out]) for out in outputs]\n",
    "\n",
    "test = np.array([x_test[0], ])\n",
    "layer_outs = [func([test, 1]) for func in fun]\n",
    "print(len(layer_outs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dUDSg7VeCzM"
   },
   "outputs": [],
   "source": [
    "# outputs = [K.function([model.input], [layer.output])([x_test, 1]) for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tskt_1npeCzP"
   },
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 128 # 30 # 128\n",
    "# lstm_out = 196 # 128 # 196\n",
    "# num_classes = 2\n",
    "# model_lstm = Sequential()\n",
    "# model_lstm.add(Embedding(vocab_size_entry, embedding_size_entry, input_length=maxlen_entry))\n",
    "# # model.add(embedding_layer)\n",
    "# #model.add(SpatialDropout1D(0.25))\n",
    "# model_lstm.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\n",
    "# #model.add(LSTM(lstm_out))\n",
    "# \"\"\"\n",
    "# ADD BATCH NORMALIZATION\n",
    "# \"\"\"\n",
    "# model_lstm.add(BatchNormalization())\n",
    "# #\n",
    "# model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "# model_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# #\n",
    "# # model.add(Dense(1,activation='sigmoid'))\n",
    "# # # model.add(Dense(1,activation='tanh'))\n",
    "# # model.compile(loss = BinaryCrossentropy(), optimizer='adam',metrics = ['accuracy'])\n",
    "# print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def oneEpochTrain(X_train_in, Y_train_in):\n",
    "#     batch_size = 32\n",
    "#     #model.fit(X_train, np.array(Y_train), epochs = 1, batch_size=batch_size, verbose = 0, class_weight=class_weight)\n",
    "#     model_lstm.fit(X_train_in, np.array(Y_train_in), epochs = 1, batch_size=batch_size, verbose = 1)\n",
    "#     #\n",
    "#     #score,acc = model.evaluate(X_test, np.array(Y_test), verbose = 2, batch_size = batch_size)\n",
    "#     #print(\"score: %.2f\" % (score))\n",
    "#     #print(\"acc: %.2f\" % (acc))\n",
    "\n",
    "# def oneEpoch(X_train_in, Y_train_in):\n",
    "#     oneEpochTrain(X_train_in, Y_train_in)\n",
    "#     #\n",
    "#     lstm_pred = model_lstm.predict(X_val)\n",
    "#     #lstm_pred = sigmoidToPolarity(lstm_pred)\n",
    "#     #lstm_pred = CatCrossEntropy_toPolarity(lstm_pred)\n",
    "#     #\n",
    "#     con_mat = confusion_matrix(np.argmax(lstm_pred, axis=1), np.argmax(Y_val, axis=1))\n",
    "#     acc = accuracy_score(np.argmax(lstm_pred, axis=1), np.argmax(Y_val, axis=1))\n",
    "#     #\n",
    "#     print()\n",
    "#     print(\"LSTM model accuracy score: \", acc)\n",
    "#     #print()\n",
    "#     print(\"LSTM model confusion matrix: \\n\", con_mat)\n",
    "#     #print()\n",
    "#     #print(\"LSTM model classification report: \\n\", classification_report(Y_val, lstm_pred))\n",
    "#     #\n",
    "#     #recall_neg = con_mat[0][0]/(con_mat[0][0] + con_mat[1][0])\n",
    "#     #recall_pos = con_mat[1][1]/(con_mat[0][1] + con_mat[1][1])\n",
    "#     #eturn [recall_neg, recall_pos]\n",
    "#     return con_mat, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oneEpoch(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lstm.fit(x_train, y_train, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # define documents\n",
    "# docs = ['Well done!',\n",
    "#         'Good work',\n",
    "#         'Great effort',\n",
    "#         'nice work',\n",
    "#         'Excellent!',\n",
    "#         'Weak',\n",
    "#         'Poor effort!',\n",
    "#         'not good',\n",
    "#         'poor work',\n",
    "#         'Could have done better.']\n",
    "# # define class labels\n",
    "# labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # integer encode the documents\n",
    "# vocab_size = 50\n",
    "# encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "# print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pad documents to a max length of 4 words\n",
    "# max_length = 4\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# print(padded_docs)\n",
    "# # 1\n",
    "# # 2\n",
    "# # 3\n",
    "# # 4\n",
    "# # pad documents to a max length of 4 words\n",
    "# max_length = 4\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 8, input_length=max_length))  # 50*8 = 400 learnable weights (input so no bias)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))  # 8*4 + 1 = 33 learnable weights\n",
    "# # compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# # summarize the model\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the model\n",
    "# model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Accuracy: %f' % (accuracy*100))\n",
    "# # 1\n",
    "# # 2\n",
    "# # 3\n",
    "# # 4\n",
    "# # 5\n",
    "# # fit the model\n",
    "# model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
