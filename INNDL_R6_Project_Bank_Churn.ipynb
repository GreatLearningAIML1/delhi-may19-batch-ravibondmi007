{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "colab_type": "code",
    "id": "LKaMXh2zU-Mg",
    "outputId": "4b2b269d-df8c-438f-8640-f0b46860881e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XdvziK-6U-Mw"
   },
   "source": [
    "# STEP 1\n",
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GPUYyl5MU-Mz",
    "outputId": "7e1befa4-efa7-4e1f-f360-8cb4af01a6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File id in colab:  1uiIzOwC8FEbDa7C8YHwepq373O1ac1Yl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"READ FROM LOCAL FILE (only in Anaconda, not for Colab)\"\"\"\n",
    "# df = pd.read_csv(\"Churn.csv\")\n",
    "\n",
    "\"\"\"READ FROM LOCAL FILE (only in Colab)\"\"\"\n",
    "# from google.colab import files\n",
    "# df = files.upload()\n",
    "\n",
    "\"\"\"READ FILE SAVED ON GOOGLE DRIVE (only in Colab)\"\"\"\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# The value for link below is derived by\n",
    "# Go to Google Drive > Find the file to be read > Right click > Select 'Get Shareable link' > Paste below\n",
    "link = \"https://drive.google.com/open?id=1uiIzOwC8FEbDa7C8YHwepq373O1ac1Yl\"\n",
    "fluff, id = link.split('=')\n",
    "print (\"File id in colab: \", id) # Verify that you have everything after '='\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('Filename.csv')     # The file name provided here is not important. Any value will work. Probably used for internal storing.\n",
    "df = pd.read_csv('Filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "qRGY7aZyW8Wo",
    "outputId": "704fb5d7-a5c8-4774-e93c-91f6ece1b0d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  ...  IsActiveMember EstimatedSalary Exited\n",
       "0          1    15634602  Hargrave  ...               1       101348.88      1\n",
       "1          2    15647311      Hill  ...               1       112542.58      0\n",
       "2          3    15619304      Onio  ...               0       113931.57      1\n",
       "3          4    15701354      Boni  ...               0        93826.63      0\n",
       "4          5    15737888  Mitchell  ...               1        79084.10      0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6I6RMc6_U-M_"
   },
   "source": [
    "# STEP 2\n",
    "Drop the columns which are unique for all users like IDs (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "yS3xW2RIU-NB",
    "outputId": "dd38effe-93cb-4d76-c6d6-4148b2397135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RowNumber          10000\n",
       "CustomerId         10000\n",
       "Surname             2932\n",
       "CreditScore          460\n",
       "Geography              3\n",
       "Gender                 2\n",
       "Age                   70\n",
       "Tenure                11\n",
       "Balance             6382\n",
       "NumOfProducts          4\n",
       "HasCrCard              2\n",
       "IsActiveMember         2\n",
       "EstimatedSalary     9999\n",
       "Exited                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "# Find columns with unique values in each row\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbZkHuXNU-NJ"
   },
   "outputs": [],
   "source": [
    "def apply_d_l(df_in):\n",
    "    # apply dummies\n",
    "    out = pd.get_dummies(df_in, columns=[\"Geography\"])\n",
    "    out = pd.get_dummies(out, columns=[\"Gender\"])\n",
    "\n",
    "    # applyabel encoding\n",
    "    #out['Family'] = out['Family'].replace({\"???\": 0, '???': 1, '???': 2, '???': 3})\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "HcqgA9DFU-NR",
    "outputId": "3d58d473-63a5-4b9b-8538-4e30b892bb96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Age  Tenure  ...  Geography_Spain  Gender_Female  Gender_Male\n",
       "0          619   42       2  ...                0              1            0\n",
       "1          608   41       1  ...                1              1            0\n",
       "2          502   42       8  ...                0              1            0\n",
       "3          699   39       1  ...                0              1            0\n",
       "4          850   43       2  ...                1              1            0\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns with unique values in each row\n",
    "df.drop([\"RowNumber\", \"CustomerId\"], axis=1, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "Drop \"Surname\"\n",
    "\"\"\"\n",
    "df.drop(\"Surname\", axis=1, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "Apply one hot encoding (dummification) on Geography, Gender\n",
    "\"\"\"\n",
    "df = apply_d_l(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yj0gYsvWU-NZ"
   },
   "source": [
    "# STEP 3\n",
    "Distinguish the feature and target set (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1Aov0wAU-Nb"
   },
   "outputs": [],
   "source": [
    "# Feature set\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "\n",
    "# Target set\n",
    "Y = df.Exited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvFsETIhU-Nj"
   },
   "source": [
    "# STEP 4\n",
    "Divide the data set into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4xnlbsjU-Nn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.30, random_state=0)\n",
    "# trainSet, testSet = train_test_split(X, Y, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Djxtr8MTU-Nw"
   },
   "source": [
    "# STEP 5\n",
    "Normalize the train and test data (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "E2Vif1JbU-Ny",
    "outputId": "9b1a30a2-d368-46aa-d4a6-2c4d53ca2e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 13)\n",
      "(3000, 13)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(7000, 2)\n",
      "(3000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "xTrain = min_max_scaler.fit_transform(xTrain)\n",
    "xTest = min_max_scaler.fit_transform(xTest)\n",
    "yTrain = tf.keras.utils.to_categorical(yTrain, num_classes=2)\n",
    "yTest = tf.keras.utils.to_categorical(yTest, num_classes=2)\n",
    "\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)\n",
    "print(type(xTrain))\n",
    "print(type(yTrain))\n",
    "print(yTrain.shape)\n",
    "print(yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxFTpSh9U-N6"
   },
   "source": [
    "# STEP 6\n",
    "Initialize & build the model (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qXLg4N4U-N9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "#Clear out tensorflow memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPFK_JksU-OE"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add input layer\n",
    "\"\"\"\n",
    "# model.add(tf.keras.layers.Dense(13, activation=activiationMethod,input_shape=(13,)))\n",
    "model.add(tf.keras.layers.Reshape((13,)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Add Hidden Layers\n",
    "\"\"\"\n",
    "# Use relu methods instead of sigmoid\n",
    "# activiationMethod = 'relu'\n",
    "# activiationMethod = 'tanh'\n",
    "activiationMethod = LeakyReLU(alpha=0.3)\n",
    "# leakyReLU = LeakyReLU(alpha=0.3)\n",
    "# activiationMethod = 'sigmoid'\n",
    "\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# Add hidden layers\n",
    "model.add(tf.keras.layers.Dense(200, activation=activiationMethod))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(100, activation=activiationMethod))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(60, activation=activiationMethod))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(30, activation=activiationMethod))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Add OUTPUT layer\n",
    "\"\"\"\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOgRUvDQU-OL"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.015)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, nesterov=True)\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "# optimizer = tf.keras.optimizers.\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "16jQbRoPU-Od",
    "outputId": "76231aad-b40c-4d77-9ca3-c42cfa0094e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 13)\n",
      "(3000, 13)\n",
      "(7000, 2)\n",
      "(3000, 2)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/2\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.5273 - acc: 0.7687 - val_loss: 0.4907 - val_acc: 0.7930\n",
      "Epoch 2/2\n",
      "7000/7000 [==============================] - 0s 61us/sample - loss: 0.4791 - acc: 0.7977 - val_loss: 0.4787 - val_acc: 0.7930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f54338770b8>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xTrain.shape)\n",
    "print(xTest.shape)\n",
    "# print(type(xTrain_scaled))\n",
    "# print(type(yTrain))\n",
    "print(yTrain.shape)\n",
    "print(yTest.shape)\n",
    "\n",
    "model.fit(xTrain,yTrain,          \n",
    "          validation_data=(xTest,yTest),\n",
    "          epochs=2,\n",
    "          batch_size=32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "iArqBFqhU-OV",
    "outputId": "1500ce7f-37b7-497c-de3f-09e6a46772a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  2800      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  6060      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1830      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  62        \n",
      "=================================================================\n",
      "Total params: 30,852\n",
      "Trainable params: 30,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klcLQVrkU-Om"
   },
   "source": [
    "# STEP 7\n",
    "Optimize the model (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39ipZ1XRU-Oo"
   },
   "outputs": [],
   "source": [
    "# Optimize using\n",
    "    # Weight initialization (Normal or Uniform)\n",
    "        # Xavier or Glorot normal distribution [underroot{2/(size of previous layer + size of this layer)}]\n",
    "        # Xavier or Glorot uniform distribution [underroot{6/(size of previous layer + size of this layer)}]\n",
    "        # he initialization normal distribution [underroot{2/size of previous layer}]\n",
    "        # he initialization uniform distribution [underroot{6/size of previous layer}]\n",
    "        # Preferred combination: he initialization with ReLU\n",
    "    # Activiation function\n",
    "        # Sigmoid function\n",
    "        # tanh\n",
    "        # Rectified Linear Unit (ReLU)\n",
    "        # Leaky ReLU\n",
    "    # Dropout (Stop model from memorizing/overfitting)\n",
    "    # Batch normalization (can be used with or inplace of dropout)\n",
    "        # Usually applied to output of hidden layers\n",
    "    # Learning rate\n",
    "    # Learning rate decay\n",
    "    # Optimizers (momentum)\n",
    "        # SGD with Momentum\n",
    "        # SGD with Nesterov momentum\n",
    "        # Adagrad (adapts learning rate of each weight)\n",
    "        # AdaDelta\n",
    "        # Adam (Adaptive moment estimation)\n",
    "    # Number of iterations\n",
    "    # Batch size\n",
    "    # Number of hidden layers\n",
    "    # Number of neurons in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t44UIQjMFWvu",
    "outputId": "040e4e23-1908-4bff-ad33-7366aace6d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call the function'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Function to test a neural network configuration\"\"\"\n",
    "def checkThisConfiguration(xTrainx, yTrainx, xTestx, yTestx, activiationMethod='relu', optimizerType='adam', \n",
    "                    epochs=2, batchSize=32, hiddenLayers=[200, 100, 60, 30], \n",
    "                    learningRate=0.01, do_nesterov=True, decayRate=0.1, verbose=0):\n",
    "    \n",
    "    print(\"\\n***Checking performance on:\")\n",
    "    print(\"ActivationMethod: \", activiationMethod, \", Optimizer: \", optimizerType, \", Learning rate: \", learningRate)\n",
    "    print(\"Epochs: \", epochs, \", Batch size: \", batchSize)\n",
    "    print(\"Hidden layers: \", hiddenLayers)\n",
    "    print(\"Nestrove?: \", do_nesterov, \", Decay rate: \", decayRate)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    model_ = tf.keras.models.Sequential()\n",
    "    \n",
    "    \"\"\"INPUT LAYER\"\"\", \n",
    "    model_.add(tf.keras.layers.Reshape((13,)))\n",
    "    \n",
    "    \"\"\"HIDDEN LAYERS\"\"\"\n",
    "    for layerSize in hiddenLayers:\n",
    "        if activiationMethod=='LeakyReLU':\n",
    "            activationMethod = LeakyReLU(alpha=0.3)\n",
    "            model_.add(tf.keras.layers.Dense(layerSize, activation=LeakyReLU(alpha=0.3)))\n",
    "        else:\n",
    "            model_.add(tf.keras.layers.Dense(layerSize, activation=activiationMethod))\n",
    "        model_.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    \"\"\"OUTPUT LAYER\"\"\"\n",
    "    model_.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    \"\"\"OPTIMIZER\"\"\"\n",
    "    optimizerDef = None\n",
    "    if optimizerType == \"adam\":\n",
    "        optimizerDef = tf.keras.optimizers.Adam(learning_rate=learningRate)\n",
    "    elif optimizerType == \"sgd\":\n",
    "        optimizerDef = tf.keras.optimizers.SGD(learning_rate=learningRate, nesterov=do_nesterov, decay=decayRate)\n",
    "    \n",
    "    if optimizerDef != None:\n",
    "        model_.compile(optimizer=optimizerDef, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    else:\n",
    "        model_.compile(optimizer=optimizerType, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    \n",
    "    model_hist = model_.fit(xTrainx,yTrainx,          \n",
    "          validation_data=(xTestx,yTestx),\n",
    "          epochs=epochs,\n",
    "          batch_size=batchSize,\n",
    "          verbose=verbose)\n",
    "    \n",
    "    return model_hist\n",
    "\n",
    "\"\"\"Call the function\"\"\"\n",
    "# model1_hist = tuneMyNetwork02(xTrain, yTrain, xTest, yTest, activiationMethod='relu', optimizerType=\"adam\", \n",
    "#                               epochs=3, batchSize=30)\n",
    "# model1_hist.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHn6VnkRFWv5"
   },
   "outputs": [],
   "source": [
    "\"\"\"Run desired combinations of hyperparameters\"\"\"\n",
    "def findBestHyperparameters(xTrainx, yTrainx, xTestx, yTestx, \n",
    "                            activiationMethods=['sigmoid', 'relu'], \n",
    "                            optimizerTypes=['sgd', 'adagrad', 'adadelta', 'adam'], \n",
    "                            epochsToTest=[2, 5, 8, 10, 20, 100], batchSizes=[10, 32, 50], \n",
    "                            hiddenLayerSets=[[13, 20, 15, 10], [50, 150, 10], [3, 8, 9], [200, 100, 60, 30]], \n",
    "                            learningRates=[0.005, 0.01, 0.02, 0.05], \n",
    "                            do_nesterov_or_not=[True, False], \n",
    "                            decayRates=[0.5, 0.1, 0.2], \n",
    "                            verbose_in=0):\n",
    "    \n",
    "#     performance_df = pd.DataFrame(columns=[\"activationMethod\", \"optimizerType\", \"epochs\",\"batchSizes\", \n",
    "#                                            \"hiddenLayerNeurons\", \"learningRate\", \"do_nesterov\", \"decayRate\", \n",
    "#                                            \"train_loss\", \"train_accuracy\", \"validation_loss\", \"validation_accuracy\"])\n",
    "    \n",
    "    activationMethod_array = []\n",
    "    optimizerType_array = []\n",
    "    epochs_array = []\n",
    "    batchSize_array = []\n",
    "    hiddenLayerNeurons_array = []\n",
    "    learningRate_array = []\n",
    "    do_nesterov_array = []\n",
    "    decayRate_array = []\n",
    "    train_loss_array = []\n",
    "    train_accuracy_array = []\n",
    "    validation_loss_array = []\n",
    "    validation_accuracy_array = []\n",
    "    \n",
    "    # Create possible combinations among the hyperparameters\n",
    "    for activationMethod_x in activiationMethods:\n",
    "        for optimizerType_x in optimizerTypes:\n",
    "            for epochs_x in epochsToTest:\n",
    "                for batchSize_x in batchSizes:\n",
    "                    for hiddenLayers_x in hiddenLayerSets:\n",
    "                        for learningRate_x in learningRates:\n",
    "                            if optimizerType_x == 'sgd':\n",
    "                                for do_nesterov_x in do_nesterov_or_not:\n",
    "                                    for decayRate_x in decayRates:\n",
    "                                        hist_x = checkThisConfiguration(xTrainx, yTrainx, xTestx, yTestx, \n",
    "                                                                        activationMethod_x, optimizerType_x, epochs_x, \n",
    "                                                                        batchSize_x, hiddenLayers_x, learningRate_x, \n",
    "                                                                        do_nesterov_x, decayRate_x, verbose=verbose_in)\n",
    "                                        \n",
    "                                        hist_params = hist_x.history\n",
    "\n",
    "                                        activationMethod_array.append(activationMethod_x)\n",
    "                                        optimizerType_array.append(optimizerType_x)\n",
    "                                        epochs_array.append(epochs_x)\n",
    "                                        batchSize_array.append(batchSize_x)\n",
    "                                        hiddenLayerNeurons_array.append(hiddenLayers_x)\n",
    "                                        learningRate_array.append(learningRate_x)\n",
    "                                        do_nesterov_array.append(do_nesterov_x)\n",
    "                                        decayRate_array.append(decayRate_x)\n",
    "                                        print(hist_params)\n",
    "                                        train_loss_array.append(hist_params['loss'][epochs_x-1])\n",
    "                                        validation_loss_array.append(hist_params['val_loss'][epochs_x-1])\n",
    "                                        # train_accuracy_array.append(hist_params['accuracy'][epochs_x-1])              # On anaconda\n",
    "                                        # validation_accuracy_array.append(hist_params['val_accuracy'][epochs_x-1])     # On anaconda\n",
    "                                        train_accuracy_array.append(hist_params['acc'][epochs_x-1])              # On colab\n",
    "                                        validation_accuracy_array.append(hist_params['val_acc'][epochs_x-1])     # On colab\n",
    "\n",
    "                            else:\n",
    "                                do_nesterov_x = False\n",
    "                                decayRate_x = 0\n",
    "                                hist_x = checkThisConfiguration(xTrainx, yTrainx, xTestx, yTestx, \n",
    "                                                                activationMethod_x, optimizerType_x, epochs_x, \n",
    "                                                                batchSize_x, hiddenLayers_x, learningRate_x, \n",
    "                                                                verbose=verbose_in)\n",
    "                                \n",
    "                                hist_params = hist_x.history\n",
    "\n",
    "                                activationMethod_array.append(activationMethod_x)\n",
    "                                optimizerType_array.append(optimizerType_x)\n",
    "                                epochs_array.append(epochs_x)\n",
    "                                batchSize_array.append(batchSize_x)\n",
    "                                hiddenLayerNeurons_array.append(hiddenLayers_x)\n",
    "                                learningRate_array.append(learningRate_x)\n",
    "                                do_nesterov_array.append(do_nesterov_x)\n",
    "                                decayRate_array.append(decayRate_x)\n",
    "                                print(hist_params)\n",
    "                                train_loss_array.append(hist_params['loss'][epochs_x-1])\n",
    "                                validation_loss_array.append(hist_params['val_loss'][epochs_x-1])\n",
    "                                # train_accuracy_array.append(hist_params['accuracy'][epochs_x-1])              # On anaconda\n",
    "                                # validation_accuracy_array.append(hist_params['val_accuracy'][epochs_x-1])     # On anaconda\n",
    "                                train_accuracy_array.append(hist_params['acc'][epochs_x-1])              # On colab\n",
    "                                validation_accuracy_array.append(hist_params['val_acc'][epochs_x-1])     # On colab\n",
    "    \n",
    "    #\n",
    "    performance_df_x = pd.DataFrame({\"activationMethod\": activationMethod_array, \n",
    "                                   \"optimizerType\": optimizerType_array, \n",
    "                                   \"epochs\": epochs_array, \n",
    "                                   \"batchSizes\": batchSize_array,\n",
    "                                   \"hiddenLayerNeurons\": hiddenLayerNeurons_array, \n",
    "                                   \"learningRate\": learningRate_array, \n",
    "                                   \"do_nesterov\": do_nesterov_array, \n",
    "                                   \"decayRate\": decayRate_array, \n",
    "                                   \"train_loss\": train_loss_array, \n",
    "                                   \"train_accuracy\": train_accuracy_array, \n",
    "                                   \"validation_loss\": validation_loss_array, \n",
    "                                   \"validation_accuracy\": validation_accuracy_array})\n",
    "    \n",
    "    return performance_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xJNgEJTVFWwA",
    "outputId": "952d5ca9-70f7-43bb-bc60-83ef3947d5fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  sgd , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 121us/sample - loss: 0.5782 - acc: 0.7054 - val_loss: 0.6620 - val_acc: 0.7937\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 73us/sample - loss: 0.5370 - acc: 0.7526 - val_loss: 0.5696 - val_acc: 0.7993\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 73us/sample - loss: 0.5254 - acc: 0.7590 - val_loss: 0.5132 - val_acc: 0.8000\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5205 - acc: 0.7639 - val_loss: 0.4994 - val_acc: 0.7903\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.5149 - acc: 0.7734 - val_loss: 0.4962 - val_acc: 0.7883\n",
      "{'loss': [0.5782074161938259, 0.5369598661490849, 0.5254261676243374, 0.5205006335122244, 0.5149096311160496], 'acc': [0.7054286, 0.7525714, 0.759, 0.7638571, 0.77342856], 'val_loss': [0.6620292293230693, 0.5695968276659648, 0.5132452943325043, 0.49944669326146446, 0.4962130310535431], 'val_acc': [0.79366666, 0.79933333, 0.8, 0.79033333, 0.78833336]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  sgd , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 117us/sample - loss: 0.5384 - acc: 0.7451 - val_loss: 0.5379 - val_acc: 0.7930\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.4883 - acc: 0.7876 - val_loss: 0.4845 - val_acc: 0.7933\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.4749 - acc: 0.7923 - val_loss: 0.4577 - val_acc: 0.8123\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.4710 - acc: 0.7959 - val_loss: 0.4549 - val_acc: 0.8127\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.4683 - acc: 0.7949 - val_loss: 0.4535 - val_acc: 0.8080\n",
      "{'loss': [0.5383748188018799, 0.48831559910093036, 0.47491914824077064, 0.47097580579348974, 0.46828409699031287], 'acc': [0.7451429, 0.78757143, 0.79228574, 0.79585713, 0.79485714], 'val_loss': [0.5378737664222717, 0.4844820079008738, 0.4577401775519053, 0.4548520746231079, 0.4534950164159139], 'val_acc': [0.793, 0.79333335, 0.81233335, 0.81266665, 0.808]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  adagrad , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/adagrad.py:107: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 124us/sample - loss: 0.6156 - acc: 0.6604 - val_loss: 0.6952 - val_acc: 0.3987\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5682 - acc: 0.7233 - val_loss: 0.6035 - val_acc: 0.7943\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 0.5454 - acc: 0.7473 - val_loss: 0.5258 - val_acc: 0.7780\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.5288 - acc: 0.7606 - val_loss: 0.4991 - val_acc: 0.7880\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5162 - acc: 0.7740 - val_loss: 0.4931 - val_acc: 0.7963\n",
      "{'loss': [0.6155707125663757, 0.5681964193752834, 0.5453783372470311, 0.5287811758518219, 0.5161626539570945], 'acc': [0.6604286, 0.72328573, 0.7472857, 0.7605714, 0.774], 'val_loss': [0.6952406490643819, 0.6035009285608928, 0.5257654145558676, 0.4990840424696604, 0.4931278303464254], 'val_acc': [0.39866668, 0.79433334, 0.778, 0.788, 0.7963333]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  adagrad , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 131us/sample - loss: 0.6207 - acc: 0.6621 - val_loss: 0.6159 - val_acc: 0.7930\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5746 - acc: 0.7209 - val_loss: 0.6068 - val_acc: 0.7663\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5499 - acc: 0.7413 - val_loss: 0.5360 - val_acc: 0.7743\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5347 - acc: 0.7596 - val_loss: 0.5108 - val_acc: 0.7937\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5220 - acc: 0.7706 - val_loss: 0.4962 - val_acc: 0.7870\n",
      "{'loss': [0.6206919876507351, 0.574584680897849, 0.5498509286471776, 0.534742918763842, 0.5219674033437457], 'acc': [0.6621429, 0.72085714, 0.74128574, 0.75957143, 0.7705714], 'val_loss': [0.6158862907091777, 0.6068096181551615, 0.5359834728240966, 0.510817504564921, 0.496157346089681], 'val_acc': [0.793, 0.76633334, 0.77433336, 0.79366666, 0.787]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  adadelta , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 137us/sample - loss: 0.9370 - acc: 0.5047 - val_loss: 0.7238 - val_acc: 0.2247\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.8582 - acc: 0.5144 - val_loss: 0.6545 - val_acc: 0.6040\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.8000 - acc: 0.5251 - val_loss: 0.7187 - val_acc: 0.5537\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 0.7569 - acc: 0.5379 - val_loss: 0.7131 - val_acc: 0.5600\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.7238 - acc: 0.5526 - val_loss: 0.6888 - val_acc: 0.5733\n",
      "{'loss': [0.9370137821606227, 0.8582264712878636, 0.8000106444358825, 0.7569076813970294, 0.7238275802476065], 'acc': [0.5047143, 0.51442856, 0.52514285, 0.5378571, 0.5525714], 'val_loss': [0.7237595604260763, 0.6544820334116618, 0.7186799578666687, 0.7130761974652609, 0.6888252976735433], 'val_acc': [0.22466667, 0.604, 0.55366665, 0.56, 0.5733333]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  adadelta , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 131us/sample - loss: 0.9367 - acc: 0.4191 - val_loss: 0.8449 - val_acc: 0.2070\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 81us/sample - loss: 0.8710 - acc: 0.4319 - val_loss: 0.8344 - val_acc: 0.2743\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 84us/sample - loss: 0.8283 - acc: 0.4440 - val_loss: 0.8030 - val_acc: 0.4303\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.7951 - acc: 0.4679 - val_loss: 0.7737 - val_acc: 0.4747\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 83us/sample - loss: 0.7685 - acc: 0.4829 - val_loss: 0.7504 - val_acc: 0.4930\n",
      "{'loss': [0.9366890909331186, 0.871026939868927, 0.8283021473884582, 0.7951367220878601, 0.7685460649217878], 'acc': [0.41914284, 0.43185714, 0.444, 0.46785715, 0.48285714], 'val_loss': [0.8448974410692851, 0.8344106911023458, 0.80297802734375, 0.7737146930694581, 0.7503722112973531], 'val_acc': [0.207, 0.27433333, 0.43033335, 0.47466666, 0.493]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  adam , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 157us/sample - loss: 0.4780 - acc: 0.7927 - val_loss: 0.4881 - val_acc: 0.7930\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 109us/sample - loss: 0.4366 - acc: 0.8127 - val_loss: 0.4223 - val_acc: 0.8180\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.4337 - acc: 0.8163 - val_loss: 0.4594 - val_acc: 0.8197\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.4259 - acc: 0.8166 - val_loss: 0.4629 - val_acc: 0.8037\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.4195 - acc: 0.8260 - val_loss: 0.4218 - val_acc: 0.8097\n",
      "{'loss': [0.4779718016896929, 0.43655738520622256, 0.4337482643808637, 0.425919962814876, 0.41954030391148156], 'acc': [0.7927143, 0.8127143, 0.8162857, 0.8165714, 0.826], 'val_loss': [0.48813515790303547, 0.4223100952307383, 0.4593861699104309, 0.4628521534601847, 0.4218426547050476], 'val_acc': [0.793, 0.818, 0.8196667, 0.80366665, 0.8096667]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  sigmoid , Optimizer:  adam , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 158us/sample - loss: 0.4832 - acc: 0.7963 - val_loss: 0.5376 - val_acc: 0.7930\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.4484 - acc: 0.8007 - val_loss: 0.4679 - val_acc: 0.8013\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.4409 - acc: 0.8020 - val_loss: 0.4474 - val_acc: 0.7940\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.4345 - acc: 0.8159 - val_loss: 0.4705 - val_acc: 0.8167\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 107us/sample - loss: 0.4357 - acc: 0.8143 - val_loss: 0.4236 - val_acc: 0.8213\n",
      "{'loss': [0.48317141359192983, 0.44843405607768466, 0.44094953894615174, 0.4345402284349714, 0.4356694291659764], 'acc': [0.7962857, 0.8007143, 0.802, 0.8158572, 0.8142857], 'val_loss': [0.5375586359500885, 0.4678976457118988, 0.4473753748734792, 0.47052662682533264, 0.4236167716185252], 'val_acc': [0.793, 0.8013333, 0.794, 0.81666666, 0.82133335]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sigmoid combinations\"\"\"\n",
    "performance_df1 = findBestHyperparameters(xTrain, yTrain, xTest, yTest, \n",
    "                        activiationMethods=['sigmoid'],\n",
    "                        optimizerTypes=['sgd', 'adagrad', 'adadelta', 'adam'],\n",
    "                        epochsToTest=[5],\n",
    "                        batchSizes=[32],\n",
    "                        hiddenLayerSets=[[200, 100, 60, 30]],\n",
    "                        learningRates=[0.01, 0.02], \n",
    "                        do_nesterov_or_not=[True],\n",
    "                        decayRates=[0.1], verbose_in=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YQSevWKmFWwO",
    "outputId": "d5dc4f5c-2704-4fd0-cfad-f409595c79f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  sgd , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 122us/sample - loss: 0.6139 - acc: 0.6716 - val_loss: 0.5697 - val_acc: 0.7353\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5601 - acc: 0.7279 - val_loss: 0.5375 - val_acc: 0.7553\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5485 - acc: 0.7379 - val_loss: 0.5325 - val_acc: 0.7453\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5367 - acc: 0.7474 - val_loss: 0.5267 - val_acc: 0.7477\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5343 - acc: 0.7443 - val_loss: 0.5236 - val_acc: 0.7493\n",
      "{'loss': [0.6138851735932487, 0.5600994415964399, 0.5484502863883972, 0.5366726416860308, 0.5342968590259553], 'acc': [0.67157143, 0.7278572, 0.73785716, 0.7474286, 0.7442857], 'val_loss': [0.5696544768015543, 0.5374750396410625, 0.5325309735139211, 0.5266701143582662, 0.5235576151212057], 'val_acc': [0.7353333, 0.7553333, 0.7453333, 0.74766666, 0.7493333]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  sgd , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 123us/sample - loss: 0.5511 - acc: 0.7336 - val_loss: 0.5011 - val_acc: 0.7907\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.4961 - acc: 0.7840 - val_loss: 0.4789 - val_acc: 0.7980\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.4848 - acc: 0.7894 - val_loss: 0.4696 - val_acc: 0.8047\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 81us/sample - loss: 0.4753 - acc: 0.7960 - val_loss: 0.4635 - val_acc: 0.8077\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 84us/sample - loss: 0.4733 - acc: 0.7973 - val_loss: 0.4593 - val_acc: 0.8110\n",
      "{'loss': [0.5510659923894065, 0.49608763480186463, 0.48481199758393423, 0.4753046997955867, 0.47331769033840726], 'acc': [0.7335714, 0.784, 0.7894286, 0.796, 0.79728574], 'val_loss': [0.5010722490946452, 0.47888144755363465, 0.46960144305229184, 0.463509521484375, 0.4592715102036794], 'val_acc': [0.79066664, 0.798, 0.80466664, 0.80766666, 0.811]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  adagrad , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 126us/sample - loss: 0.6291 - acc: 0.6593 - val_loss: 0.5861 - val_acc: 0.7157\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5605 - acc: 0.7343 - val_loss: 0.5330 - val_acc: 0.7970\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.5324 - acc: 0.7611 - val_loss: 0.5055 - val_acc: 0.7953\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5100 - acc: 0.7814 - val_loss: 0.4834 - val_acc: 0.8137\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.4963 - acc: 0.7947 - val_loss: 0.4706 - val_acc: 0.8260\n",
      "{'loss': [0.6291196522712708, 0.5604719789368766, 0.5323903240476335, 0.5100321544919695, 0.4962510816710336], 'acc': [0.6592857, 0.7342857, 0.76114285, 0.7814286, 0.7947143], 'val_loss': [0.5860830345153809, 0.5330113588968913, 0.5055449094772339, 0.4834385550022125, 0.4705526298681895], 'val_acc': [0.71566665, 0.797, 0.7953333, 0.81366664, 0.826]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  adagrad , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 126us/sample - loss: 0.6400 - acc: 0.6496 - val_loss: 0.6190 - val_acc: 0.6683\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5680 - acc: 0.7219 - val_loss: 0.5385 - val_acc: 0.7547\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5386 - acc: 0.7473 - val_loss: 0.5019 - val_acc: 0.7883\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5188 - acc: 0.7681 - val_loss: 0.4834 - val_acc: 0.8003\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5028 - acc: 0.7751 - val_loss: 0.4733 - val_acc: 0.8073\n",
      "{'loss': [0.6399728426252093, 0.5680193685804095, 0.5385523345811026, 0.5187940972873143, 0.5028364758491516], 'acc': [0.6495714, 0.72185713, 0.7472857, 0.7681429, 0.77514285], 'val_loss': [0.6189887770016989, 0.5385270285606384, 0.501923812230428, 0.48344077173868816, 0.47334580413500466], 'val_acc': [0.66833335, 0.7546667, 0.78833336, 0.8003333, 0.80733335]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  adadelta , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 139us/sample - loss: 0.8124 - acc: 0.5197 - val_loss: 0.7658 - val_acc: 0.5007\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 0.7855 - acc: 0.5299 - val_loss: 0.7478 - val_acc: 0.5243\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 83us/sample - loss: 0.7600 - acc: 0.5296 - val_loss: 0.7273 - val_acc: 0.5417\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 0.7422 - acc: 0.5380 - val_loss: 0.7126 - val_acc: 0.5493\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.7257 - acc: 0.5459 - val_loss: 0.7001 - val_acc: 0.5640\n",
      "{'loss': [0.8123639400345939, 0.7854602653639657, 0.7599637735230582, 0.7422266129766192, 0.7256508405549186], 'acc': [0.5197143, 0.52985716, 0.5295714, 0.538, 0.54585713], 'val_loss': [0.7658257816632589, 0.7478484501838684, 0.7272813668251038, 0.7126256465911865, 0.7001284867922465], 'val_acc': [0.5006667, 0.52433336, 0.5416667, 0.54933333, 0.564]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  adadelta , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 135us/sample - loss: 0.8132 - acc: 0.5269 - val_loss: 0.7117 - val_acc: 0.5583\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 87us/sample - loss: 0.7849 - acc: 0.5310 - val_loss: 0.7369 - val_acc: 0.5427\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.7671 - acc: 0.5364 - val_loss: 0.7292 - val_acc: 0.5620\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.7518 - acc: 0.5416 - val_loss: 0.7230 - val_acc: 0.5663\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.7363 - acc: 0.5536 - val_loss: 0.7106 - val_acc: 0.5717\n",
      "{'loss': [0.8131503891944886, 0.7849019694328309, 0.7671383669716971, 0.7518491467067173, 0.7363416929244995], 'acc': [0.52685714, 0.531, 0.5364286, 0.54157144, 0.5535714], 'val_loss': [0.7116661508878072, 0.7368686599731445, 0.7292309435208638, 0.7229642103513082, 0.7106495846112569], 'val_acc': [0.55833334, 0.5426667, 0.562, 0.56633335, 0.57166666]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  adam , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 158us/sample - loss: 0.4961 - acc: 0.7807 - val_loss: 0.4696 - val_acc: 0.7883\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.4473 - acc: 0.8073 - val_loss: 0.4300 - val_acc: 0.8213\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.4322 - acc: 0.8201 - val_loss: 0.4574 - val_acc: 0.7897\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.4276 - acc: 0.8199 - val_loss: 0.4194 - val_acc: 0.8247\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 105us/sample - loss: 0.4060 - acc: 0.8307 - val_loss: 0.4069 - val_acc: 0.8377\n",
      "{'loss': [0.49612752209390915, 0.4473388798577445, 0.4322062579904284, 0.4276291412966592, 0.4060030824116298], 'acc': [0.7807143, 0.8072857, 0.82014287, 0.8198571, 0.8307143], 'val_loss': [0.4696486802101135, 0.43001183025042217, 0.4573697170416514, 0.419427499294281, 0.4069340989589691], 'val_acc': [0.78833336, 0.82133335, 0.78966665, 0.8246667, 0.8376667]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  tanh , Optimizer:  adam , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 159us/sample - loss: 0.4824 - acc: 0.7841 - val_loss: 0.4822 - val_acc: 0.7940\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.4543 - acc: 0.8016 - val_loss: 0.4466 - val_acc: 0.8040\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.4527 - acc: 0.8040 - val_loss: 0.4500 - val_acc: 0.7943\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.4505 - acc: 0.8016 - val_loss: 0.4539 - val_acc: 0.7993\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.4570 - acc: 0.7996 - val_loss: 0.4734 - val_acc: 0.7930\n",
      "{'loss': [0.482389203275953, 0.45425148350851874, 0.4527402438095638, 0.45050178950173514, 0.45703824598448617], 'acc': [0.78414285, 0.8015714, 0.804, 0.8015714, 0.79957145], 'val_loss': [0.48220914908250173, 0.4465975860754649, 0.4500062464078267, 0.4539476620356242, 0.47335218556722003], 'val_acc': [0.794, 0.804, 0.79433334, 0.79933333, 0.793]}\n"
     ]
    }
   ],
   "source": [
    "performance_df2 = findBestHyperparameters(xTrain, yTrain, xTest, yTest, \n",
    "                        activiationMethods=['tanh'],\n",
    "                        optimizerTypes=['sgd', 'adagrad', 'adadelta', 'adam'],\n",
    "                        epochsToTest=[5],\n",
    "                        batchSizes=[32],\n",
    "                        hiddenLayerSets=[[200, 100, 60, 30]],\n",
    "                        learningRates=[0.01, 0.02], \n",
    "                        do_nesterov_or_not=[True],\n",
    "                        decayRates=[0.1], verbose_in=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7MjNRyUaFWwZ",
    "outputId": "06bede88-a50f-4e6e-fda8-72442506f2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  sgd , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 124us/sample - loss: 0.6327 - acc: 0.6587 - val_loss: 1.3877 - val_acc: 0.2070\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5700 - acc: 0.7100 - val_loss: 0.8752 - val_acc: 0.3710\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5551 - acc: 0.7267 - val_loss: 0.5599 - val_acc: 0.7377\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5470 - acc: 0.7387 - val_loss: 0.5194 - val_acc: 0.7777\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5381 - acc: 0.7486 - val_loss: 0.5128 - val_acc: 0.7810\n",
      "{'loss': [0.6327258095741272, 0.5700405972685133, 0.5551122195720672, 0.5469691556862423, 0.5381454468114035], 'acc': [0.6587143, 0.71, 0.7267143, 0.7387143, 0.74857146], 'val_loss': [1.3876995601654052, 0.875245470046997, 0.5598721521695454, 0.5194353130658468, 0.5127939364910126], 'val_acc': [0.207, 0.371, 0.73766667, 0.7776667, 0.781]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  sgd , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 128us/sample - loss: 0.5551 - acc: 0.7294 - val_loss: 0.6192 - val_acc: 0.7593\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.4965 - acc: 0.7806 - val_loss: 0.5395 - val_acc: 0.7897\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.4824 - acc: 0.7903 - val_loss: 0.4695 - val_acc: 0.8167\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.4750 - acc: 0.7959 - val_loss: 0.4526 - val_acc: 0.8190\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.4706 - acc: 0.7977 - val_loss: 0.4469 - val_acc: 0.8193\n",
      "{'loss': [0.5550804107870374, 0.49653214836120607, 0.4824085257393973, 0.47504985700334823, 0.47059724521636964], 'acc': [0.7294286, 0.7805714, 0.7902857, 0.79585713, 0.7977143], 'val_loss': [0.6191918797492981, 0.5394532558123271, 0.4695150895118713, 0.45260592007637024, 0.4469432306289673], 'val_acc': [0.7593333, 0.78966665, 0.81666666, 0.819, 0.8193333]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  adagrad , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 139us/sample - loss: 0.6344 - acc: 0.6603 - val_loss: 0.7094 - val_acc: 0.4843\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5668 - acc: 0.7247 - val_loss: 0.5983 - val_acc: 0.7123\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.5350 - acc: 0.7627 - val_loss: 0.5173 - val_acc: 0.7830\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5153 - acc: 0.7839 - val_loss: 0.4808 - val_acc: 0.8207\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.5016 - acc: 0.7907 - val_loss: 0.4656 - val_acc: 0.8280\n",
      "{'loss': [0.6343801555974143, 0.5668381745474679, 0.5350426550592695, 0.5152761960710798, 0.5015923190457481], 'acc': [0.6602857, 0.7247143, 0.76271427, 0.78385717, 0.79071426], 'val_loss': [0.7093624917666117, 0.5982632643381754, 0.5172686346371969, 0.48082092316945396, 0.4656123411655426], 'val_acc': [0.48433334, 0.7123333, 0.783, 0.8206667, 0.828]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  adagrad , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 131us/sample - loss: 0.6404 - acc: 0.6549 - val_loss: 0.5207 - val_acc: 0.7937\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.5624 - acc: 0.7296 - val_loss: 0.4992 - val_acc: 0.8140\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5288 - acc: 0.7627 - val_loss: 0.4852 - val_acc: 0.8190\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5093 - acc: 0.7829 - val_loss: 0.4760 - val_acc: 0.8177\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 83us/sample - loss: 0.4945 - acc: 0.7921 - val_loss: 0.4641 - val_acc: 0.8257\n",
      "{'loss': [0.6404163806779044, 0.5623866898672921, 0.5288426334517343, 0.5092961220060076, 0.49452162756238666], 'acc': [0.65485716, 0.7295714, 0.76271427, 0.7828571, 0.79214287], 'val_loss': [0.5207416570981344, 0.49919442494710287, 0.4851898903846741, 0.47599345628420514, 0.46411128775278726], 'val_acc': [0.79366666, 0.814, 0.819, 0.81766665, 0.82566667]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  adadelta , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 136us/sample - loss: 1.0753 - acc: 0.5120 - val_loss: 0.7821 - val_acc: 0.4853\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 1.0034 - acc: 0.5109 - val_loss: 0.9547 - val_acc: 0.4983\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 85us/sample - loss: 0.9430 - acc: 0.5166 - val_loss: 0.8951 - val_acc: 0.5293\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 81us/sample - loss: 0.9079 - acc: 0.5204 - val_loss: 0.8361 - val_acc: 0.5460\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 0.8695 - acc: 0.5280 - val_loss: 0.8028 - val_acc: 0.5487\n",
      "{'loss': [1.07527424628394, 1.0034331842831203, 0.9430027001925877, 0.9079088343211583, 0.8694806914329529], 'acc': [0.512, 0.51085716, 0.5165714, 0.5204286, 0.528], 'val_loss': [0.7821438954671224, 0.9547443976402282, 0.8951302189826965, 0.8360544020334879, 0.8028208335240682], 'val_acc': [0.48533332, 0.49833333, 0.52933335, 0.546, 0.54866666]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  adadelta , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 139us/sample - loss: 0.9218 - acc: 0.5160 - val_loss: 0.5232 - val_acc: 0.7930\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.8854 - acc: 0.5234 - val_loss: 0.5905 - val_acc: 0.7097\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 84us/sample - loss: 0.8535 - acc: 0.5256 - val_loss: 0.7375 - val_acc: 0.5900\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.8332 - acc: 0.5286 - val_loss: 0.7944 - val_acc: 0.5450\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.8094 - acc: 0.5479 - val_loss: 0.7857 - val_acc: 0.5460\n",
      "{'loss': [0.9218116215297154, 0.885408611706325, 0.8534998220716205, 0.8332062091146196, 0.8094270357404436], 'acc': [0.516, 0.52342856, 0.5255714, 0.5285714, 0.54785717], 'val_loss': [0.523192274014155, 0.5904777677853902, 0.7374867331186931, 0.7944486962954204, 0.7856770753860474], 'val_acc': [0.793, 0.70966667, 0.59, 0.545, 0.546]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  adam , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 161us/sample - loss: 0.4740 - acc: 0.7987 - val_loss: 0.4658 - val_acc: 0.7933\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 108us/sample - loss: 0.4200 - acc: 0.8229 - val_loss: 0.4065 - val_acc: 0.8420\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.3952 - acc: 0.8360 - val_loss: 0.3907 - val_acc: 0.8340\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 107us/sample - loss: 0.3811 - acc: 0.8417 - val_loss: 0.3576 - val_acc: 0.8523\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3759 - acc: 0.8451 - val_loss: 0.3782 - val_acc: 0.8300\n",
      "{'loss': [0.4739870477744511, 0.4199538926056453, 0.39517443897042953, 0.38113233154160636, 0.3759416716098785], 'acc': [0.7987143, 0.82285714, 0.836, 0.84171426, 0.84514284], 'val_loss': [0.4658457033634186, 0.40654191668828327, 0.3906910133361816, 0.3575897047519684, 0.3781609145005544], 'val_acc': [0.79333335, 0.842, 0.834, 0.8523333, 0.83]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  LeakyReLU , Optimizer:  adam , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 157us/sample - loss: 0.4609 - acc: 0.8019 - val_loss: 0.4432 - val_acc: 0.8057\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.4285 - acc: 0.8190 - val_loss: 0.4415 - val_acc: 0.8077\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.4078 - acc: 0.8261 - val_loss: 0.4388 - val_acc: 0.7967\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3839 - acc: 0.8390 - val_loss: 0.3614 - val_acc: 0.8550\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3781 - acc: 0.8467 - val_loss: 0.4365 - val_acc: 0.7927\n",
      "{'loss': [0.4609472524097988, 0.4285373604978834, 0.40781291702815464, 0.3839091886452266, 0.3781408490453448], 'acc': [0.8018571, 0.819, 0.82614285, 0.839, 0.84671426], 'val_loss': [0.4431761060555776, 0.4414815269311269, 0.4388450047175089, 0.36142881484826406, 0.43645468044281005], 'val_acc': [0.8056667, 0.80766666, 0.7966667, 0.855, 0.7926667]}\n"
     ]
    }
   ],
   "source": [
    "performance_df3 = findBestHyperparameters(xTrain, yTrain, xTest, yTest, \n",
    "                        activiationMethods=['LeakyReLU'],\n",
    "                        optimizerTypes=['sgd', 'adagrad', 'adadelta', 'adam'],\n",
    "                        epochsToTest=[5],\n",
    "                        batchSizes=[32],\n",
    "                        hiddenLayerSets=[[200, 100, 60, 30]],\n",
    "                        learningRates=[0.01, 0.02], \n",
    "                        do_nesterov_or_not=[True],\n",
    "                        decayRates=[0.1], verbose_in=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WT2sJa7PFWwi",
    "outputId": "d7016a51-8e57-45b1-f0c9-1001cc93d9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  sgd , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 131us/sample - loss: 0.6658 - acc: 0.6371 - val_loss: 0.6013 - val_acc: 0.6803\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5875 - acc: 0.7020 - val_loss: 0.5442 - val_acc: 0.7390\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5662 - acc: 0.7190 - val_loss: 0.5367 - val_acc: 0.7550\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.5604 - acc: 0.7221 - val_loss: 0.5343 - val_acc: 0.7573\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.5571 - acc: 0.7269 - val_loss: 0.5321 - val_acc: 0.7567\n",
      "{'loss': [0.6657922226701464, 0.5874882426261901, 0.5662426836150033, 0.5603589200292315, 0.5570595631258828], 'acc': [0.63714284, 0.702, 0.719, 0.7221429, 0.7268571], 'val_loss': [0.6013418018023173, 0.5441504615147909, 0.5367436725298563, 0.5343230882485708, 0.5320558031400044], 'val_acc': [0.6803333, 0.739, 0.755, 0.75733334, 0.75666666]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  sgd , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 128us/sample - loss: 0.5854 - acc: 0.7176 - val_loss: 0.4913 - val_acc: 0.7933\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.4962 - acc: 0.7774 - val_loss: 0.4537 - val_acc: 0.8070\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.4771 - acc: 0.7877 - val_loss: 0.4487 - val_acc: 0.8057\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.4642 - acc: 0.7959 - val_loss: 0.4545 - val_acc: 0.7987\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 75us/sample - loss: 0.4569 - acc: 0.7996 - val_loss: 0.4529 - val_acc: 0.8020\n",
      "{'loss': [0.5854445488452912, 0.4962312593460083, 0.4771224364212581, 0.4642300933769771, 0.456886885200228], 'acc': [0.71757144, 0.77742857, 0.7877143, 0.79585713, 0.79957145], 'val_loss': [0.49127918124198916, 0.45372443946202595, 0.44869506057103475, 0.4545186375776927, 0.4528804777463277], 'val_acc': [0.79333335, 0.807, 0.8056667, 0.79866666, 0.802]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adagrad , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 134us/sample - loss: 0.6901 - acc: 0.6333 - val_loss: 0.5873 - val_acc: 0.7513\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5891 - acc: 0.7096 - val_loss: 0.5248 - val_acc: 0.7847\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5527 - acc: 0.7350 - val_loss: 0.5078 - val_acc: 0.7930\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 79us/sample - loss: 0.5317 - acc: 0.7610 - val_loss: 0.4970 - val_acc: 0.7953\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 77us/sample - loss: 0.5156 - acc: 0.7726 - val_loss: 0.4929 - val_acc: 0.8027\n",
      "{'loss': [0.6900503902435303, 0.5890895118032183, 0.5526772229330881, 0.531723876953125, 0.5155704154968261], 'acc': [0.6332857, 0.7095714, 0.735, 0.761, 0.77257144], 'val_loss': [0.5872807402610779, 0.524786768913269, 0.507799646695455, 0.4969777880509694, 0.49294248612721764], 'val_acc': [0.75133336, 0.78466666, 0.793, 0.7953333, 0.80266666]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adagrad , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 134us/sample - loss: 0.6980 - acc: 0.6096 - val_loss: 0.8277 - val_acc: 0.2933\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 76us/sample - loss: 0.5890 - acc: 0.6987 - val_loss: 0.6722 - val_acc: 0.6143\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.5559 - acc: 0.7313 - val_loss: 0.5293 - val_acc: 0.7653\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5272 - acc: 0.7626 - val_loss: 0.4961 - val_acc: 0.7937\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.5117 - acc: 0.7751 - val_loss: 0.4790 - val_acc: 0.8043\n",
      "{'loss': [0.6980104826859066, 0.589023222378322, 0.5558788186482021, 0.5271916000843048, 0.511704179082598], 'acc': [0.60957146, 0.69871426, 0.7312857, 0.76257145, 0.77514285], 'val_loss': [0.8277244726816814, 0.6722154777844747, 0.5293100465138754, 0.4961003154913584, 0.47901103528340655], 'val_acc': [0.29333332, 0.61433333, 0.76533335, 0.79366666, 0.8043333]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adadelta , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 142us/sample - loss: 1.0411 - acc: 0.4827 - val_loss: 0.6520 - val_acc: 0.6160\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 1.0075 - acc: 0.4943 - val_loss: 0.9408 - val_acc: 0.4697\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 0.9901 - acc: 0.4984 - val_loss: 0.9701 - val_acc: 0.5027\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.9779 - acc: 0.5006 - val_loss: 0.9351 - val_acc: 0.5310\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.9565 - acc: 0.5029 - val_loss: 0.9115 - val_acc: 0.5363\n",
      "{'loss': [1.0410954738344464, 1.0074582843780517, 0.9900942256110055, 0.9778808977944511, 0.9565174794878278], 'acc': [0.4827143, 0.4942857, 0.49842858, 0.5005714, 0.50285715], 'val_loss': [0.6520061548550924, 0.9407630494435628, 0.9701014833450318, 0.9350982270240784, 0.9114687150319417], 'val_acc': [0.616, 0.46966666, 0.50266665, 0.531, 0.5363333]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adadelta , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 137us/sample - loss: 1.0161 - acc: 0.4939 - val_loss: 0.5191 - val_acc: 0.7930\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 82us/sample - loss: 1.0036 - acc: 0.4951 - val_loss: 0.6748 - val_acc: 0.6250\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.9774 - acc: 0.4931 - val_loss: 0.9230 - val_acc: 0.5067\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 78us/sample - loss: 0.9622 - acc: 0.5039 - val_loss: 0.9813 - val_acc: 0.4927\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 80us/sample - loss: 0.9438 - acc: 0.5049 - val_loss: 0.9804 - val_acc: 0.4913\n",
      "{'loss': [1.0160841869626727, 1.0035538635253907, 0.977420893737248, 0.9621789981297084, 0.9438244231087821], 'acc': [0.49385715, 0.49514285, 0.49314284, 0.50385714, 0.5048571], 'val_loss': [0.51909255250295, 0.6748123973210652, 0.9229802006085713, 0.9812848339080811, 0.9804288260142009], 'val_acc': [0.793, 0.625, 0.50666666, 0.49266666, 0.49133334]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adam , Learning rate:  0.01\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 156us/sample - loss: 0.4687 - acc: 0.7984 - val_loss: 0.4386 - val_acc: 0.8047\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.4073 - acc: 0.8220 - val_loss: 0.4025 - val_acc: 0.8213\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3824 - acc: 0.8406 - val_loss: 0.3611 - val_acc: 0.8453\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3790 - acc: 0.8457 - val_loss: 0.3649 - val_acc: 0.8420\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.3727 - acc: 0.8386 - val_loss: 0.3632 - val_acc: 0.8573\n",
      "{'loss': [0.4686692796434675, 0.4072575260911669, 0.38237189997945514, 0.37902773915018356, 0.37267701935768127], 'acc': [0.7984286, 0.822, 0.8405714, 0.8457143, 0.8385714], 'val_loss': [0.43861786913871764, 0.4024526562690735, 0.3611462148030599, 0.3648617720603943, 0.36317128213246663], 'val_acc': [0.80466664, 0.82133335, 0.84533334, 0.842, 0.85733336]}\n",
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adam , Learning rate:  0.02\n",
      "Epochs:  5 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  True , Decay rate:  0.1\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "7000/7000 [==============================] - 1s 166us/sample - loss: 0.4697 - acc: 0.7931 - val_loss: 0.4520 - val_acc: 0.8040\n",
      "Epoch 2/5\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.4229 - acc: 0.8203 - val_loss: 0.7363 - val_acc: 0.6793\n",
      "Epoch 3/5\n",
      "7000/7000 [==============================] - 1s 106us/sample - loss: 0.3901 - acc: 0.8360 - val_loss: 0.3516 - val_acc: 0.8607\n",
      "Epoch 4/5\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3792 - acc: 0.8400 - val_loss: 0.3597 - val_acc: 0.8530\n",
      "Epoch 5/5\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.3744 - acc: 0.8459 - val_loss: 0.3462 - val_acc: 0.8540\n",
      "{'loss': [0.4696756111553737, 0.4228695631708418, 0.3900742563860757, 0.3791659173965454, 0.37435014431817193], 'acc': [0.79314286, 0.82028574, 0.836, 0.84, 0.84585714], 'val_loss': [0.4519547712802887, 0.7362973976135254, 0.3516365829308828, 0.35967116443316144, 0.34616416533788047], 'val_acc': [0.804, 0.6793333, 0.8606667, 0.853, 0.854]}\n"
     ]
    }
   ],
   "source": [
    "performance_df4 = findBestHyperparameters(xTrain, yTrain, xTest, yTest, \n",
    "                        activiationMethods=['relu'],\n",
    "                        optimizerTypes=['sgd', 'adagrad', 'adadelta', 'adam'],\n",
    "                        epochsToTest=[5],\n",
    "                        batchSizes=[32],\n",
    "                        hiddenLayerSets=[[200, 100, 60, 30]],\n",
    "                        learningRates=[0.01, 0.02], \n",
    "                        do_nesterov_or_not=[True],\n",
    "                        decayRates=[0.1], verbose_in=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "colab_type": "code",
    "id": "hFiZFoZsFWwp",
    "outputId": "3b7af099-429c-4418-f7e9-73a9e17ffbf6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activationMethod</th>\n",
       "      <th>optimizerType</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batchSizes</th>\n",
       "      <th>hiddenLayerNeurons</th>\n",
       "      <th>learningRate</th>\n",
       "      <th>do_nesterov</th>\n",
       "      <th>decayRate</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374350</td>\n",
       "      <td>0.845857</td>\n",
       "      <td>0.346164</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372677</td>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.363171</td>\n",
       "      <td>0.857333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375942</td>\n",
       "      <td>0.845143</td>\n",
       "      <td>0.378161</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.406003</td>\n",
       "      <td>0.830714</td>\n",
       "      <td>0.406934</td>\n",
       "      <td>0.837667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.421843</td>\n",
       "      <td>0.809667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435669</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.423617</td>\n",
       "      <td>0.821333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378141</td>\n",
       "      <td>0.846714</td>\n",
       "      <td>0.436455</td>\n",
       "      <td>0.792667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.470597</td>\n",
       "      <td>0.797714</td>\n",
       "      <td>0.446943</td>\n",
       "      <td>0.819333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.456887</td>\n",
       "      <td>0.799571</td>\n",
       "      <td>0.452880</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.468284</td>\n",
       "      <td>0.794857</td>\n",
       "      <td>0.453495</td>\n",
       "      <td>0.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.473318</td>\n",
       "      <td>0.797286</td>\n",
       "      <td>0.459272</td>\n",
       "      <td>0.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494522</td>\n",
       "      <td>0.792143</td>\n",
       "      <td>0.464111</td>\n",
       "      <td>0.825667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501592</td>\n",
       "      <td>0.790714</td>\n",
       "      <td>0.465612</td>\n",
       "      <td>0.828000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496251</td>\n",
       "      <td>0.794714</td>\n",
       "      <td>0.470553</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502836</td>\n",
       "      <td>0.775143</td>\n",
       "      <td>0.473346</td>\n",
       "      <td>0.807333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.457038</td>\n",
       "      <td>0.799571</td>\n",
       "      <td>0.473352</td>\n",
       "      <td>0.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511704</td>\n",
       "      <td>0.775143</td>\n",
       "      <td>0.479011</td>\n",
       "      <td>0.804333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515570</td>\n",
       "      <td>0.772571</td>\n",
       "      <td>0.492942</td>\n",
       "      <td>0.802667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516163</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.493128</td>\n",
       "      <td>0.796333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521967</td>\n",
       "      <td>0.770571</td>\n",
       "      <td>0.496157</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.514910</td>\n",
       "      <td>0.773429</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.788333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.538145</td>\n",
       "      <td>0.748571</td>\n",
       "      <td>0.512794</td>\n",
       "      <td>0.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.534297</td>\n",
       "      <td>0.744286</td>\n",
       "      <td>0.523558</td>\n",
       "      <td>0.749333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.557060</td>\n",
       "      <td>0.726857</td>\n",
       "      <td>0.532056</td>\n",
       "      <td>0.756667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723828</td>\n",
       "      <td>0.552571</td>\n",
       "      <td>0.688825</td>\n",
       "      <td>0.573333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725651</td>\n",
       "      <td>0.545857</td>\n",
       "      <td>0.700128</td>\n",
       "      <td>0.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736342</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.710650</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768546</td>\n",
       "      <td>0.482857</td>\n",
       "      <td>0.750372</td>\n",
       "      <td>0.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.809427</td>\n",
       "      <td>0.547857</td>\n",
       "      <td>0.785677</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869481</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.802821</td>\n",
       "      <td>0.548667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.956517</td>\n",
       "      <td>0.502857</td>\n",
       "      <td>0.911469</td>\n",
       "      <td>0.536333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>adadelta</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>[200, 100, 60, 30]</td>\n",
       "      <td>0.02</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.943824</td>\n",
       "      <td>0.504857</td>\n",
       "      <td>0.980429</td>\n",
       "      <td>0.491333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activationMethod optimizerType  ...  validation_loss  validation_accuracy\n",
       "7             relu          adam  ...         0.346164             0.854000\n",
       "6             relu          adam  ...         0.363171             0.857333\n",
       "6        LeakyReLU          adam  ...         0.378161             0.830000\n",
       "6             tanh          adam  ...         0.406934             0.837667\n",
       "6          sigmoid          adam  ...         0.421843             0.809667\n",
       "7          sigmoid          adam  ...         0.423617             0.821333\n",
       "7        LeakyReLU          adam  ...         0.436455             0.792667\n",
       "1        LeakyReLU           sgd  ...         0.446943             0.819333\n",
       "1             relu           sgd  ...         0.452880             0.802000\n",
       "1          sigmoid           sgd  ...         0.453495             0.808000\n",
       "1             tanh           sgd  ...         0.459272             0.811000\n",
       "3        LeakyReLU       adagrad  ...         0.464111             0.825667\n",
       "2        LeakyReLU       adagrad  ...         0.465612             0.828000\n",
       "2             tanh       adagrad  ...         0.470553             0.826000\n",
       "3             tanh       adagrad  ...         0.473346             0.807333\n",
       "7             tanh          adam  ...         0.473352             0.793000\n",
       "3             relu       adagrad  ...         0.479011             0.804333\n",
       "2             relu       adagrad  ...         0.492942             0.802667\n",
       "2          sigmoid       adagrad  ...         0.493128             0.796333\n",
       "3          sigmoid       adagrad  ...         0.496157             0.787000\n",
       "0          sigmoid           sgd  ...         0.496213             0.788333\n",
       "0        LeakyReLU           sgd  ...         0.512794             0.781000\n",
       "0             tanh           sgd  ...         0.523558             0.749333\n",
       "0             relu           sgd  ...         0.532056             0.756667\n",
       "4          sigmoid      adadelta  ...         0.688825             0.573333\n",
       "4             tanh      adadelta  ...         0.700128             0.564000\n",
       "5             tanh      adadelta  ...         0.710650             0.571667\n",
       "5          sigmoid      adadelta  ...         0.750372             0.493000\n",
       "5        LeakyReLU      adadelta  ...         0.785677             0.546000\n",
       "4        LeakyReLU      adadelta  ...         0.802821             0.548667\n",
       "4             relu      adadelta  ...         0.911469             0.536333\n",
       "5             relu      adadelta  ...         0.980429             0.491333\n",
       "\n",
       "[32 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df_all = pd.concat([performance_df1, performance_df2, performance_df3, performance_df4], axis=0)\n",
    "\n",
    "performance_df_all.sort_values([\"validation_loss\"], axis=0, inplace=True)\n",
    "performance_df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G27dyFOnFWwv"
   },
   "source": [
    "##### The best combination is:\n",
    "- activation = relu\n",
    "- optimizer = adam\n",
    "- learningRate = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OW5dXBRsFWwx",
    "outputId": "7346342e-999c-4c4a-e1af-3d00bb496ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adam , Learning rate:  0.02\n",
      "Epochs:  50 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  False , Decay rate:  0\n",
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      "7000/7000 [==============================] - 1s 156us/sample - loss: 0.4669 - acc: 0.8036 - val_loss: 0.4216 - val_acc: 0.8337\n",
      "Epoch 2/50\n",
      "7000/7000 [==============================] - 1s 107us/sample - loss: 0.4127 - acc: 0.8197 - val_loss: 0.3773 - val_acc: 0.8420\n",
      "Epoch 3/50\n",
      "7000/7000 [==============================] - 1s 97us/sample - loss: 0.3834 - acc: 0.8410 - val_loss: 0.4162 - val_acc: 0.8097\n",
      "Epoch 4/50\n",
      "7000/7000 [==============================] - 1s 98us/sample - loss: 0.3803 - acc: 0.8410 - val_loss: 0.3718 - val_acc: 0.8427\n",
      "Epoch 5/50\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3767 - acc: 0.8410 - val_loss: 0.4529 - val_acc: 0.7757\n",
      "Epoch 6/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3771 - acc: 0.8414 - val_loss: 0.4054 - val_acc: 0.8440\n",
      "Epoch 7/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3723 - acc: 0.8437 - val_loss: 0.3529 - val_acc: 0.8490\n",
      "Epoch 8/50\n",
      "7000/7000 [==============================] - 1s 105us/sample - loss: 0.3730 - acc: 0.8450 - val_loss: 0.3682 - val_acc: 0.8353\n",
      "Epoch 9/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3666 - acc: 0.8480 - val_loss: 0.3505 - val_acc: 0.8573\n",
      "Epoch 10/50\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.3706 - acc: 0.8470 - val_loss: 0.3530 - val_acc: 0.8567\n",
      "Epoch 11/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3673 - acc: 0.8473 - val_loss: 0.3574 - val_acc: 0.8557\n",
      "Epoch 12/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3669 - acc: 0.8470 - val_loss: 0.3760 - val_acc: 0.8343\n",
      "Epoch 13/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3602 - acc: 0.8500 - val_loss: 0.3692 - val_acc: 0.8427\n",
      "Epoch 14/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3556 - acc: 0.8504 - val_loss: 0.3565 - val_acc: 0.8530\n",
      "Epoch 15/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3645 - acc: 0.8510 - val_loss: 0.3625 - val_acc: 0.8517\n",
      "Epoch 16/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3622 - acc: 0.8479 - val_loss: 0.3513 - val_acc: 0.8497\n",
      "Epoch 17/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3594 - acc: 0.8501 - val_loss: 0.3566 - val_acc: 0.8450\n",
      "Epoch 18/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3645 - acc: 0.8480 - val_loss: 0.3479 - val_acc: 0.8570\n",
      "Epoch 19/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3631 - acc: 0.8480 - val_loss: 0.3586 - val_acc: 0.8453\n",
      "Epoch 20/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3584 - acc: 0.8511 - val_loss: 0.3490 - val_acc: 0.8547\n",
      "Epoch 21/50\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.3578 - acc: 0.8509 - val_loss: 0.3525 - val_acc: 0.8573\n",
      "Epoch 22/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3585 - acc: 0.8536 - val_loss: 0.3499 - val_acc: 0.8557\n",
      "Epoch 23/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3559 - acc: 0.8559 - val_loss: 0.3872 - val_acc: 0.8330\n",
      "Epoch 24/50\n",
      "7000/7000 [==============================] - 1s 96us/sample - loss: 0.3575 - acc: 0.8526 - val_loss: 0.3599 - val_acc: 0.8457\n",
      "Epoch 25/50\n",
      "7000/7000 [==============================] - 1s 106us/sample - loss: 0.3571 - acc: 0.8499 - val_loss: 0.3622 - val_acc: 0.8483\n",
      "Epoch 26/50\n",
      "7000/7000 [==============================] - 1s 98us/sample - loss: 0.3592 - acc: 0.8501 - val_loss: 0.3544 - val_acc: 0.8510\n",
      "Epoch 27/50\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3604 - acc: 0.8527 - val_loss: 0.3645 - val_acc: 0.8473\n",
      "Epoch 28/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3567 - acc: 0.8513 - val_loss: 0.3403 - val_acc: 0.8510\n",
      "Epoch 29/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3518 - acc: 0.8516 - val_loss: 0.3431 - val_acc: 0.8597\n",
      "Epoch 30/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3553 - acc: 0.8543 - val_loss: 0.3465 - val_acc: 0.8580\n",
      "Epoch 31/50\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3577 - acc: 0.8499 - val_loss: 0.3582 - val_acc: 0.8467\n",
      "Epoch 32/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3527 - acc: 0.8563 - val_loss: 0.3391 - val_acc: 0.8617\n",
      "Epoch 33/50\n",
      "7000/7000 [==============================] - 1s 96us/sample - loss: 0.3547 - acc: 0.8539 - val_loss: 0.3461 - val_acc: 0.8580\n",
      "Epoch 34/50\n",
      "7000/7000 [==============================] - 1s 98us/sample - loss: 0.3498 - acc: 0.8580 - val_loss: 0.3405 - val_acc: 0.8583\n",
      "Epoch 35/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3481 - acc: 0.8586 - val_loss: 0.3436 - val_acc: 0.8620\n",
      "Epoch 36/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3505 - acc: 0.8550 - val_loss: 0.3462 - val_acc: 0.8597\n",
      "Epoch 37/50\n",
      "7000/7000 [==============================] - 1s 100us/sample - loss: 0.3495 - acc: 0.8514 - val_loss: 0.3566 - val_acc: 0.8513\n",
      "Epoch 38/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3506 - acc: 0.8514 - val_loss: 0.3408 - val_acc: 0.8623\n",
      "Epoch 39/50\n",
      "7000/7000 [==============================] - 1s 97us/sample - loss: 0.3483 - acc: 0.8526 - val_loss: 0.3415 - val_acc: 0.8613\n",
      "Epoch 40/50\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.3472 - acc: 0.8546 - val_loss: 0.3560 - val_acc: 0.8443\n",
      "Epoch 41/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3484 - acc: 0.8561 - val_loss: 0.3452 - val_acc: 0.8570\n",
      "Epoch 42/50\n",
      "7000/7000 [==============================] - 1s 99us/sample - loss: 0.3485 - acc: 0.8549 - val_loss: 0.3477 - val_acc: 0.8610\n",
      "Epoch 43/50\n",
      "7000/7000 [==============================] - 1s 101us/sample - loss: 0.3487 - acc: 0.8546 - val_loss: 0.3503 - val_acc: 0.8490\n",
      "Epoch 44/50\n",
      "7000/7000 [==============================] - 1s 105us/sample - loss: 0.3472 - acc: 0.8596 - val_loss: 0.3643 - val_acc: 0.8480\n",
      "Epoch 45/50\n",
      "7000/7000 [==============================] - 1s 104us/sample - loss: 0.3422 - acc: 0.8553 - val_loss: 0.3395 - val_acc: 0.8500\n",
      "Epoch 46/50\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3470 - acc: 0.8567 - val_loss: 0.3427 - val_acc: 0.8580\n",
      "Epoch 47/50\n",
      "7000/7000 [==============================] - 1s 105us/sample - loss: 0.3471 - acc: 0.8540 - val_loss: 0.3393 - val_acc: 0.8607\n",
      "Epoch 48/50\n",
      "7000/7000 [==============================] - 1s 97us/sample - loss: 0.3439 - acc: 0.8581 - val_loss: 0.3559 - val_acc: 0.8577\n",
      "Epoch 49/50\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3433 - acc: 0.8571 - val_loss: 0.3454 - val_acc: 0.8557\n",
      "Epoch 50/50\n",
      "7000/7000 [==============================] - 1s 102us/sample - loss: 0.3463 - acc: 0.8537 - val_loss: 0.3584 - val_acc: 0.8547\n"
     ]
    }
   ],
   "source": [
    "hist_preferred = checkThisConfiguration(xTrain, yTrain, xTest, yTest, \n",
    "                                        activiationMethod='relu', optimizerType='adam', epochs=50, \n",
    "                                        batchSize=32, hiddenLayers=[200, 100, 60, 30], learningRate=0.02, \n",
    "                                        do_nesterov=False, decayRate=0, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccFfPdZFFWw6"
   },
   "source": [
    "# STEP 8\n",
    "Predict the results using 0.5 as a threshold (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8JYD7-lFWw8"
   },
   "outputs": [],
   "source": [
    "model_preferred = hist_preferred.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "BdlddzqqFWxC",
    "outputId": "89e174d9-2305-4f12-854f-7c0a51d622f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.663569  , 0.33643103],\n",
       "       [0.74241114, 0.25758886],\n",
       "       [0.9043453 , 0.09565477],\n",
       "       ...,\n",
       "       [0.86731553, 0.13268451],\n",
       "       [0.8380441 , 0.16195583],\n",
       "       [0.25188065, 0.7481193 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_prob = model_preferred.predict(xTest)\n",
    "y_predict_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLBmTv-EFWxU"
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for entry in y_predict_prob:\n",
    "    if entry[0] > 0.5:\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "\n",
    "y_test = []\n",
    "for entry in yTest:\n",
    "    if entry[0] == 1:\n",
    "        y_test.append(0)\n",
    "    else:\n",
    "        y_test.append(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZp9o7iRFWxb"
   },
   "source": [
    "# STEP 9\n",
    "Print the Accuracy score and confusion matrix (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hM_wu0bpFWxd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "7YNsPCzxFWxk",
    "outputId": "63271bdf-a83d-4a7b-80dc-551cb256b823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8546666666666667\n",
      "Confusion matrix: \n",
      " [[2211  168]\n",
      " [ 268  353]]\n",
      "Recall to predict existing customers:  0.5684380032206119\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ACCURACY SCORE\"\"\"\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\"\"\"CONFUSION MATRIX\"\"\"\n",
    "con_mat = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix: \\n\", con_mat)\n",
    "\n",
    "\"\"\"RECALL\"\"\"\n",
    "recall = con_mat[1,1]/(con_mat[1,1] + con_mat[1,0])\n",
    "print(\"Recall to predict existing customers: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7IcGWN1FWyN"
   },
   "source": [
    "### Inference\n",
    "- From the recall value it can be seen that the prevision of the model to predict the existing customers is below 50%. Hence, it is not a good model.\n",
    "- Seems there is class imbalance, causing the model to predict the non-exiting records better than the exiting records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfqJkQGzFWyR"
   },
   "source": [
    "# Apply SMOTE to remove some class imbalance. Then model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "u4GgsEgqFWyU",
    "outputId": "5bf43d7f-bc27-4b30-e62d-8a9e7880cc6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge imbalanced-learn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0fiOfBEFWyc"
   },
   "outputs": [],
   "source": [
    "y_train = []     # list\n",
    "for entry in yTrain:\n",
    "    if entry[0] == 1:\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m_ZxWcw3FWyi",
    "outputId": "c60e299e-0372-4dd5-8041-240beed600b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "y_train_df = pd.DataFrame(y_train, columns=[\"exiting\"])\n",
    "y_train_df\n",
    "\n",
    "# Get (7000, ) from (7000, 1)\n",
    "y_train = y_train_df[\"exiting\"]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "cgcpdpGOFWyq",
    "outputId": "336d9b46-3e06-4437-bd46-4b510215fefd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 1416\n",
      "Before OverSampling, counts of label '0': 5584 \n",
      "\n",
      "After OverSampling, the shape of train_X: (11168, 13)\n",
      "After OverSampling, the shape of train_y: (11168,) \n",
      "\n",
      "After OverSampling, counts of label '1': 5584\n",
      "After OverSampling, counts of label '0': 5584\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(xTrain, y_train.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "fPg4hKD6FWyy",
    "outputId": "97e10ebf-5e76-4540-996b-793182faa63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 2)\n",
      "(11168, 2)\n"
     ]
    }
   ],
   "source": [
    "# Convert the \n",
    "y_train_res_categorized = tf.keras.utils.to_categorical(y_train_res, num_classes=2)\n",
    "# yTest_categorized = tf.keras.utils.to_categorical(yTest, num_classes=2)\n",
    "\n",
    "# print(yTest_categorized.shape)\n",
    "print(yTest.shape)\n",
    "print(y_train_res_categorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gVGzOA9BFWy3",
    "outputId": "06282e05-a7f9-429a-e602-d2771758ad21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Checking performance on:\n",
      "ActivationMethod:  relu , Optimizer:  adam , Learning rate:  0.02\n",
      "Epochs:  50 , Batch size:  32\n",
      "Hidden layers:  [200, 100, 60, 30]\n",
      "Nestrove?:  False , Decay rate:  0\n",
      "Train on 11168 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      "11168/11168 [==============================] - 1s 130us/sample - loss: 0.5858 - acc: 0.6889 - val_loss: 0.4274 - val_acc: 0.8083\n",
      "Epoch 2/50\n",
      "11168/11168 [==============================] - 1s 97us/sample - loss: 0.5119 - acc: 0.7490 - val_loss: 0.4815 - val_acc: 0.7797\n",
      "Epoch 3/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.5019 - acc: 0.7542 - val_loss: 0.4054 - val_acc: 0.8367\n",
      "Epoch 4/50\n",
      "11168/11168 [==============================] - 1s 92us/sample - loss: 0.4885 - acc: 0.7614 - val_loss: 0.6045 - val_acc: 0.6947\n",
      "Epoch 5/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4825 - acc: 0.7665 - val_loss: 0.3969 - val_acc: 0.8263\n",
      "Epoch 6/50\n",
      "11168/11168 [==============================] - 1s 94us/sample - loss: 0.4748 - acc: 0.7715 - val_loss: 0.4431 - val_acc: 0.7997\n",
      "Epoch 7/50\n",
      "11168/11168 [==============================] - 1s 95us/sample - loss: 0.4734 - acc: 0.7753 - val_loss: 0.4303 - val_acc: 0.7970\n",
      "Epoch 8/50\n",
      "11168/11168 [==============================] - 1s 91us/sample - loss: 0.4638 - acc: 0.7787 - val_loss: 0.4436 - val_acc: 0.7673\n",
      "Epoch 9/50\n",
      "11168/11168 [==============================] - 1s 98us/sample - loss: 0.4662 - acc: 0.7825 - val_loss: 0.4101 - val_acc: 0.8200\n",
      "Epoch 10/50\n",
      "11168/11168 [==============================] - 1s 95us/sample - loss: 0.4591 - acc: 0.7839 - val_loss: 0.6219 - val_acc: 0.6893\n",
      "Epoch 11/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4544 - acc: 0.7807 - val_loss: 0.4735 - val_acc: 0.7820\n",
      "Epoch 12/50\n",
      "11168/11168 [==============================] - 1s 95us/sample - loss: 0.4507 - acc: 0.7868 - val_loss: 0.4137 - val_acc: 0.8033\n",
      "Epoch 13/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4525 - acc: 0.7857 - val_loss: 0.6202 - val_acc: 0.7003\n",
      "Epoch 14/50\n",
      "11168/11168 [==============================] - 1s 96us/sample - loss: 0.4465 - acc: 0.7880 - val_loss: 0.4680 - val_acc: 0.7890\n",
      "Epoch 15/50\n",
      "11168/11168 [==============================] - 1s 95us/sample - loss: 0.4528 - acc: 0.7843 - val_loss: 0.4249 - val_acc: 0.8030\n",
      "Epoch 16/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4472 - acc: 0.7879 - val_loss: 0.4076 - val_acc: 0.8120\n",
      "Epoch 17/50\n",
      "11168/11168 [==============================] - 1s 92us/sample - loss: 0.4365 - acc: 0.7971 - val_loss: 0.4243 - val_acc: 0.8127\n",
      "Epoch 18/50\n",
      "11168/11168 [==============================] - 1s 94us/sample - loss: 0.4357 - acc: 0.7990 - val_loss: 0.3462 - val_acc: 0.8567\n",
      "Epoch 19/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4344 - acc: 0.7971 - val_loss: 0.5249 - val_acc: 0.7193\n",
      "Epoch 20/50\n",
      "11168/11168 [==============================] - 1s 97us/sample - loss: 0.4377 - acc: 0.7985 - val_loss: 0.4059 - val_acc: 0.8180\n",
      "Epoch 21/50\n",
      "11168/11168 [==============================] - 1s 97us/sample - loss: 0.4321 - acc: 0.7999 - val_loss: 0.4038 - val_acc: 0.8220\n",
      "Epoch 22/50\n",
      "11168/11168 [==============================] - 1s 98us/sample - loss: 0.4262 - acc: 0.8039 - val_loss: 0.4844 - val_acc: 0.8400\n",
      "Epoch 23/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4273 - acc: 0.8010 - val_loss: 0.4007 - val_acc: 0.8117\n",
      "Epoch 24/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4188 - acc: 0.8074 - val_loss: 0.5565 - val_acc: 0.6867\n",
      "Epoch 25/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4315 - acc: 0.7952 - val_loss: 0.3761 - val_acc: 0.8413\n",
      "Epoch 26/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4192 - acc: 0.8061 - val_loss: 0.4243 - val_acc: 0.8103\n",
      "Epoch 27/50\n",
      "11168/11168 [==============================] - 1s 92us/sample - loss: 0.4162 - acc: 0.8081 - val_loss: 0.3748 - val_acc: 0.8393\n",
      "Epoch 28/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4335 - acc: 0.7964 - val_loss: 0.4071 - val_acc: 0.8170\n",
      "Epoch 29/50\n",
      "11168/11168 [==============================] - 1s 91us/sample - loss: 0.4327 - acc: 0.7984 - val_loss: 0.3849 - val_acc: 0.8317\n",
      "Epoch 30/50\n",
      "11168/11168 [==============================] - 1s 95us/sample - loss: 0.4187 - acc: 0.8086 - val_loss: 0.4045 - val_acc: 0.8220\n",
      "Epoch 31/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4089 - acc: 0.8118 - val_loss: 0.4107 - val_acc: 0.8207\n",
      "Epoch 32/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4176 - acc: 0.8032 - val_loss: 0.5052 - val_acc: 0.7493\n",
      "Epoch 33/50\n",
      "11168/11168 [==============================] - 1s 94us/sample - loss: 0.4206 - acc: 0.8041 - val_loss: 0.4020 - val_acc: 0.8237\n",
      "Epoch 34/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.4050 - acc: 0.8162 - val_loss: 0.3983 - val_acc: 0.8243\n",
      "Epoch 35/50\n",
      "11168/11168 [==============================] - 1s 94us/sample - loss: 0.4038 - acc: 0.8118 - val_loss: 0.4639 - val_acc: 0.7957\n",
      "Epoch 36/50\n",
      "11168/11168 [==============================] - 1s 92us/sample - loss: 0.4007 - acc: 0.8200 - val_loss: 0.3616 - val_acc: 0.8597\n",
      "Epoch 37/50\n",
      "11168/11168 [==============================] - 1s 93us/sample - loss: 0.3964 - acc: 0.8176 - val_loss: 0.4355 - val_acc: 0.7897\n",
      "Epoch 38/50\n",
      "11168/11168 [==============================] - 1s 98us/sample - loss: 0.3882 - acc: 0.8226 - val_loss: 0.3682 - val_acc: 0.8440\n",
      "Epoch 39/50\n",
      "11168/11168 [==============================] - 1s 101us/sample - loss: 0.3913 - acc: 0.8215 - val_loss: 0.5681 - val_acc: 0.7230\n",
      "Epoch 40/50\n",
      "11168/11168 [==============================] - 1s 100us/sample - loss: 0.3924 - acc: 0.8224 - val_loss: 0.4912 - val_acc: 0.8103\n",
      "Epoch 41/50\n",
      "11168/11168 [==============================] - 1s 99us/sample - loss: 0.3978 - acc: 0.8195 - val_loss: 0.4044 - val_acc: 0.8167\n",
      "Epoch 42/50\n",
      "11168/11168 [==============================] - 1s 96us/sample - loss: 0.3936 - acc: 0.8193 - val_loss: 0.4894 - val_acc: 0.7547\n",
      "Epoch 43/50\n",
      "11168/11168 [==============================] - 1s 98us/sample - loss: 0.3930 - acc: 0.8206 - val_loss: 0.3990 - val_acc: 0.8260\n",
      "Epoch 44/50\n",
      "11168/11168 [==============================] - 1s 101us/sample - loss: 0.3899 - acc: 0.8210 - val_loss: 0.4148 - val_acc: 0.8230\n",
      "Epoch 45/50\n",
      "11168/11168 [==============================] - 1s 101us/sample - loss: 0.3886 - acc: 0.8222 - val_loss: 0.4037 - val_acc: 0.8290\n",
      "Epoch 46/50\n",
      "11168/11168 [==============================] - 1s 102us/sample - loss: 0.4019 - acc: 0.8190 - val_loss: 1.3802 - val_acc: 0.7543\n",
      "Epoch 47/50\n",
      "11168/11168 [==============================] - 1s 99us/sample - loss: 0.3917 - acc: 0.8189 - val_loss: 8.4368 - val_acc: 0.7780\n",
      "Epoch 48/50\n",
      "11168/11168 [==============================] - 1s 98us/sample - loss: 0.3985 - acc: 0.8146 - val_loss: 0.7624 - val_acc: 0.7483\n",
      "Epoch 49/50\n",
      "11168/11168 [==============================] - 1s 95us/sample - loss: 0.3906 - acc: 0.8205 - val_loss: 0.4637 - val_acc: 0.8197\n",
      "Epoch 50/50\n",
      "11168/11168 [==============================] - 1s 100us/sample - loss: 0.3782 - acc: 0.8301 - val_loss: 0.6764 - val_acc: 0.7983\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create model on the balanced data\"\"\"\n",
    "hist_preferred_smotTrain = checkThisConfiguration(X_train_res, y_train_res_categorized, xTest, yTest, \n",
    "                                        activiationMethod='relu', optimizerType='adam', epochs=50, \n",
    "                                        batchSize=32, hiddenLayers=[200, 100, 60, 30], learningRate=0.02, \n",
    "                                        do_nesterov=False, decayRate=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "TZMuJ_W9FWy7",
    "outputId": "3d2623bd-6c81-4e42-fe7e-96ffea78528b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7983333333333333\n",
      "Confusion matrix: \n",
      " [[1961  418]\n",
      " [ 187  434]]\n",
      "Recall to predict existing customers:  0.6988727858293076\n"
     ]
    }
   ],
   "source": [
    "model_smot = hist_preferred_smotTrain.model\n",
    "\n",
    "y_predict_prob_smot = model_smot.predict(xTest)\n",
    "# y_predict_prob\n",
    "\n",
    "# Create list of y_pred and y_test containing only one value (instead of 2 values)\n",
    "y_pred_smot = []\n",
    "for entry in y_predict_prob_smot:\n",
    "    if entry[0] > 0.5:\n",
    "        y_pred_smot.append(0)\n",
    "    else:\n",
    "        y_pred_smot.append(1)\n",
    "\n",
    "y_test = []\n",
    "for entry in yTest:\n",
    "    if entry[0] == 1:\n",
    "        y_test.append(0)\n",
    "    else:\n",
    "        y_test.append(1)\n",
    "\n",
    "\n",
    "\"\"\"ACCURACY SCORE\"\"\"\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_smot))\n",
    "\n",
    "\"\"\"CONFUSION MATRIX\"\"\"\n",
    "con_mat = confusion_matrix(y_test, y_pred_smot)\n",
    "print(\"Confusion matrix: \\n\", con_mat)\n",
    "\n",
    "\"\"\"RECALL\"\"\"\n",
    "recall = con_mat[1,1]/(con_mat[1,1] + con_mat[1,0])\n",
    "print(\"Recall to predict existing customers: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "boJBm5wXFWzB",
    "outputId": "f0f21921-9ac4-4afc-fe56-e41cbd38b105"
   },
   "outputs": [],
   "source": [
    "# y_train_res_categorized = tf.keras.utils.to_categorical(y_train_res, num_classes=2)\n",
    "\n",
    "# \"\"\"Create model on the balanced data (learningRate=0.01)\"\"\"\n",
    "# hist_preferred_smotTrain = checkThisConfiguration(X_train_res, y_train_res_categorized, xTest, yTest, \n",
    "#                                         activiationMethod='relu', optimizerType='adam', epochs=50, \n",
    "#                                         batchSize=32, hiddenLayers=[200, 100, 60, 30], learningRate=0.01, \n",
    "#                                         do_nesterov=False, decayRate=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "iPsM9QivJm-l",
    "outputId": "5621973c-7d67-4ca0-866d-aa7adcfb1c18"
   },
   "outputs": [],
   "source": [
    "# model_smot = hist_preferred_smotTrain.model\n",
    "\n",
    "# y_predict_prob_smot = model_smot.predict(xTest)\n",
    "# # y_predict_prob\n",
    "\n",
    "# # Create list of y_pred and y_test containing only one value (instead of 2 values)\n",
    "# y_pred_smot = []\n",
    "# for entry in y_predict_prob_smot:\n",
    "#     if entry[0] > 0.5:\n",
    "#         y_pred_smot.append(0)\n",
    "#     else:\n",
    "#         y_pred_smot.append(1)\n",
    "\n",
    "# y_test = []\n",
    "# for entry in yTest:\n",
    "#     if entry[0] == 1:\n",
    "#         y_test.append(0)\n",
    "#     else:\n",
    "#         y_test.append(1)\n",
    "\n",
    "\n",
    "# \"\"\"ACCURACY SCORE\"\"\"\n",
    "# print(\"Accuracy: \", accuracy_score(y_test, y_pred_smot))\n",
    "\n",
    "# \"\"\"CONFUSION MATRIX\"\"\"\n",
    "# con_mat = confusion_matrix(y_test, y_pred_smot)\n",
    "# print(\"Confusion matrix: \\n\", con_mat)\n",
    "\n",
    "# \"\"\"RECALL\"\"\"\n",
    "# recall = con_mat[1,1]/(con_mat[1,1] + con_mat[1,0])\n",
    "# print(\"Recall to predict existing customers: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQnZB_fAJ4qT"
   },
   "source": [
    "# INFERENCE\n",
    "After using SMOT for class balance of the data (over target value)\n",
    "- The overall accuracy of the model has decreased\n",
    "- The recall is substantially increased (for identifying users who may exit)\n",
    "\n",
    "As the target of the modeling was to identify users who may exit, therefore we will take the 'RECALL' as the preferred performance metric over overall accuracy. Hence, we will use the model with the training on SMOT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "INNDL_R6_Project_Bank_Churn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
